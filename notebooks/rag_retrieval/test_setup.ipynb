{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-13 15:55:55 __main__ INFO     torch.__version__='2.3.1', torch.version.cuda='12.1'\n",
      "2024-06-13 15:55:55 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2024-06-13 15:55:55 __main__ INFO     transformers.__version__='4.41.2'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-13 15:55:56 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-13 15:56:00 src.models INFO     loaded model </home/local_arnab/Codes/saved_model_weights/meta-llama/Meta-Llama-3-8B> | size: 15316.516 MB | dtype: torch.float16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    # model_key = \"gpt2\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_nnsight(model, layer_name):\n",
    "    layer = model\n",
    "    for name in layer_name.split(\".\"):\n",
    "        layer = layer[int(name)] if name.isdigit() else getattr(layer, name)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' Seattle', 0.969),\n",
       " ('\\xa0', 0.004),\n",
       " (' Seat', 0.003),\n",
       " (' se', 0.003),\n",
       " (' Se', 0.002),\n",
       " (' the', 0.001),\n",
       " (' Bellev', 0.001),\n",
       " ('\\n', 0.001),\n",
       " (' Washington', 0.001),\n",
       " (' Sea', 0.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The Space Needle is located in the city of\"\n",
    "layer_name = mt.layer_names[-1]\n",
    "\n",
    "with mt.trace() as tracer:\n",
    "    with tracer.invoke(prompt) as invoker:\n",
    "        layer = get_module_nnsight(mt, layer_name)\n",
    "        hidden_states = functional.untuple(layer.output).save()\n",
    "        logits = mt.output.logits.save()\n",
    "\n",
    "functional.interpret_logits(\n",
    "    logits=logits[:, -1].squeeze(),\n",
    "    tokenizer=mt.tokenizer,\n",
    "    get_proba=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' Seattle', 0.969),\n",
       " ('\\xa0', 0.004),\n",
       " (' Seat', 0.003),\n",
       " (' se', 0.003),\n",
       " (' Se', 0.002),\n",
       " (' the', 0.001),\n",
       " (' Bellev', 0.001),\n",
       " ('\\n', 0.001),\n",
       " (' Washington', 0.001),\n",
       " (' Sea', 0.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functional.logit_lens(\n",
    "    h=hidden_states[:, -1],\n",
    "    mt=mt,\n",
    "    get_proba=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import prepare_input\n",
    "\n",
    "# prompt = \"Assume an alternate universe where Eiffel Tower is one of the tourist attractions in the capital of Italy. In that universe the Eiffel Tower is located in the city of\"\n",
    "prompt = \"Assume an alternate universe where Eiffel Tower is one of the tourist attractions in the capital of Italy. In that universe the tall iron structure in the city of Rome is called\"\n",
    "# prompt = \"The capital of France is Oslo. The capital of France is\"\n",
    "\n",
    "inputs = prepare_input(prompts=prompt, tokenizer=mt, add_bos_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Assume an alternate universe where Eiffel Tower is one of the tourist attractions in the capital of Italy. In that universe the tall iron structure in the city of Rome is called the Eiffel Tower. The Eiffel Tower is a 324-metre tall iron lattice tower located on the Champ de Mars in Paris,'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = mt._model.generate(**inputs, max_new_tokens=30, top_k=1)\n",
    "\n",
    "mt.tokenizer.decode(generation[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
