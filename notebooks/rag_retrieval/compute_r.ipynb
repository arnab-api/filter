{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 12:54:30 __main__ INFO     torch.__version__='2.3.1', torch.version.cuda='12.1'\n",
      "2024-07-11 12:54:31 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2024-07-11 12:54:31 __main__ INFO     transformers.__version__='4.42.3'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 12:58:43 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 12:58:52 src.models INFO     loaded model </home/local_arnab/Codes/saved_model_weights/meta-llama/Meta-Llama-3-8B> | size: 30633.023 MB | dtype: torch.float32 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    torch_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 84552]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import is_llama_variant\n",
    "\n",
    "subj = \"The Space Needle\"\n",
    "obj = \"Nairobi\"\n",
    "\n",
    "obj_start_idx = 1 if is_llama_variant(mt) else 0\n",
    "o_toks = mt.tokenizer(obj).input_ids[obj_start_idx:]\n",
    "\n",
    "o_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = mt.tokenizer(subj, return_tensors=\"pt\").to(mt.device)\n",
    "# ------------------------------\n",
    "n_rel_toks = 10\n",
    "# ------------------------------\n",
    "\n",
    "subj_end = tokenized.input_ids.shape[-1]\n",
    "\n",
    "tokenized[\"input_ids\"] = torch.cat([tokenized.input_ids, torch.tensor([mt.tokenizer.pad_token_id]*10)[None].to(mt.device)], dim=-1)\n",
    "tokenized[\"attention_mask\"] = torch.cat([tokenized.attention_mask, torch.tensor([1]*10)[None].to(mt.device)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "embedder = baukit.get_module(mt._model, mt.embedder_name).weight.detach().clone()\n",
    "embedder = F.normalize(embedder, p=2, dim=1)\n",
    "embedder.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 13:00:39 __main__ INFO     iter=0, loss=4.766937255859375 | token_map_loss=4.766937255859375\n",
      "2024-07-09 13:00:40 __main__ INFO     iter=100, loss=3.981580084655434e-05 | token_map_loss=3.981580084655434e-05\n",
      "2024-07-09 13:00:42 __main__ INFO     iter=200, loss=1.1087664120168483e-10 | token_map_loss=1.1087664120168483e-10\n",
      "2024-07-09 13:00:44 __main__ INFO     iter=300, loss=7.105427357601002e-14 | token_map_loss=7.105427357601002e-14\n",
      "2024-07-09 13:00:46 __main__ INFO     iter=400, loss=0.0 | token_map_loss=0.0\n",
      "2024-07-09 13:00:48 __main__ INFO     iter=500, loss=0.0 | token_map_loss=0.0\n",
      "2024-07-09 13:00:50 __main__ INFO     iter=600, loss=0.0 | token_map_loss=0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m token_map_loss\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | token_map_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_map_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "learning_rate = 1e-3\n",
    "num_steps = 2500\n",
    "# -----------------------------------\n",
    "\n",
    "soft_tokens = embedder.mean(dim=0).repeat(n_rel_toks, 1).T\n",
    "soft_tokens += torch.randn_like(soft_tokens) * 1e-3\n",
    "# soft_tokens = F.normalize(soft_tokens, p=2, dim=0)\n",
    "soft_tokens = torch.nn.Parameter(soft_tokens)\n",
    "\n",
    "optimizer = torch.optim.Adam([soft_tokens], lr=learning_rate)\n",
    "\n",
    "def \n",
    "\n",
    "for iter in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        logger.info(f\"{iter=}, loss={loss.item()} | token_map_loss={token_map_loss.item()}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([122549, 122549, 122549, 122549, 122549, 122549, 122549, 122549, 122549,\n",
       "        122549], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dot_prods.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>The relationship between the Albert Einstein and Matthew McConaughey is a strange one. The actor has been seen wearing the physicist’s glasses on numerous occasions, and he even named his son after him. But what does this have to do with the actor’s career?\\nAlbert Einstein was a physicist\\nAlbert Einstein was a physicist who was born in Ulm, Germany.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import prepare_input\n",
    "\n",
    "# prompt = \"Assume an alternate universe where Eiffel Tower is one of the tourist attractions in the capital of Italy. In that universe the Eiffel Tower is located in the city of\"\n",
    "prompt = \"The relationship between the Albert Einstein and Matthew McConaughey is\"\n",
    "# prompt = \"The capital of France is Oslo. The capital of France is\"\n",
    "\n",
    "inputs = prepare_input(\n",
    "    prompts = prompt,\n",
    "    tokenizer=mt,\n",
    "    add_bos_token=False\n",
    ")\n",
    "\n",
    "generation = mt._model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=60,\n",
    "    top_k = 5\n",
    ")\n",
    "\n",
    "mt.tokenizer.decode(generation[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
