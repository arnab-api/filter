{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ec34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfae7235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:24:21 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-08-07 16:24:21 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-08-07 16:24:21 __main__ INFO     transformers.__version__='4.54.1'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7720ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:24:24 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-07 16:24:24 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-07 16:24:24 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-08-07 16:24:24 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683855df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:24:25 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-08-07 16:24:25 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:24:25 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-08-07 16:24:25 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-08-07 16:24:25 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/models/meta-llama/Llama-3.3-70B-Instruct/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf3a52a14d243d2b03094930116dc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:25:12 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-08-07 16:25:12 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n",
      "2025-08-07 16:25:12 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62d97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectOneTask: (profession of a famous person)\n",
      "Categories: actor(20), singer(20), comedian(20), director(20), basketball player(20), football player(20), soccer player(20), tennis player(20), golfer(20), boxer(20), news anchor(20), journalist(20), author(20), fashion designer(20), entrepreneur(19), politician(20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOneTask\n",
    "\n",
    "select_prof = SelectOneTask.load(\n",
    "    path=os.path.join(env_utils.DEFAULT_DATA_DIR, \"selection\", \"profession.json\")\n",
    ")\n",
    "\n",
    "print(select_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45087a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:25:25 src.selection.data ERROR    Sample = Will Smith -> Angelina Jolie (0): ['Angelina Jolie', 'Robert Lewandowski', 'James Harden', 'Jim Carrey', 'Steven Spielberg', 'Brandon Sanderson']\n",
      "    Top prediction \" The\"[578] (p=0.188, logit=17.250) does not match the object Angelina Jolie[23950, \" Angel\"].\n",
      "    Retry count: 1. Retrying ...\n",
      "    \n",
      "2025-08-07 16:25:25 src.selection.data ERROR    Sample = Chris Hemsworth -> Al Pacino (1): ['Vinícius Júnior', 'Al Pacino', 'Rihanna', 'Kevin Durant', 'Robin Roberts', 'Phil Mickelson']\n",
      "    Top prediction \" \"[220] (p=0.428, logit=19.125) does not match the object Al Pacino[1708, \" Al\"].\n",
      "    Retry count: 2. Retrying ...\n",
      "    \n",
      "Chris Hemsworth -> Morgan Freeman (4): ['Canelo Álvarez', 'Bill Burr', 'Son Heung-min', 'Billie Eilish', 'Morgan Freeman', 'Jim Acosta']\n"
     ]
    }
   ],
   "source": [
    "sample = select_prof.get_random_sample(\n",
    "    mt = mt,\n",
    "    prompt_template_idx=2,\n",
    "    option_style=\"numbered\",\n",
    "    category=\"actor\"\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66399b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Which person from the following list is by profession a actor?\n",
      "1. Canelo Álvarez\n",
      "2. Bill Burr\n",
      "3. Son Heung-min\n",
      "4. Billie Eilish\n",
      "5. Morgan Freeman\n",
      "6. Jim Acosta\n",
      "Answer:\" >> Morgan Freeman\n"
     ]
    }
   ],
   "source": [
    "# sample.prompt_template = select_prof.prompt_templates[3]\n",
    "\n",
    "print(f'\"{sample.prompt()}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f57d67c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which person from the following list is by profession a actor?\n",
      "Options: Canelo Álvarez, Bill Burr, Son Heung-min, Billie Eilish, Morgan Freeman, Jim Acosta.\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(sample.prompt(option_style=\"single_line\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0499b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Morgan Freeman is an actor by profession. He is a renowned American actor, director, and narrator known\" >> Morgan Freeman\n"
     ]
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")[0]\n",
    "print(f'\"{gen}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a9a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patches to ablate possible answer information from options\n",
      "2025-08-07 16:25:33 src.experiments.utils DEBUG    Predictions: ['\" Morgan\"[23809] (p=0.324, logit=19.250)', '\" \"[220] (p=0.285, logit=19.125)', '\" The\"[578] (p=0.135, logit=18.375)', '\" Bill\"[8766] (p=0.082, logit=17.875)', '\" (\"[320] (p=0.027, logit=16.750)']\n",
      "2025-08-07 16:25:33 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-131e9c2c-74c4\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-131e9c2c-74c4\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"Which\", \" person\", \" from\", \" the\", \" following\", \" list\", \" is\", \" by\", \" profession\", \" a\", \" actor\", \"?\\n\", \"1\", \".\", \" C\", \"anel\", \"o\", \" \\u00c1\", \"lv\", \"arez\", \"\\n\", \"2\", \".\", \" Bill\", \" Burr\", \"\\n\", \"3\", \".\", \" Son\", \" He\", \"ung\", \"-min\", \"\\n\", \"4\", \".\", \" Bill\", \"ie\", \" E\", \"il\", \"ish\", \"\\n\", \"5\", \".\", \" Morgan\", \" Freeman\", \"\\n\", \"6\", \".\", \" Jim\", \" Ac\", \"osta\", \"\\n\", \"Answer\", \":\"], \"values\": [0.01747436448931694, 0.004380988888442516, 0.0033139227889478207, 0.0007949828868731856, 0.0020924569107592106, 0.0012582301860675216, 0.0026930333115160465, 0.0014939308166503906, 0.0027706145774573088, 0.00635452289134264, 0.01916809007525444, 0.01589355431497097, 0.005049324128776789, 0.0024887085892260075, 0.0014175415271893144, 0.0013459206093102694, 0.0016790389781817794, 0.0011622548336163163, 0.0010623454581946135, 0.001603978918865323, 0.008767843246459961, 0.02273864671587944, 0.008885192684829235, 0.011921691708266735, 0.03301086276769638, 0.12441406399011612, 0.029487991705536842, 0.01387939415872097, 0.0012275695335119963, 0.00125207903329283, 0.00045011937618255615, 0.0006682634120807052, 0.00268193194642663, 0.0066589354537427425, 0.01669006422162056, 0.03144531324505806, 0.0017010688316076994, 0.005039977841079235, 0.001071500824764371, 0.003585123922675848, 0.00377311697229743, 0.00956115685403347, 0.0026046752464026213, 0.009752750396728516, 0.07283935695886612, 0.17158202826976776, 0.015621185302734375, 0.011053848080337048, 0.0067501068115234375, 0.003448009490966797, 0.0037899494636803865, 0.008716678246855736, 0.004413127899169922, 0.0379791259765625]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f5f96f30a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.subj,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c142db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['actor', 'singer', 'comedian', 'director', 'basketball player', 'football player', 'soccer player', 'tennis player', 'golfer', 'boxer', 'news anchor', 'journalist', 'author', 'fashion designer', 'entrepreneur', 'politician'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_prof.category_wise_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129fe333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from src.selection.utils import KeyedSet, get_first_token_id\n",
    "from src.selection.data import SelectionSample\n",
    "from src.functional import predict_next_token\n",
    "\n",
    "######################################################################\n",
    "N_DISTRACTORS = 5\n",
    "WINDOW_SPEC = {\n",
    "    mt.layer_name_format: 1,\n",
    "    mt.mlp_module_name_format: 9,\n",
    "    mt.attn_module_name_format: 9,\n",
    "}\n",
    "module_name_format = mt.layer_name_format\n",
    "# module_name_format = mt.mlp_module_name_format\n",
    "# module_name_format = mt.attn_module_name_format\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "def get_counterfactual_samples_on_pivot_entity(\n",
    "    task: SelectOneTask = select_prof,\n",
    "    patch_category: str | None = None,\n",
    "    clean_category: str | None = None,\n",
    "    shuffle_clean_options: bool = False,\n",
    "    prompt_template_idx=2,\n",
    "    option_style=\"numbered\",\n",
    "    filter_by_lm_prediction: bool = True,\n",
    "):\n",
    "    categories = list(task.category_wise_examples.keys())\n",
    "    if patch_category is None:\n",
    "        patch_category = random.choice(categories)\n",
    "\n",
    "    patch_subj, patch_obj = random.sample(\n",
    "        task.category_wise_examples[patch_category], 2\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Patch category: {patch_category}, subject: {patch_subj}, object: {patch_obj}\"\n",
    "    )\n",
    "\n",
    "    if clean_category is None:\n",
    "        clean_category = random.choice(list(set(categories) - {patch_category}))\n",
    "\n",
    "    clean_options = task.category_wise_examples[clean_category]\n",
    "    random.shuffle(clean_options)\n",
    "\n",
    "    clean_subj, clean_obj = random.sample(\n",
    "        (KeyedSet(clean_options, mt.tokenizer) - KeyedSet([patch_obj], mt.tokenizer)).values, 2\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Clean category: {clean_category}, subject: {clean_subj}, object: {clean_obj}\"\n",
    "    )\n",
    "\n",
    "    distractors = []\n",
    "    other_categories = random.sample(\n",
    "        list(set(categories) - {patch_category, clean_category}),\n",
    "        k=N_DISTRACTORS - 1,\n",
    "    )\n",
    "\n",
    "    for other_category in other_categories:\n",
    "        other_examples = task.category_wise_examples[other_category]\n",
    "        random.shuffle(other_examples)\n",
    "        other_examples = KeyedSet(other_examples, mt.tokenizer)\n",
    "        distractors.append(\n",
    "            random.choice(\n",
    "                (\n",
    "                    other_examples\n",
    "                    - KeyedSet(\n",
    "                        [patch_obj, clean_obj] + distractors, tokenizer=mt.tokenizer\n",
    "                    )\n",
    "                ).values\n",
    "            )\n",
    "        )\n",
    "\n",
    "    patch_options = [patch_obj, clean_obj] + distractors\n",
    "    random.shuffle(patch_options)\n",
    "    patch_obj_idx = patch_options.index(patch_obj)\n",
    "    logger.info(f\"{patch_obj_idx=} | {patch_options}\")\n",
    "\n",
    "    clean_options = copy.deepcopy(patch_options)\n",
    "\n",
    "    if shuffle_clean_options:\n",
    "        # Useful for the pointer experiments\n",
    "        while (\n",
    "            clean_options.index(clean_obj) == patch_obj_idx\n",
    "            or clean_options.index(patch_obj) == patch_obj_idx\n",
    "        ):\n",
    "            random.shuffle(clean_options)\n",
    "\n",
    "    clean_obj_idx = clean_options.index(clean_obj)\n",
    "\n",
    "    logger.info(f\"{clean_obj_idx=} | {clean_options}\")\n",
    "\n",
    "    kwargs = dict(\n",
    "        prompt_template= task.prompt_templates[prompt_template_idx],\n",
    "        default_option_style=option_style,\n",
    "    )\n",
    "\n",
    "    patch_sample = SelectionSample(\n",
    "        subj=patch_subj,\n",
    "        obj=patch_obj,\n",
    "        obj_idx=patch_obj_idx,\n",
    "        obj_token_id=get_first_token_id(patch_obj, mt.tokenizer, prefix=\" \"),\n",
    "        options=patch_options,\n",
    "        category=patch_category,\n",
    "        **kwargs,\n",
    "    )\n",
    "    clean_sample = SelectionSample(\n",
    "        subj=clean_subj,\n",
    "        obj=clean_obj,\n",
    "        obj_idx=clean_obj_idx,\n",
    "        obj_token_id=get_first_token_id(clean_obj, mt.tokenizer, prefix=\" \"),\n",
    "        options=clean_options,\n",
    "        category=clean_category,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    if filter_by_lm_prediction:\n",
    "        for sample in [patch_sample, clean_sample]:\n",
    "            pred = predict_next_token(\n",
    "                mt=mt,\n",
    "                inputs=sample.prompt(),\n",
    "            )[0]\n",
    "            logger.info(f\"{sample.subj} -> {sample.obj} | pred={[str(p) for p in pred]}\")\n",
    "            if pred[0].token_id != sample.obj_token_id:\n",
    "                logger.error(\n",
    "                    f'Prediction mismatch: {pred[0].token_id}[\"{mt.tokenizer.decode(pred[0].token_id)}\"] != {sample.obj_token_id}[\"{mt.tokenizer.decode(sample.obj_token_id)}\"]'\n",
    "                )\n",
    "                return get_counterfactual_samples_on_pivot_entity(\n",
    "                    task=task,\n",
    "                    patch_category=patch_category,\n",
    "                    clean_category=clean_category,\n",
    "                    shuffle_clean_options=shuffle_clean_options,\n",
    "                    prompt_template_idx=prompt_template_idx,\n",
    "                    option_style=option_style,\n",
    "                    filter_by_lm_prediction=filter_by_lm_prediction,\n",
    "                )\n",
    "            sample.prediction = pred\n",
    "\n",
    "    return patch_sample, clean_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "425f6285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:25:37 __main__ INFO     Patch category: politician, subject: Ron DeSantis, object: Ted Cruz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:25:37 __main__ INFO     Clean category: actor, subject: Will Smith, object: Ryan Reynolds\n",
      "2025-08-07 16:25:37 __main__ INFO     patch_obj_idx=1 | ['Ryan Reynolds', 'Ted Cruz', 'James Patterson', 'Gabriel Iglesias', 'Paulo Dybala', 'Diane von Furstenberg']\n",
      "2025-08-07 16:25:37 __main__ INFO     clean_obj_idx=0 | ['Ryan Reynolds', 'Ted Cruz', 'James Patterson', 'Gabriel Iglesias', 'Paulo Dybala', 'Diane von Furstenberg']\n"
     ]
    }
   ],
   "source": [
    "patch_sample, clean_sample = get_counterfactual_samples_on_pivot_entity(\n",
    "    patch_category=\"politician\",\n",
    "    clean_category=\"actor\",\n",
    "    filter_by_lm_prediction=False,\n",
    "    prompt_template_idx=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "510772fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Ryan Reynolds\n",
      "2. Ted Cruz\n",
      "3. James Patterson\n",
      "4. Gabriel Iglesias\n",
      "5. Paulo Dybala\n",
      "6. Diane von Furstenberg\n",
      "Who among these people mentioned above is a politician by profession?\n",
      "Answer: >> Ted Cruz\n",
      "0 patches to ablate possible answer information from options\n",
      "2025-08-07 16:25:41 src.experiments.utils DEBUG    Generated full answer: \" Ted Cruz\n",
      "The given list includes a variety of professions such as actors, authors, comedians, fashion designers, and athletes. However, the only\"\n",
      "2025-08-07 16:25:41 src.experiments.utils DEBUG    Predictions: ['\" Ted\"[23989] (p=0.742, logit=21.500)', '\" \"[220] (p=0.069, logit=19.125)', '\" (\"[320] (p=0.061, logit=19.000)', '\" The\"[578] (p=0.029, logit=18.250)', '\" Option\"[7104] (p=0.022, logit=18.000)']\n",
      "2025-08-07 16:25:41 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-e6603d83-7d2b\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-e6603d83-7d2b\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Ryan\", \" Reynolds\", \"\\n\", \"2\", \".\", \" Ted\", \" Cruz\", \"\\n\", \"3\", \".\", \" James\", \" Patterson\", \"\\n\", \"4\", \".\", \" Gabriel\", \" Ig\", \"les\", \"ias\", \"\\n\", \"5\", \".\", \" Paulo\", \" Dy\", \"b\", \"ala\", \"\\n\", \"6\", \".\", \" Diane\", \" von\", \" Fur\", \"sten\", \"berg\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" politician\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.013058471493422985, 0.003372764680534601, 0.0049644471146166325, 0.004219484515488148, 0.006548690609633923, 0.012136459350585938, 0.003525161650031805, 0.0380859375, 0.29973143339157104, 0.20947265625, 0.0036079406272619963, 0.0097808837890625, 0.004331016447395086, 0.001048612641170621, 0.004871082492172718, 0.004776954650878906, 0.0019145965343341231, 0.0013306618202477694, 0.0009287774446420372, 0.0005369737627916038, 0.0009183585643768311, 0.0031163692474365234, 0.0011121273273602128, 0.002170562744140625, 0.001545810722745955, 0.0006556451553478837, 0.0003840170684270561, 0.000665868807118386, 0.0010729789501056075, 0.0021016597747802734, 0.0017080307006835938, 0.0027820586692541838, 0.0009578049066476524, 0.0008604169124737382, 0.0003992948622908443, 0.0008244514465332031, 0.0018604279030114412, 0.007260131649672985, 0.010170364752411842, 0.007254934404045343, 0.0017353534931316972, 0.006099891848862171, 0.00581669807434082, 0.0021031617652624846, 0.0008749008411541581, 0.02627258375287056, 0.0043961526826024055, 0.0009419441339559853, 0.02271728590130806, 0.002187621546909213, 0.03392486646771431]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f5fb330d890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Ryan Reynolds\n",
      "2. Ted Cruz\n",
      "3. James Patterson\n",
      "4. Gabriel Iglesias\n",
      "5. Paulo Dybala\n",
      "6. Diane von Furstenberg\n",
      "Who among these people mentioned above is a actor by profession?\n",
      "Answer: >> Ryan Reynolds\n",
      "0 patches to ablate possible answer information from options\n",
      "2025-08-07 16:25:45 src.experiments.utils DEBUG    Generated full answer: \" Ryan Reynolds and Gabriel Iglesias are actors by profession. Ryan Reynolds is a Canadian actor, film producer, and screenwriter, while Gabriel Igles\"\n",
      "2025-08-07 16:25:45 src.experiments.utils DEBUG    Predictions: ['\" Ryan\"[13960] (p=0.602, logit=20.000)', '\" Among\"[22395] (p=0.056, logit=17.625)', '\" Option\"[7104] (p=0.049, logit=17.500)', '\" \"[220] (p=0.043, logit=17.375)', '\" The\"[578] (p=0.043, logit=17.375)']\n",
      "2025-08-07 16:25:45 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-0559fd45-eb36\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-0559fd45-eb36\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Ryan\", \" Reynolds\", \"\\n\", \"2\", \".\", \" Ted\", \" Cruz\", \"\\n\", \"3\", \".\", \" James\", \" Patterson\", \"\\n\", \"4\", \".\", \" Gabriel\", \" Ig\", \"les\", \"ias\", \"\\n\", \"5\", \".\", \" Paulo\", \" Dy\", \"b\", \"ala\", \"\\n\", \"6\", \".\", \" Diane\", \" von\", \" Fur\", \"sten\", \"berg\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" actor\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.02037658728659153, 0.007564735598862171, 0.08548126369714737, 0.15031738579273224, 0.06752319633960724, 0.025524521246552467, 0.04153137281537056, 0.0069175721146166325, 0.0024894713424146175, 0.015337372198700905, 0.005216789431869984, 0.0070518492721021175, 0.00985107384622097, 0.002494716551154852, 0.004901695065200329, 0.00353584298864007, 0.006855010986328125, 0.02273101732134819, 0.004332923796027899, 0.004912757780402899, 0.06719970703125, 0.05173339694738388, 0.0027082443702965975, 0.0036407471634447575, 0.0009227752452716231, 0.0012790679465979338, 0.00038814841536805034, 0.0031599998474121094, 0.004192161373794079, 0.002530002500861883, 0.0026252747047692537, 0.002267169998958707, 0.0006922691827639937, 0.01473250426352024, 0.0003631003201007843, 0.000741678464692086, 0.0011227249633520842, 0.00514564523473382, 0.0076105473563075066, 0.010307693853974342, 0.0035077095963060856, 0.0031251907348632812, 0.006120014004409313, 0.004782604984939098, 0.0036628723610192537, 0.03931274265050888, 0.003279399825260043, 0.001719570136629045, 0.02568645402789116, 0.00223156507126987, 0.036847688257694244]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f5f960e7b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "for sample in [patch_sample, clean_sample]:\n",
    "    print(sample.prompt(), \">>\", sample.obj)\n",
    "    attn_pattern = verify_head_patterns(\n",
    "        prompt=sample.prompt(),\n",
    "        options=sample.options,\n",
    "        pivot=sample.subj,\n",
    "        mt=mt,\n",
    "        heads=HEADS,\n",
    "        generate_full_answer=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca19f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 8192)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
       "          (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
       "          (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((8192,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=128256, bias=False)\n",
       "  (generator): Generator(\n",
       "    (streamer): Streamer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf72a519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:26:54 src.hooking.llama_attention DEBUG    LlamaAttentionPatcher <> model.layers.33.self_attn\n",
      "2025-08-07 16:26:54 src.hooking.llama_attention DEBUG    hidden_shape=(1, 52, -1, 128) | input_shape=torch.Size([1, 52]) | torch.Size([1, 52, 8192])\n",
      "2025-08-07 16:26:54 src.hooking.llama_attention DEBUG    query_states.size()=torch.Size([1, 64, 52, 128]) | key_states.size()=torch.Size([1, 8, 52, 128]) | value_states.size()=torch.Size([1, 8, 52, 128])\n",
      "2025-08-07 16:26:55 src.hooking.llama_attention DEBUG    LlamaAttentionPatcher <> model.layers.33.self_attn\n",
      "2025-08-07 16:26:55 src.hooking.llama_attention DEBUG    hidden_shape=(1, 52, -1, 128) | input_shape=torch.Size([1, 52]) | torch.Size([1, 52, 8192])\n",
      "2025-08-07 16:26:55 src.hooking.llama_attention DEBUG    query_states.size()=torch.Size([1, 64, 52, 128]) | key_states.size()=torch.Size([1, 8, 52, 128]) | value_states.size()=torch.Size([1, 8, 52, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 52, 128]),\n",
       " torch.Size([1, 64, 52, 128]),\n",
       " torch.Size([1, 52, 8192]),\n",
       " torch.Size([1, 52, 8192]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import baukit\n",
    "from src.functional import get_module_nnsight, PatchSpec\n",
    "from src.hooking.llama_attention import LlamaAttentionPatcher\n",
    "import types\n",
    "from typing import Literal\n",
    "from src.tokens import prepare_input\n",
    "\n",
    "\n",
    "def set_attn_implementation(mt, attn_implementation: Literal[\"sdpa\", \"eager\"]):\n",
    "    mt.config._attn_implementation = attn_implementation\n",
    "    for layer_idx in range(mt.config.num_hidden_layers):\n",
    "        attn_block_name = mt.attn_module_name_format.format(layer_idx)\n",
    "        attn_block = baukit.get_module(mt._model, attn_block_name)\n",
    "        attn_block.config._attn_implementation = attn_implementation\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "batch_size = 1  # tokenized.input_ids.shape[0]\n",
    "n_heads = mt.config.num_attention_heads\n",
    "head_dim = mt.n_embd // n_heads\n",
    "query_idx = -1 # almost always the last token\n",
    "###################################################################################\n",
    "\n",
    "mt.reset_forward()\n",
    "set_attn_implementation(mt, \"sdpa\")\n",
    "\n",
    "layer_idx, head_idx = HEADS[0]\n",
    "\n",
    "attn_block_name = mt.attn_module_name_format.format(layer_idx)\n",
    "attn_block = baukit.get_module(mt._model, attn_block_name)\n",
    "attn_block.forward = types.MethodType(\n",
    "    LlamaAttentionPatcher(block_name=attn_block_name),\n",
    "    attn_block,\n",
    ")\n",
    "\n",
    "patch_tokenized = prepare_input(prompts=patch_sample.prompt(), tokenizer=mt)\n",
    "patch_seq_len = patch_tokenized.input_ids.shape[1]\n",
    "input_ln = mt.layer_name_format.format(layer_idx) + \".input_layernorm\"\n",
    "\n",
    "with mt.trace(patch_tokenized) as trace:\n",
    "    ln_module = get_module_nnsight(mt, input_ln)\n",
    "    patch_ln = ln_module.output.save()\n",
    "\n",
    "    q_proj_name = mt.attn_module_name_format.format(layer_idx) + \".q_proj\"\n",
    "    q_proj_module = get_module_nnsight(mt, q_proj_name)\n",
    "    patch_q_proj = q_proj_module.output.view(batch_size, patch_seq_len, n_heads, head_dim).transpose(1, 2).save()\n",
    "    # patch_q_proj = PatchSpec(\n",
    "    #     location=(q_proj_name + f\".{head_idx}\", -1),\n",
    "    #     patch=patch_q_proj[:, head_idx, query_idx, :].squeeze().save()\n",
    "    # )\n",
    "\n",
    "clean_tokenized = prepare_input(prompts=clean_sample.prompt(), tokenizer=mt)\n",
    "clean_seq_len = clean_tokenized.input_ids.shape[1]\n",
    "with mt.trace(clean_tokenized) as trace:\n",
    "    ln_module = get_module_nnsight(mt, input_ln)\n",
    "    clean_ln = ln_module.output.save()\n",
    "\n",
    "    q_proj_name = mt.attn_module_name_format.format(layer_idx) + \".q_proj\"\n",
    "    q_proj_module = get_module_nnsight(mt, q_proj_name)\n",
    "    clean_q_proj = q_proj_module.output.view(batch_size, clean_seq_len, n_heads, head_dim).transpose(1, 2).save()\n",
    "    # clean_q_proj = PatchSpec(\n",
    "    #     location=(q_proj_name + f\".{head_idx}\", -1),\n",
    "    #     patch=clean_q_proj[:, head_idx, query_idx, :].squeeze().save()\n",
    "    # )\n",
    "\n",
    "mt.reset_forward()\n",
    "set_attn_implementation(mt, \"eager\")\n",
    "\n",
    "patch_q_proj.shape, clean_q_proj.shape, patch_ln.shape, clean_ln.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ccebe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(patch_sample.prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a13e7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_sample.prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b92a102f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(\n",
    "    patch_q_proj[:, head_idx, query_idx, :], \n",
    "    clean_q_proj[:, head_idx, query_idx, :],\n",
    "    atol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9042b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(\n",
    "    patch_ln[:, query_idx, :], \n",
    "    clean_ln[:, query_idx, :],\n",
    "    atol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49583481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0544, -0.0879,  0.0996,  ..., -0.1147,  0.4746,  0.0728]],\n",
       "        device='cuda:3', dtype=torch.bfloat16, grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.0056, -0.1758,  0.0693,  ..., -0.0752,  0.4062,  0.0608]],\n",
       "        device='cuda:3', dtype=torch.bfloat16, grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_ln[:, query_idx, :], clean_ln[:, query_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2022593f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 52, 8192])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_ln.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04ead16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 52, 8192]) torch.Size([1, 52, 8192])\n",
      "False\n",
      "torch.Size([1, 64, 52, 128]) torch.Size([1, 64, 52, 128])\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n",
      "45 False\n"
     ]
    }
   ],
   "source": [
    "# manual calculation\n",
    "attn_module = baukit.get_module(mt._model, mt.attn_module_name_format.format(layer_idx))\n",
    "patch_q_proj_manual = attn_module.q_proj(patch_ln)\n",
    "clean_q_proj_manual = attn_module.q_proj(clean_ln)\n",
    "\n",
    "print(patch_q_proj_manual.shape, clean_q_proj_manual.shape)\n",
    "print(torch.allclose(\n",
    "    patch_q_proj_manual[:, query_idx, :], \n",
    "    clean_q_proj_manual[:, query_idx, :],\n",
    "    atol=1e-3\n",
    "))\n",
    "\n",
    "patch_q_proj_manual = patch_q_proj_manual.reshape(batch_size, patch_seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "clean_q_proj_manual = clean_q_proj_manual.reshape(batch_size, clean_seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "print(patch_q_proj_manual.shape, clean_q_proj_manual.shape)\n",
    "\n",
    "for idx in range(n_heads):\n",
    "    print(head_idx, torch.allclose(\n",
    "        patch_q_proj_manual[:, idx, query_idx, :], \n",
    "        clean_q_proj_manual[:, idx, query_idx, :],\n",
    "        atol=1e-3\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "098535e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(\n",
    "    patch_q_proj_manual[:, head_idx, query_idx, :], \n",
    "    patch_q_proj[:, head_idx, query_idx, :],\n",
    "    atol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6de9135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "torch.Size([1, 64, 52, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('model.layers.33.self_attn.q_proj', 45, -1),\n",
       " ('model.layers.33.self_attn.q_proj', -1))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_q_proj = clean_q_proj.clone()\n",
    "replace_q_proj[:, head_idx, query_idx, :] = patch_q_proj[:, head_idx, query_idx, :]\n",
    "\n",
    "print(torch.allclose(\n",
    "        replace_q_proj[:, head_idx, query_idx, :],\n",
    "        clean_q_proj[:, head_idx, query_idx, :],\n",
    "        atol=1e-3\n",
    "    )\n",
    ")\n",
    "print(replace_q_proj.shape)\n",
    "\n",
    "replace_q_proj = replace_q_proj.transpose(1, 2).reshape(batch_size, clean_seq_len, -1)\n",
    "\n",
    "rep_patch = PatchSpec(\n",
    "    location=(q_proj_name, -1),\n",
    "    patch=replace_q_proj[:, -1, :].squeeze(),\n",
    ")\n",
    "\n",
    "ln_patch = PatchSpec(\n",
    "    location=(input_ln, -1),\n",
    "    patch=patch_ln[:, query_idx, :].squeeze(),\n",
    ")\n",
    "\n",
    "head_q_patch = PatchSpec(\n",
    "    location=(q_proj_name, head_idx, -1),\n",
    "    patch=patch_q_proj[:, head_idx, query_idx, :].squeeze(),\n",
    ")\n",
    "\n",
    "head_q_patch.location, rep_patch.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3d26910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_q_proj_rs = clean_q_proj.view(batch_size, clean_seq_len, -1)\n",
    "# patch_q_proj_rs = patch_q_proj.view(batch_size, clean_seq_len, -1)\n",
    "# clean_q_proj_rs[:, -1, :].shape, patch_q_proj_rs[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95232df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.allclose(\n",
    "#     replace_q_proj[:, -1, :].squeeze(), \n",
    "#     # patch_q_proj_rs[:, -1, :].squeeze(),\n",
    "#     clean_q_proj_rs[:, -1, :].squeeze(), \n",
    "#     atol=1e-3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7cf44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9835953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 52, 128]), torch.Size([1, 64, 52, 128]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_q_proj.shape, clean_q_proj.shape\n",
    "# torch.allclose(patch_q_proj.patch, clean_q_proj.patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "759c4ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-8356cf61-ae9d\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-8356cf61-ae9d\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Ryan\", \" Reynolds\", \"\\n\", \"2\", \".\", \" Ted\", \" Cruz\", \"\\n\", \"3\", \".\", \" James\", \" Patterson\", \"\\n\", \"4\", \".\", \" Gabriel\", \" Ig\", \"les\", \"ias\", \"\\n\", \"5\", \".\", \" Paulo\", \" Dy\", \"b\", \"ala\", \"\\n\", \"6\", \".\", \" Diane\", \" von\", \" Fur\", \"sten\", \"berg\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" actor\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.026123046875, 0.00701904296875, 0.0242919921875, 0.07861328125, 0.0235595703125, 0.03271484375, 0.020751953125, 0.01373291015625, 0.006378173828125, 0.0177001953125, 0.00872802734375, 0.005615234375, 0.01708984375, 0.0074462890625, 0.00262451171875, 0.008056640625, 0.007171630859375, 0.01470947265625, 0.0059814453125, 0.00579833984375, 0.0189208984375, 0.01165771484375, 0.004669189453125, 0.0024566650390625, 0.00341796875, 0.0048828125, 0.002105712890625, 0.00311279296875, 0.00188446044921875, 0.003875732421875, 0.005126953125, 0.005889892578125, 0.00213623046875, 0.0213623046875, 0.0022430419921875, 0.00104522705078125, 0.0016937255859375, 0.0130615234375, 0.0096435546875, 0.0113525390625, 0.00335693359375, 0.0059814453125, 0.0076904296875, 0.0107421875, 0.0035247802734375, 0.03125, 0.004730224609375, 0.00537109375, 0.062255859375, 0.00482177734375, 0.0303955078125]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f5f96f2f550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Ryan', prob=0.6015625, logit=20.0, token_id=13960, metadata=None),\n",
       " PredictedToken(token=' Among', prob=0.055908203125, logit=17.625, token_id=22395, metadata=None),\n",
       " PredictedToken(token=' Option', prob=0.04931640625, logit=17.5, token_id=7104, metadata=None),\n",
       " PredictedToken(token=' ', prob=0.04345703125, logit=17.375, token_id=220, metadata=None),\n",
       " PredictedToken(token=' The', prob=0.04345703125, logit=17.375, token_id=578, metadata=None)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.attention import get_attention_matrices, visualize_attn_matrix\n",
    "from src.functional import interpret_logits\n",
    "\n",
    "clean_tokenized = prepare_input(prompts=clean_sample.prompt(), tokenizer=mt)\n",
    "\n",
    "attn_info = get_attention_matrices(\n",
    "    input=clean_tokenized,\n",
    "    mt=mt,\n",
    ")\n",
    "\n",
    "attn_matrix = attn_info.attention_matrices[layer_idx, head_idx].squeeze()\n",
    "visualize_attn_matrix(\n",
    "    attn_matrix=attn_matrix,\n",
    "    tokens=[mt.tokenizer.decode(t) for t in clean_tokenized.input_ids[0]],\n",
    "    q_index=-1,\n",
    ")\n",
    "\n",
    "interpret_logits(\n",
    "    tokenizer=mt,\n",
    "    logits=attn_info.logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "136268c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.33.input_layernorm -1 None replace\n",
      "is working\n",
      "log torch.Size([1, 52, 8192]) ('model.layers.33.input_layernorm', -1) torch.Size([8192])\n",
      ">> false\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-22b89270-d820\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-22b89270-d820\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Ryan\", \" Reynolds\", \"\\n\", \"2\", \".\", \" Ted\", \" Cruz\", \"\\n\", \"3\", \".\", \" James\", \" Patterson\", \"\\n\", \"4\", \".\", \" Gabriel\", \" Ig\", \"les\", \"ias\", \"\\n\", \"5\", \".\", \" Paulo\", \" Dy\", \"b\", \"ala\", \"\\n\", \"6\", \".\", \" Diane\", \" von\", \" Fur\", \"sten\", \"berg\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" actor\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.0218505859375, 0.004364013671875, 0.0096435546875, 0.01190185546875, 0.00970458984375, 0.0223388671875, 0.00823974609375, 0.033935546875, 0.14453125, 0.0859375, 0.004486083984375, 0.0098876953125, 0.007476806640625, 0.00390625, 0.0019683837890625, 0.01025390625, 0.0030364990234375, 0.007354736328125, 0.005584716796875, 0.00384521484375, 0.00408935546875, 0.00872802734375, 0.0031890869140625, 0.0050048828125, 0.009521484375, 0.00421142578125, 0.0031890869140625, 0.002410888671875, 0.00173187255859375, 0.004486083984375, 0.0034942626953125, 0.00897216796875, 0.003662109375, 0.00173187255859375, 0.0031890869140625, 0.00244140625, 0.0042724609375, 0.017578125, 0.00933837890625, 0.00994873046875, 0.00384521484375, 0.0062255859375, 0.007080078125, 0.004425048828125, 0.0012054443359375, 0.00122833251953125, 0.000545501708984375, 0.001678466796875, 0.005767822265625, 0.0021820068359375, 0.033935546875]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f5fb34d8710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Ryan', prob=0.63671875, logit=20.125, token_id=13960, metadata=None),\n",
       " PredictedToken(token=' Among', prob=0.046142578125, logit=17.5, token_id=22395, metadata=None),\n",
       " PredictedToken(token=' Option', prob=0.046142578125, logit=17.5, token_id=7104, metadata=None),\n",
       " PredictedToken(token=' ', prob=0.040771484375, logit=17.375, token_id=220, metadata=None),\n",
       " PredictedToken(token=' (', prob=0.040771484375, logit=17.375, token_id=320, metadata=None)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.attention import get_attention_matrices, visualize_attn_matrix\n",
    "\n",
    "clean_tokenized = prepare_input(prompts=clean_sample.prompt(), tokenizer=mt)\n",
    "\n",
    "patched_attn_info = get_attention_matrices(\n",
    "    input=clean_tokenized,\n",
    "    mt=mt,\n",
    "    patches=[ln_patch],\n",
    "    # patches = [head_q_patch]\n",
    "    # patches = [rep_patch]\n",
    ")\n",
    "\n",
    "patched_attn_matrix = patched_attn_info.attention_matrices[layer_idx, head_idx].squeeze()\n",
    "visualize_attn_matrix(\n",
    "    attn_matrix=patched_attn_matrix,\n",
    "    tokens=[mt.tokenizer.decode(t) for t in clean_tokenized.input_ids[0]],\n",
    "    q_index=-1,\n",
    ")\n",
    "\n",
    "interpret_logits(\n",
    "    tokenizer=mt,\n",
    "    logits=patched_attn_info.logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba7faf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model.layers.33.input_layernorm', -1) torch.Size([8192])\n",
      "('model.layers.33.self_attn.q_proj', 45, -1) torch.Size([128])\n",
      "('model.layers.33.self_attn.q_proj', -1) torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "for patch in [ln_patch, head_q_patch, rep_patch]:\n",
    "    print(patch.location, patch.patch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39978035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep_patch.location = ln_patch.location #! the problem is with the location, not the patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06d6da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.33.input_layernorm -1 None replace\n",
      "is working\n",
      "log torch.Size([1, 52, 8192]) ('model.layers.33.input_layernorm', -1) torch.Size([8192])\n",
      ">> false\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-ac63ef3c-68db\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-ac63ef3c-68db\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Ryan\", \" Reynolds\", \"\\n\", \"2\", \".\", \" Ted\", \" Cruz\", \"\\n\", \"3\", \".\", \" James\", \" Patterson\", \"\\n\", \"4\", \".\", \" Gabriel\", \" Ig\", \"les\", \"ias\", \"\\n\", \"5\", \".\", \" Paulo\", \" Dy\", \"b\", \"ala\", \"\\n\", \"6\", \".\", \" Diane\", \" von\", \" Fur\", \"sten\", \"berg\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" actor\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.0037384033203125, 0.011962890625, 0.005096435546875, 0.005828857421875, 0.00726318359375, 0.01470947265625, 0.004638671875, 0.0025787353515625, 0.0030059814453125, 0.00543212890625, 0.00689697265625, 0.0011138916015625, 0.0015716552734375, 9.202957153320312e-05, 0.000507354736328125, 0.006317138671875, 0.00909423828125, 0.00153350830078125, 0.0007476806640625, 0.00171661376953125, 0.00170135498046875, 0.004730224609375, 0.003173828125, 0.00145721435546875, 0.000583648681640625, 0.0014801025390625, 0.0004825592041015625, 0.00095367431640625, 0.005645751953125, 0.025390625, 0.003021240234375, 0.00152587890625, 0.000400543212890625, 0.031982421875, 0.000400543212890625, 0.00141143798828125, 0.0111083984375, 0.006683349609375, 0.008056640625, 0.006011962890625, 0.0011444091796875, 0.0034332275390625, 0.004486083984375, 0.0093994140625, 0.018798828125, 0.0166015625, 0.0028076171875, 0.006134033203125, 0.01611328125, 0.00191497802734375, 0.703125]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f5f48524310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Ryan', prob=0.57421875, logit=18.5, token_id=13960, metadata=None),\n",
       " PredictedToken(token=' Ted', prob=0.07763671875, logit=16.5, token_id=23989, metadata=None),\n",
       " PredictedToken(token=' (', prob=0.047119140625, logit=16.0, token_id=320, metadata=None),\n",
       " PredictedToken(token=' ', prob=0.03662109375, logit=15.75, token_id=220, metadata=None),\n",
       " PredictedToken(token=' The', prob=0.032470703125, logit=15.625, token_id=578, metadata=None)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from src.attention import get_attention_matrices, visualize_attn_matrix\n",
    "\n",
    "clean_tokenized = prepare_input(prompts=clean_sample.prompt(), tokenizer=mt)\n",
    "\n",
    "patched_attn_info = get_attention_matrices(\n",
    "    input=clean_tokenized,\n",
    "    mt=mt,\n",
    "    # patches = [head_q_patch]\n",
    "    patches = [rep_patch]\n",
    ")\n",
    "\n",
    "patched_attn_matrix = patched_attn_info.attention_matrices[layer_idx, head_idx].squeeze()\n",
    "visualize_attn_matrix(\n",
    "    attn_matrix=patched_attn_matrix,\n",
    "    tokens=[mt.tokenizer.decode(t) for t in clean_tokenized.input_ids[0]],\n",
    "    q_index=-1,\n",
    ")\n",
    "\n",
    "interpret_logits(\n",
    "    tokenizer=mt,\n",
    "    logits=patched_attn_info.logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870e69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
