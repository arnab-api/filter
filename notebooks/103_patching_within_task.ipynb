{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ec34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfae7235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:12:53 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-08-08 01:12:53 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-08-08 01:12:53 __main__ INFO     transformers.__version__='4.54.1'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7720ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:12:56 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-08 01:12:56 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-08 01:12:57 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-08-08 01:12:57 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683855df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:12:58 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-08-08 01:12:58 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:12:58 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-08-08 01:12:58 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-08-08 01:12:58 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/models/meta-llama/Llama-3.3-70B-Instruct/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe4ecf6e17b45b4a49593d243207cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:13:47 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-08-08 01:13:47 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n",
      "2025-08-08 01:13:47 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62d97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectOneTask: (profession of a famous person)\n",
      "Categories: actor(20), singer(20), comedian(20), director(20), basketball player(20), football player(20), soccer player(20), tennis player(20), golfer(20), boxer(20), news anchor(20), journalist(20), author(20), fashion designer(20), entrepreneur(19), politician(20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOneTask\n",
    "\n",
    "select_prof = SelectOneTask.load(\n",
    "    path=os.path.join(env_utils.DEFAULT_DATA_DIR, \"selection\", \"profession.json\")\n",
    ")\n",
    "\n",
    "print(select_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45087a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leonardo DiCaprio -> Chris Hemsworth (4): ['Kawhi Leonard', 'Andrey Rublev', 'Barack Obama', 'Errol Spence Jr.', 'Chris Hemsworth', 'Vinícius Júnior']\n"
     ]
    }
   ],
   "source": [
    "sample = select_prof.get_random_sample(\n",
    "    mt = mt,\n",
    "    prompt_template_idx=2,\n",
    "    option_style=\"numbered\",\n",
    "    category=\"actor\"\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66399b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Which person from the following list is by profession a actor?\n",
      "1. Kawhi Leonard\n",
      "2. Andrey Rublev\n",
      "3. Barack Obama\n",
      "4. Errol Spence Jr.\n",
      "5. Chris Hemsworth\n",
      "6. Vinícius Júnior\n",
      "Answer:\" >> Chris Hemsworth\n"
     ]
    }
   ],
   "source": [
    "# sample.prompt_template = select_prof.prompt_templates[3]\n",
    "\n",
    "print(f'\"{sample.prompt()}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f57d67c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which person from the following list is by profession a actor?\n",
      "Options: Kawhi Leonard, Andrey Rublev, Barack Obama, Errol Spence Jr., Chris Hemsworth, Vinícius Júnior.\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(sample.prompt(option_style=\"single_line\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0499b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Chris Hemsworth\n",
      "Explanation: Chris Hemsworth is an Australian actor. He rose to prominence playing Kim\" >> Chris Hemsworth\n"
     ]
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")[0]\n",
    "print(f'\"{gen}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a9a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patches to ablate possible answer information from options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'])\n",
      "2025-08-08 01:13:58 src.experiments.utils DEBUG    Predictions: ['\" Chris\"[11517] (p=0.578, logit=20.875)', '\" \"[220] (p=0.188, logit=19.750)', '\" The\"[578] (p=0.146, logit=19.500)', '\" Option\"[7104] (p=0.025, logit=17.750)', '\" (\"[320] (p=0.017, logit=17.375)']\n",
      "2025-08-08 01:13:58 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-5068a234-97c8\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-5068a234-97c8\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"Which\", \" person\", \" from\", \" the\", \" following\", \" list\", \" is\", \" by\", \" profession\", \" a\", \" actor\", \"?\\n\", \"1\", \".\", \" Kaw\", \"hi\", \" Leonard\", \"\\n\", \"2\", \".\", \" And\", \"rey\", \" Rub\", \"lev\", \"\\n\", \"3\", \".\", \" Barack\", \" Obama\", \"\\n\", \"4\", \".\", \" Er\", \"rol\", \" Sp\", \"ence\", \" Jr\", \".\\n\", \"5\", \".\", \" Chris\", \" Hem\", \"sworth\", \"\\n\", \"6\", \".\", \" Vin\", \"\\u00ed\", \"ci\", \"us\", \" J\", \"\\u00fan\", \"ior\", \"\\n\", \"Answer\", \":\"], \"values\": [0.01615295372903347, 0.00620956439524889, 0.0046516419388353825, 0.0007609844324178994, 0.001997280167415738, 0.0015493392711505294, 0.0037004470359534025, 0.0016414641868323088, 0.0037420273292809725, 0.00584335345774889, 0.02463684044778347, 0.01494445838034153, 0.005631637759506702, 0.0017543792491778731, 0.0012140274047851562, 0.0028192519675940275, 0.0028841972816735506, 0.012142372317612171, 0.019106293097138405, 0.00691146869212389, 0.001887714839540422, 0.0013400912284851074, 0.0006933510303497314, 0.0016467273235321045, 0.003046420169994235, 0.001953697297722101, 0.0066123963333666325, 0.004516196437180042, 0.005257225129753351, 0.01228256244212389, 0.016663361340761185, 0.010409546084702015, 0.01089620590209961, 0.0064390660263597965, 0.007507515139877796, 0.0018671154975891113, 0.0025457083247601986, 0.00211007590405643, 0.01802673377096653, 0.004209327511489391, 0.04099883884191513, 0.04072265699505806, 0.1514892578125, 0.22470703721046448, 0.017009735107421875, 0.003814124967902899, 0.006560134701430798, 0.0008160829311236739, 0.0002735614834818989, 0.0005344569799490273, 0.00026287586661055684, 0.00018361955881118774, 0.0011103451251983643, 0.0014071285258978605, 0.005816268734633923, 0.02713928185403347]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f39600ca0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.attention import get_attention_matrices\n",
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.subj,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c142db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['actor', 'singer', 'comedian', 'director', 'basketball player', 'football player', 'soccer player', 'tennis player', 'golfer', 'boxer', 'news anchor', 'journalist', 'author', 'fashion designer', 'entrepreneur', 'politician'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_prof.category_wise_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "129fe333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from src.selection.utils import KeyedSet, get_first_token_id\n",
    "from src.selection.data import SelectionSample\n",
    "from src.functional import predict_next_token\n",
    "\n",
    "######################################################################\n",
    "N_DISTRACTORS = 5\n",
    "WINDOW_SPEC = {\n",
    "    mt.layer_name_format: 1,\n",
    "    mt.mlp_module_name_format: 9,\n",
    "    mt.attn_module_name_format: 9,\n",
    "}\n",
    "module_name_format = mt.layer_name_format\n",
    "# module_name_format = mt.mlp_module_name_format\n",
    "# module_name_format = mt.attn_module_name_format\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "def get_counterfactual_samples_on_pivot_entity(\n",
    "    task: SelectOneTask = select_prof,\n",
    "    patch_category: str | None = None,\n",
    "    clean_category: str | None = None,\n",
    "    shuffle_clean_options: bool = False,\n",
    "    prompt_template_idx=2,\n",
    "    option_style=\"numbered\",\n",
    "    filter_by_lm_prediction: bool = True,\n",
    "):\n",
    "    categories = list(task.category_wise_examples.keys())\n",
    "    if patch_category is None:\n",
    "        patch_category = random.choice(categories)\n",
    "\n",
    "    patch_subj, patch_obj = random.sample(\n",
    "        task.category_wise_examples[patch_category], 2\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Patch category: {patch_category}, subject: {patch_subj}, object: {patch_obj}\"\n",
    "    )\n",
    "\n",
    "    if clean_category is None:\n",
    "        clean_category = random.choice(list(set(categories) - {patch_category}))\n",
    "\n",
    "    clean_options = task.category_wise_examples[clean_category]\n",
    "    random.shuffle(clean_options)\n",
    "\n",
    "    clean_subj, clean_obj = random.sample(\n",
    "        (KeyedSet(clean_options, mt.tokenizer) - KeyedSet([patch_obj], mt.tokenizer)).values, 2\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Clean category: {clean_category}, subject: {clean_subj}, object: {clean_obj}\"\n",
    "    )\n",
    "\n",
    "    distractors = []\n",
    "    other_categories = random.sample(\n",
    "        list(set(categories) - {patch_category, clean_category}),\n",
    "        k=N_DISTRACTORS - 1,\n",
    "    )\n",
    "\n",
    "    for other_category in other_categories:\n",
    "        other_examples = task.category_wise_examples[other_category]\n",
    "        random.shuffle(other_examples)\n",
    "        other_examples = KeyedSet(other_examples, mt.tokenizer)\n",
    "        distractors.append(\n",
    "            random.choice(\n",
    "                (\n",
    "                    other_examples\n",
    "                    - KeyedSet(\n",
    "                        [patch_obj, clean_obj] + distractors, tokenizer=mt.tokenizer\n",
    "                    )\n",
    "                ).values\n",
    "            )\n",
    "        )\n",
    "\n",
    "    patch_options = [patch_obj, clean_obj] + distractors\n",
    "    random.shuffle(patch_options)\n",
    "    patch_obj_idx = patch_options.index(patch_obj)\n",
    "    logger.info(f\"{patch_obj_idx=} | {patch_options}\")\n",
    "\n",
    "    clean_options = copy.deepcopy(patch_options)\n",
    "\n",
    "    if shuffle_clean_options:\n",
    "        # Useful for the pointer experiments\n",
    "        while (\n",
    "            clean_options.index(clean_obj) == patch_obj_idx\n",
    "            or clean_options.index(patch_obj) == patch_obj_idx\n",
    "        ):\n",
    "            random.shuffle(clean_options)\n",
    "\n",
    "    clean_obj_idx = clean_options.index(clean_obj)\n",
    "\n",
    "    logger.info(f\"{clean_obj_idx=} | {clean_options}\")\n",
    "\n",
    "    kwargs = dict(\n",
    "        prompt_template= task.prompt_templates[prompt_template_idx],\n",
    "        default_option_style=option_style,\n",
    "    )\n",
    "\n",
    "    patch_sample = SelectionSample(\n",
    "        subj=patch_subj,\n",
    "        obj=patch_obj,\n",
    "        obj_idx=patch_obj_idx,\n",
    "        obj_token_id=get_first_token_id(patch_obj, mt.tokenizer, prefix=\" \"),\n",
    "        options=patch_options,\n",
    "        category=patch_category,\n",
    "        **kwargs,\n",
    "    )\n",
    "    clean_sample = SelectionSample(\n",
    "        subj=clean_subj,\n",
    "        obj=clean_obj,\n",
    "        obj_idx=clean_obj_idx,\n",
    "        obj_token_id=get_first_token_id(clean_obj, mt.tokenizer, prefix=\" \"),\n",
    "        options=clean_options,\n",
    "        category=clean_category,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    if filter_by_lm_prediction:\n",
    "        for sample in [patch_sample, clean_sample]:\n",
    "            pred = predict_next_token(\n",
    "                mt=mt,\n",
    "                inputs=sample.prompt(),\n",
    "            )[0]\n",
    "            logger.info(f\"{sample.subj} -> {sample.obj} | pred={[str(p) for p in pred]}\")\n",
    "            if pred[0].token_id != sample.obj_token_id:\n",
    "                logger.error(\n",
    "                    f'Prediction mismatch: {pred[0].token_id}[\"{mt.tokenizer.decode(pred[0].token_id)}\"] != {sample.obj_token_id}[\"{mt.tokenizer.decode(sample.obj_token_id)}\"]'\n",
    "                )\n",
    "                return get_counterfactual_samples_on_pivot_entity(\n",
    "                    task=task,\n",
    "                    patch_category=patch_category,\n",
    "                    clean_category=clean_category,\n",
    "                    shuffle_clean_options=shuffle_clean_options,\n",
    "                    prompt_template_idx=prompt_template_idx,\n",
    "                    option_style=option_style,\n",
    "                    filter_by_lm_prediction=filter_by_lm_prediction,\n",
    "                )\n",
    "            sample.prediction = pred\n",
    "\n",
    "    return patch_sample, clean_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "425f6285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:17:29 __main__ INFO     Patch category: politician, subject: Marco Rubio, object: Bernie Sanders\n",
      "2025-08-08 01:17:29 __main__ INFO     Clean category: actor, subject: Meryl Streep, object: Julia Roberts\n",
      "2025-08-08 01:17:30 __main__ INFO     patch_obj_idx=1 | ['Julia Roberts', 'Bernie Sanders', 'Dmitry Bivol', 'Simona Halep', 'Gillian Flynn', 'Tim Cook']\n",
      "2025-08-08 01:17:30 __main__ INFO     clean_obj_idx=0 | ['Julia Roberts', 'Bernie Sanders', 'Dmitry Bivol', 'Simona Halep', 'Gillian Flynn', 'Tim Cook']\n"
     ]
    }
   ],
   "source": [
    "patch_sample, clean_sample = get_counterfactual_samples_on_pivot_entity(\n",
    "    patch_category=\"politician\",\n",
    "    clean_category=\"actor\",\n",
    "    filter_by_lm_prediction=False,\n",
    "    prompt_template_idx=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "510772fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Julia Roberts\n",
      "2. Bernie Sanders\n",
      "3. Dmitry Bivol\n",
      "4. Simona Halep\n",
      "5. Gillian Flynn\n",
      "6. Tim Cook\n",
      "Who among these people mentioned above is a politician by profession?\n",
      "Answer: >> Bernie Sanders\n",
      "0 patches to ablate possible answer information from options\n",
      "2025-08-08 01:17:34 src.experiments.utils DEBUG    Generated full answer: \" Bernie Sanders\n",
      "The best answer is Bernie Sanders.\"\n",
      "dict_keys(['loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'])\n",
      "2025-08-08 01:17:34 src.experiments.utils DEBUG    Predictions: ['\" Bernie\"[30324] (p=0.684, logit=21.125)', '\" Among\"[22395] (p=0.056, logit=18.625)', '\" (\"[320] (p=0.050, logit=18.500)', '\" Option\"[7104] (p=0.050, logit=18.500)', '\" The\"[578] (p=0.039, logit=18.250)']\n",
      "2025-08-08 01:17:34 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a6ebf0df-83a5\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a6ebf0df-83a5\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Julia\", \" Roberts\", \"\\n\", \"2\", \".\", \" Bernie\", \" Sanders\", \"\\n\", \"3\", \".\", \" Dmitry\", \" B\", \"ivol\", \"\\n\", \"4\", \".\", \" Sim\", \"ona\", \" Hale\", \"p\", \"\\n\", \"5\", \".\", \" Gill\", \"ian\", \" Flynn\", \"\\n\", \"6\", \".\", \" Tim\", \" Cook\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" politician\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.01310119591653347, 0.0037831782829016447, 0.0040107727982103825, 0.0032727241050451994, 0.009978103451430798, 0.009725570678710938, 0.00369682302698493, 0.07744140923023224, 0.22153320908546448, 0.21940918266773224, 0.0034177780617028475, 0.008381652645766735, 0.006954193115234375, 0.0030506134498864412, 0.0033657788299024105, 0.008222579956054688, 0.006198310758918524, 0.0048042298294603825, 0.0009912491077557206, 0.000623166561126709, 0.00031685532303527, 0.0006343275308609009, 0.001470589661039412, 0.0013751506339758635, 0.0018283843528479338, 0.0009924888145178556, 0.0006247043493203819, 0.00141229631844908, 0.001066744327545166, 0.002413845155388117, 0.00275001535192132, 0.0020589828491210938, 0.0023785114753991365, 0.002543544862419367, 0.00737686175853014, 0.014521097764372826, 0.01045832596719265, 0.00327472691424191, 0.005678415298461914, 0.006332003977149725, 0.004132890608161688, 0.0018863677978515625, 0.02732086181640625, 0.006812763400375843, 0.0014432907337322831, 0.02214202843606472, 0.0021781802643090487, 0.03146515041589737]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f3986dfd550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Julia Roberts\n",
      "2. Bernie Sanders\n",
      "3. Dmitry Bivol\n",
      "4. Simona Halep\n",
      "5. Gillian Flynn\n",
      "6. Tim Cook\n",
      "Who among these people mentioned above is a actor by profession?\n",
      "Answer: >> Julia Roberts\n",
      "0 patches to ablate possible answer information from options\n",
      "2025-08-08 01:17:38 src.experiments.utils DEBUG    Generated full answer: \" Julia Roberts\n",
      "Explanation: Julia Roberts is an American actress and producer. She is known for her roles in movies like \"Pretty Woman\", \"Erin\"\n",
      "dict_keys(['loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'])\n",
      "2025-08-08 01:17:38 src.experiments.utils DEBUG    Predictions: ['\" Julia\"[40394] (p=0.895, logit=22.375)', '\" Among\"[22395] (p=0.021, logit=18.625)', '\" The\"[578] (p=0.019, logit=18.500)', '\" Only\"[8442] (p=0.016, logit=18.375)', '\" Option\"[7104] (p=0.014, logit=18.250)']\n",
      "2025-08-08 01:17:38 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-711bf444-c3e7\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-711bf444-c3e7\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Julia\", \" Roberts\", \"\\n\", \"2\", \".\", \" Bernie\", \" Sanders\", \"\\n\", \"3\", \".\", \" Dmitry\", \" B\", \"ivol\", \"\\n\", \"4\", \".\", \" Sim\", \"ona\", \" Hale\", \"p\", \"\\n\", \"5\", \".\", \" Gill\", \"ian\", \" Flynn\", \"\\n\", \"6\", \".\", \" Tim\", \" Cook\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" actor\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.02070312574505806, 0.008847045712172985, 0.07655639946460724, 0.22587890923023224, 0.10079345852136612, 0.03596191480755806, 0.03536377102136612, 0.0036302567459642887, 0.002508795354515314, 0.009988022036850452, 0.0062049864791333675, 0.00902404822409153, 0.0008737087482586503, 0.001962375594303012, 0.0011148452758789062, 0.002593791577965021, 0.0021833418868482113, 0.0036643981002271175, 0.0017505645519122481, 0.0014773845905438066, 0.00047372429980896413, 0.0014610409270972013, 0.0019013345008715987, 0.0016258240211755037, 0.0015689849387854338, 0.0019903182983398438, 0.0044097900390625, 0.006249189376831055, 0.006176757626235485, 0.004158592317253351, 0.0042129517532885075, 0.00418434152379632, 0.002961111022159457, 0.002199030015617609, 0.01256408728659153, 0.011924361810088158, 0.010544967837631702, 0.0074142455123364925, 0.006649017333984375, 0.012406254187226295, 0.005818367004394531, 0.0025417327415198088, 0.02479858323931694, 0.004306984134018421, 0.0018571853870525956, 0.03032226487994194, 0.006042003631591797, 0.02920990064740181]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f39600f1810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "for sample in [patch_sample, clean_sample]:\n",
    "    print(sample.prompt(), \">>\", sample.obj)\n",
    "    attn_pattern = verify_head_patterns(\n",
    "        prompt=sample.prompt(),\n",
    "        options=sample.options,\n",
    "        pivot=sample.subj,\n",
    "        mt=mt,\n",
    "        heads=HEADS,\n",
    "        generate_full_answer=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf72a519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:17:44 src.hooking.llama_attention DEBUG    LlamaAttentionPatcher <> model.layers.33.self_attn\n",
      "2025-08-08 01:17:44 src.hooking.llama_attention DEBUG    hidden_shape=(1, 49, -1, 128) | input_shape=torch.Size([1, 49]) | torch.Size([1, 49, 8192])\n",
      "2025-08-08 01:17:44 src.hooking.llama_attention DEBUG    query_states.size()=torch.Size([1, 64, 49, 128]) | key_states.size()=torch.Size([1, 8, 49, 128]) | value_states.size()=torch.Size([1, 8, 49, 128])\n",
      "2025-08-08 01:17:44 src.hooking.llama_attention DEBUG    LlamaAttentionPatcher <> model.layers.33.self_attn\n",
      "2025-08-08 01:17:44 src.hooking.llama_attention DEBUG    hidden_shape=(1, 49, -1, 128) | input_shape=torch.Size([1, 49]) | torch.Size([1, 49, 8192])\n",
      "2025-08-08 01:17:44 src.hooking.llama_attention DEBUG    query_states.size()=torch.Size([1, 64, 49, 128]) | key_states.size()=torch.Size([1, 8, 49, 128]) | value_states.size()=torch.Size([1, 8, 49, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 49, 128]),\n",
       " torch.Size([1, 64, 49, 128]),\n",
       " torch.Size([1, 49, 8192]),\n",
       " torch.Size([1, 49, 8192]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import baukit\n",
    "from src.functional import get_module_nnsight, PatchSpec\n",
    "from src.hooking.llama_attention import LlamaAttentionPatcher\n",
    "import types\n",
    "from typing import Literal\n",
    "from src.tokens import prepare_input\n",
    "\n",
    "\n",
    "def set_attn_implementation(mt, attn_implementation: Literal[\"sdpa\", \"eager\"]):\n",
    "    mt.config._attn_implementation = attn_implementation\n",
    "    for layer_idx in range(mt.config.num_hidden_layers):\n",
    "        attn_block_name = mt.attn_module_name_format.format(layer_idx)\n",
    "        attn_block = baukit.get_module(mt._model, attn_block_name)\n",
    "        attn_block.config._attn_implementation = attn_implementation\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "batch_size = 1  # tokenized.input_ids.shape[0]\n",
    "n_heads = mt.config.num_attention_heads\n",
    "head_dim = mt.n_embd // n_heads\n",
    "query_idx = -1 # almost always the last token\n",
    "###################################################################################\n",
    "\n",
    "mt.reset_forward()\n",
    "set_attn_implementation(mt, \"sdpa\")\n",
    "\n",
    "layer_idx, head_idx = HEADS[0]\n",
    "\n",
    "attn_block_name = mt.attn_module_name_format.format(layer_idx)\n",
    "attn_block = baukit.get_module(mt._model, attn_block_name)\n",
    "attn_block.forward = types.MethodType(\n",
    "    LlamaAttentionPatcher(block_name=attn_block_name),\n",
    "    attn_block,\n",
    ")\n",
    "\n",
    "patch_tokenized = prepare_input(prompts=patch_sample.prompt(), tokenizer=mt)\n",
    "patch_seq_len = patch_tokenized.input_ids.shape[1]\n",
    "input_ln = mt.layer_name_format.format(layer_idx) + \".input_layernorm\"\n",
    "\n",
    "with mt.trace(patch_tokenized) as trace:\n",
    "    ln_module = get_module_nnsight(mt, input_ln)\n",
    "    patch_ln = ln_module.output.save()\n",
    "\n",
    "    q_proj_name = mt.attn_module_name_format.format(layer_idx) + \".q_proj\"\n",
    "    q_proj_module = get_module_nnsight(mt, q_proj_name)\n",
    "    patch_q_proj = q_proj_module.output.view(batch_size, patch_seq_len, n_heads, head_dim).transpose(1, 2).save()\n",
    "    # patch_q_proj = PatchSpec(\n",
    "    #     location=(q_proj_name + f\".{head_idx}\", -1),\n",
    "    #     patch=patch_q_proj[:, head_idx, query_idx, :].squeeze().save()\n",
    "    # )\n",
    "\n",
    "clean_tokenized = prepare_input(prompts=clean_sample.prompt(), tokenizer=mt)\n",
    "clean_seq_len = clean_tokenized.input_ids.shape[1]\n",
    "with mt.trace(clean_tokenized) as trace:\n",
    "    ln_module = get_module_nnsight(mt, input_ln)\n",
    "    clean_ln = ln_module.output.save()\n",
    "\n",
    "    q_proj_name = mt.attn_module_name_format.format(layer_idx) + \".q_proj\"\n",
    "    q_proj_module = get_module_nnsight(mt, q_proj_name)\n",
    "    clean_q_proj = q_proj_module.output.view(batch_size, clean_seq_len, n_heads, head_dim).transpose(1, 2).save()\n",
    "    # clean_q_proj = PatchSpec(\n",
    "    #     location=(q_proj_name + f\".{head_idx}\", -1),\n",
    "    #     patch=clean_q_proj[:, head_idx, query_idx, :].squeeze().save()\n",
    "    # )\n",
    "\n",
    "mt.reset_forward()\n",
    "set_attn_implementation(mt, \"eager\")\n",
    "\n",
    "patch_q_proj.shape, clean_q_proj.shape, patch_ln.shape, clean_ln.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2d10d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:17:45 src.hooking.llama_attention DEBUG    LlamaAttentionPatcher <> model.layers.33.self_attn\n",
      "2025-08-08 01:17:45 src.hooking.llama_attention DEBUG    hidden_shape=(1, 49, -1, 128) | input_shape=torch.Size([1, 49]) | torch.Size([1, 49, 8192])\n",
      "2025-08-08 01:17:45 src.hooking.llama_attention DEBUG    query_states.size()=torch.Size([1, 64, 49, 128]) | key_states.size()=torch.Size([1, 8, 49, 128]) | value_states.size()=torch.Size([1, 8, 49, 128])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-3648c8f6-c7fd\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-3648c8f6-c7fd\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Julia\", \" Roberts\", \"\\n\", \"2\", \".\", \" Bernie\", \" Sanders\", \"\\n\", \"3\", \".\", \" Dmitry\", \" B\", \"ivol\", \"\\n\", \"4\", \".\", \" Sim\", \"ona\", \" Hale\", \"p\", \"\\n\", \"5\", \".\", \" Gill\", \"ian\", \" Flynn\", \"\\n\", \"6\", \".\", \" Tim\", \" Cook\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" actor\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.0196533203125, 0.006134033203125, 0.0439453125, 0.1689453125, 0.0294189453125, 0.0419921875, 0.0191650390625, 0.0120849609375, 0.004547119140625, 0.00762939453125, 0.01324462890625, 0.01019287109375, 0.0025482177734375, 0.006683349609375, 0.004913330078125, 0.0015716552734375, 0.004852294921875, 0.0040283203125, 0.004547119140625, 0.0064697265625, 0.00164794921875, 0.002716064453125, 0.0014801025390625, 0.0029296875, 0.0025177001953125, 0.006500244140625, 0.0087890625, 0.0089111328125, 0.002288818359375, 0.00860595703125, 0.00787353515625, 0.0054931640625, 0.0067138671875, 0.0023956298828125, 0.025634765625, 0.008056640625, 0.009765625, 0.0032806396484375, 0.0059814453125, 0.010986328125, 0.0064697265625, 0.00164794921875, 0.023681640625, 0.00469970703125, 0.005401611328125, 0.04443359375, 0.00799560546875, 0.020751953125]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f3986dff750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' Julia', prob=0.88671875, logit=22.375, token_id=40394, metadata=None),\n",
       "  PredictedToken(token=' Among', prob=0.023681640625, logit=18.75, token_id=22395, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0208740234375, logit=18.625, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' Only', prob=0.0162353515625, logit=18.375, token_id=8442, metadata=None),\n",
       "  PredictedToken(token=' Option', prob=0.01434326171875, logit=18.25, token_id=7104, metadata=None)],\n",
       " {40394: (1,\n",
       "   PredictedToken(token=' Julia', prob=0.88671875, logit=22.375, token_id=40394, metadata=None)),\n",
       "  30324: (249,\n",
       "   PredictedToken(token=' Bernie', prob=1.8849968910217285e-06, logit=9.3125, token_id=30324, metadata=None))})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.attention import visualize_attn_matrix\n",
    "from src.functional import get_hs, interpret_logits\n",
    "\n",
    "mt.reset_forward()\n",
    "set_attn_implementation(mt, \"sdpa\")\n",
    "\n",
    "layer_idx, head_idx = HEADS[0]\n",
    "\n",
    "attn_matrices = {layer_idx: {}}\n",
    "\n",
    "attn_block_name = mt.attn_module_name_format.format(layer_idx)\n",
    "attn_block = baukit.get_module(mt._model, attn_block_name)\n",
    "attn_block.forward = types.MethodType(\n",
    "    LlamaAttentionPatcher(\n",
    "        block_name=attn_block_name,\n",
    "        save_attn_for=[head_idx],\n",
    "        store_attn_matrices=attn_matrices[layer_idx],\n",
    "    ),\n",
    "    attn_block,\n",
    ")\n",
    "\n",
    "logit_location = (mt.lm_head_name, -1)\n",
    "logits = get_hs(\n",
    "    mt = mt,\n",
    "    input = clean_tokenized,\n",
    "    locations = [logit_location],\n",
    "    return_dict=False\n",
    ").squeeze()  # (seq_len, vocab_size)\n",
    "\n",
    "mt.reset_forward()\n",
    "set_attn_implementation(mt, \"eager\")\n",
    "\n",
    "head_matrix = attn_matrices[layer_idx][head_idx].squeeze().to(torch.float32).cpu().numpy()\n",
    "\n",
    "visualize_attn_matrix(\n",
    "    attn_matrix=head_matrix,\n",
    "    tokens=[mt.tokenizer.decode(t) for t in clean_tokenized.input_ids[0]],\n",
    "    q_index=-1,\n",
    ")\n",
    "\n",
    "interpret_logits(\n",
    "    tokenizer=mt,\n",
    "    logits=logits,\n",
    "    interested_tokens=[patch_sample.obj_token_id, clean_sample.obj_token_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e560d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 01:17:46 src.hooking.llama_attention DEBUG    LlamaAttentionPatcher <> model.layers.33.self_attn\n",
      "2025-08-08 01:17:46 src.hooking.llama_attention DEBUG    hidden_shape=(1, 49, -1, 128) | input_shape=torch.Size([1, 49]) | torch.Size([1, 49, 8192])\n",
      "2025-08-08 01:17:46 src.hooking.llama_attention DEBUG    query_states.size()=torch.Size([1, 64, 49, 128]) | key_states.size()=torch.Size([1, 8, 49, 128]) | value_states.size()=torch.Size([1, 8, 49, 128])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-397ae800-756f\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-397ae800-756f\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Julia\", \" Roberts\", \"\\n\", \"2\", \".\", \" Bernie\", \" Sanders\", \"\\n\", \"3\", \".\", \" Dmitry\", \" B\", \"ivol\", \"\\n\", \"4\", \".\", \" Sim\", \"ona\", \" Hale\", \"p\", \"\\n\", \"5\", \".\", \" Gill\", \"ian\", \" Flynn\", \"\\n\", \"6\", \".\", \" Tim\", \" Cook\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" actor\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.0198974609375, 0.005157470703125, 0.01300048828125, 0.01226806640625, 0.009521484375, 0.0201416015625, 0.00897216796875, 0.07373046875, 0.12353515625, 0.064453125, 0.006744384765625, 0.010498046875, 0.017333984375, 0.00396728515625, 0.01904296875, 0.0091552734375, 0.01104736328125, 0.0089111328125, 0.0047607421875, 0.005157470703125, 0.002288818359375, 0.003387451171875, 0.0034332275390625, 0.0029449462890625, 0.003173828125, 0.0047607421875, 0.003387451171875, 0.006439208984375, 0.0015716552734375, 0.00604248046875, 0.0062255859375, 0.007568359375, 0.011474609375, 0.005767822265625, 0.017822265625, 0.01165771484375, 0.015869140625, 0.0050048828125, 0.0074462890625, 0.00799560546875, 0.0084228515625, 0.0025634765625, 0.00150299072265625, 0.00079345703125, 0.0017547607421875, 0.007171630859375, 0.00225830078125, 0.00244140625]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f390034bb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' Julia', prob=0.89453125, logit=22.375, token_id=40394, metadata=None),\n",
       "  PredictedToken(token=' Among', prob=0.0238037109375, logit=18.75, token_id=22395, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0185546875, logit=18.5, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' Only', prob=0.016357421875, logit=18.375, token_id=8442, metadata=None),\n",
       "  PredictedToken(token=' Option', prob=0.01446533203125, logit=18.25, token_id=7104, metadata=None)],\n",
       " {40394: (1,\n",
       "   PredictedToken(token=' Julia', prob=0.89453125, logit=22.375, token_id=40394, metadata=None)),\n",
       "  30324: (190,\n",
       "   PredictedToken(token=' Bernie', prob=3.129243850708008e-06, logit=9.8125, token_id=30324, metadata=None))})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.attention import visualize_attn_matrix\n",
    "\n",
    "mt.reset_forward()\n",
    "set_attn_implementation(mt, \"sdpa\")\n",
    "\n",
    "layer_idx, head_idx = HEADS[0]\n",
    "\n",
    "attn_matrices = {layer_idx: {}}\n",
    "\n",
    "attn_block_name = mt.attn_module_name_format.format(layer_idx)\n",
    "attn_block = baukit.get_module(mt._model, attn_block_name)\n",
    "attn_block.forward = types.MethodType(\n",
    "    LlamaAttentionPatcher(\n",
    "        block_name=attn_block_name,\n",
    "        save_attn_for=[head_idx],\n",
    "        store_attn_matrices=attn_matrices[layer_idx],\n",
    "        query_patches=[[head_idx, query_idx, patch_q_proj[:, head_idx, query_idx, :].squeeze()]],\n",
    "    ),\n",
    "    attn_block,\n",
    ")\n",
    "\n",
    "logit_location = (mt.lm_head_name, -1)\n",
    "patch_logits = get_hs(\n",
    "    mt = mt,\n",
    "    input = clean_tokenized,\n",
    "    locations = [logit_location],\n",
    "    return_dict=False\n",
    ").squeeze()  # (seq_len, vocab_size)\n",
    "\n",
    "mt.reset_forward()\n",
    "set_attn_implementation(mt, \"eager\")\n",
    "\n",
    "head_matrix = attn_matrices[layer_idx][head_idx].squeeze().to(torch.float32).cpu().numpy()\n",
    "\n",
    "visualize_attn_matrix(\n",
    "    attn_matrix=head_matrix,\n",
    "    tokens=[mt.tokenizer.decode(t) for t in clean_tokenized.input_ids[0]],\n",
    "    q_index=-1,\n",
    ")\n",
    "\n",
    "interpret_logits(\n",
    "    tokenizer=mt,\n",
    "    logits=patch_logits,\n",
    "    interested_tokens=[patch_sample.obj_token_id, clean_sample.obj_token_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "231d29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.allclose(patch_logits, logits, atol = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b92a102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.allclose(\n",
    "#     patch_q_proj[:, head_idx, query_idx, :], \n",
    "#     clean_q_proj[:, head_idx, query_idx, :],\n",
    "#     atol=1e-3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9042b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.allclose(\n",
    "#     patch_ln[:, query_idx, :], \n",
    "#     clean_ln[:, query_idx, :],\n",
    "#     atol=1e-3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49583481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_ln[:, query_idx, :], clean_ln[:, query_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "04ead16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # manual calculation\n",
    "# attn_module = baukit.get_module(mt._model, mt.attn_module_name_format.format(layer_idx))\n",
    "# patch_q_proj_manual = attn_module.q_proj(patch_ln)\n",
    "# clean_q_proj_manual = attn_module.q_proj(clean_ln)\n",
    "\n",
    "# print(patch_q_proj_manual.shape, clean_q_proj_manual.shape)\n",
    "# print(torch.allclose(\n",
    "#     patch_q_proj_manual[:, query_idx, :], \n",
    "#     clean_q_proj_manual[:, query_idx, :],\n",
    "#     atol=1e-3\n",
    "# ))\n",
    "\n",
    "# patch_q_proj_manual = patch_q_proj_manual.reshape(batch_size, patch_seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "# clean_q_proj_manual = clean_q_proj_manual.reshape(batch_size, clean_seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "# print(patch_q_proj_manual.shape, clean_q_proj_manual.shape)\n",
    "\n",
    "# for idx in range(n_heads):\n",
    "#     print(head_idx, torch.allclose(\n",
    "#         patch_q_proj_manual[:, idx, query_idx, :], \n",
    "#         clean_q_proj_manual[:, idx, query_idx, :],\n",
    "#         atol=1e-3\n",
    "#     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "098535e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.allclose(\n",
    "#     patch_q_proj_manual[:, head_idx, query_idx, :], \n",
    "#     patch_q_proj[:, head_idx, query_idx, :],\n",
    "#     atol=1e-3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6de9135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "torch.Size([1, 64, 49, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('model.layers.33.self_attn.q_proj', 45, -1),\n",
       " ('model.layers.33.self_attn.q_proj', -1))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_q_proj = clean_q_proj.clone()\n",
    "replace_q_proj[:, head_idx, query_idx, :] = patch_q_proj[:, head_idx, query_idx, :]\n",
    "\n",
    "print(torch.allclose(\n",
    "        replace_q_proj[:, head_idx, query_idx, :],\n",
    "        clean_q_proj[:, head_idx, query_idx, :],\n",
    "        atol=1e-3\n",
    "    )\n",
    ")\n",
    "print(replace_q_proj.shape)\n",
    "\n",
    "replace_q_proj = replace_q_proj.transpose(1, 2).reshape(batch_size, clean_seq_len, -1)\n",
    "\n",
    "rep_patch = PatchSpec(\n",
    "    location=(q_proj_name, -1),\n",
    "    patch=replace_q_proj[:, -1, :].squeeze(),\n",
    ")\n",
    "\n",
    "ln_patch = PatchSpec(\n",
    "    location=(input_ln, -1),\n",
    "    patch=patch_ln[:, query_idx, :].squeeze(),\n",
    ")\n",
    "\n",
    "head_q_patch = PatchSpec(\n",
    "    location=(q_proj_name, head_idx, -1),\n",
    "    patch=patch_q_proj[:, head_idx, query_idx, :].squeeze(),\n",
    ")\n",
    "\n",
    "head_q_patch.location, rep_patch.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3d26910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_q_proj_rs = clean_q_proj.view(batch_size, clean_seq_len, -1)\n",
    "# patch_q_proj_rs = patch_q_proj.view(batch_size, clean_seq_len, -1)\n",
    "# clean_q_proj_rs[:, -1, :].shape, patch_q_proj_rs[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95232df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.allclose(\n",
    "#     replace_q_proj[:, -1, :].squeeze(), \n",
    "#     # patch_q_proj_rs[:, -1, :].squeeze(),\n",
    "#     clean_q_proj_rs[:, -1, :].squeeze(), \n",
    "#     atol=1e-3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9835953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 49, 128]), torch.Size([1, 64, 49, 128]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_q_proj.shape, clean_q_proj.shape\n",
    "# torch.allclose(patch_q_proj.patch, clean_q_proj.patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "759c4ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-b0782d34-f7e1\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-b0782d34-f7e1\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Julia\", \" Roberts\", \"\\n\", \"2\", \".\", \" Bernie\", \" Sanders\", \"\\n\", \"3\", \".\", \" Dmitry\", \" B\", \"ivol\", \"\\n\", \"4\", \".\", \" Sim\", \"ona\", \" Hale\", \"p\", \"\\n\", \"5\", \".\", \" Gill\", \"ian\", \" Flynn\", \"\\n\", \"6\", \".\", \" Tim\", \" Cook\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" actor\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.019775390625, 0.006011962890625, 0.04345703125, 0.1708984375, 0.0289306640625, 0.042236328125, 0.01904296875, 0.01153564453125, 0.00439453125, 0.00726318359375, 0.01324462890625, 0.0101318359375, 0.0023956298828125, 0.0057373046875, 0.004913330078125, 0.001495361328125, 0.0047607421875, 0.003936767578125, 0.004547119140625, 0.0064697265625, 0.00164794921875, 0.002716064453125, 0.00142669677734375, 0.003021240234375, 0.00250244140625, 0.006622314453125, 0.0089111328125, 0.00897216796875, 0.002288818359375, 0.00872802734375, 0.007781982421875, 0.0054931640625, 0.006805419921875, 0.00250244140625, 0.025390625, 0.008056640625, 0.00946044921875, 0.003265380859375, 0.005889892578125, 0.01080322265625, 0.006256103515625, 0.00167083740234375, 0.0244140625, 0.00482177734375, 0.0054931640625, 0.0458984375, 0.008056640625, 0.021484375]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f3960141b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' Julia', prob=0.89453125, logit=22.375, token_id=40394, metadata=None),\n",
       "  PredictedToken(token=' Among', prob=0.02099609375, logit=18.625, token_id=22395, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0185546875, logit=18.5, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' Only', prob=0.016357421875, logit=18.375, token_id=8442, metadata=None),\n",
       "  PredictedToken(token=' Option', prob=0.01446533203125, logit=18.25, token_id=7104, metadata=None)],\n",
       " {40394: (1,\n",
       "   PredictedToken(token=' Julia', prob=0.89453125, logit=22.375, token_id=40394, metadata=None)),\n",
       "  30324: (236,\n",
       "   PredictedToken(token=' Bernie', prob=2.0265579223632812e-06, logit=9.375, token_id=30324, metadata=None))})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.attention import get_attention_matrices, visualize_attn_matrix\n",
    "from src.functional import interpret_logits\n",
    "\n",
    "clean_tokenized = prepare_input(prompts=clean_sample.prompt(), tokenizer=mt)\n",
    "\n",
    "attn_info = get_attention_matrices(\n",
    "    input=clean_tokenized,\n",
    "    mt=mt,\n",
    ")\n",
    "\n",
    "attn_matrix = attn_info.attention_matrices[layer_idx, head_idx].squeeze()\n",
    "visualize_attn_matrix(\n",
    "    attn_matrix=attn_matrix,\n",
    "    tokens=[mt.tokenizer.decode(t) for t in clean_tokenized.input_ids[0]],\n",
    "    q_index=-1,\n",
    ")\n",
    "\n",
    "interpret_logits(\n",
    "    tokenizer=mt,\n",
    "    logits=attn_info.logits,\n",
    "    interested_tokens=[patch_sample.obj_token_id, clean_sample.obj_token_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "136268c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-afe8f997-94f5\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-afe8f997-94f5\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Julia\", \" Roberts\", \"\\n\", \"2\", \".\", \" Bernie\", \" Sanders\", \"\\n\", \"3\", \".\", \" Dmitry\", \" B\", \"ivol\", \"\\n\", \"4\", \".\", \" Sim\", \"ona\", \" Hale\", \"p\", \"\\n\", \"5\", \".\", \" Gill\", \"ian\", \" Flynn\", \"\\n\", \"6\", \".\", \" Tim\", \" Cook\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" actor\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.02001953125, 0.005096435546875, 0.01318359375, 0.01220703125, 0.0093994140625, 0.0203857421875, 0.009033203125, 0.0732421875, 0.1240234375, 0.064453125, 0.006744384765625, 0.01055908203125, 0.0172119140625, 0.00396728515625, 0.01904296875, 0.0089111328125, 0.01080322265625, 0.00872802734375, 0.004791259765625, 0.00518798828125, 0.002288818359375, 0.003387451171875, 0.003448486328125, 0.00299072265625, 0.003143310546875, 0.004852294921875, 0.003387451171875, 0.00653076171875, 0.00157928466796875, 0.006134033203125, 0.006256103515625, 0.007598876953125, 0.01141357421875, 0.005767822265625, 0.0177001953125, 0.01177978515625, 0.0157470703125, 0.005096435546875, 0.007354736328125, 0.00799560546875, 0.00836181640625, 0.0025177001953125, 0.00150299072265625, 0.00079345703125, 0.00176239013671875, 0.007080078125, 0.00225830078125, 0.00244140625]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f3900396290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([PredictedToken(token=' Julia', prob=0.890625, logit=22.375, token_id=40394, metadata=None),\n",
       "  PredictedToken(token=' Among', prob=0.0238037109375, logit=18.75, token_id=22395, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0185546875, logit=18.5, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' Only', prob=0.016357421875, logit=18.375, token_id=8442, metadata=None),\n",
       "  PredictedToken(token=' Option', prob=0.014404296875, logit=18.25, token_id=7104, metadata=None)],\n",
       " {40394: (1,\n",
       "   PredictedToken(token=' Julia', prob=0.890625, logit=22.375, token_id=40394, metadata=None)),\n",
       "  30324: (191,\n",
       "   PredictedToken(token=' Bernie', prob=3.11434268951416e-06, logit=9.8125, token_id=30324, metadata=None))})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.attention import get_attention_matrices, visualize_attn_matrix\n",
    "from src.functional import patch_with_nnsight, patch_with_baukit\n",
    "\n",
    "clean_tokenized = prepare_input(prompts=clean_sample.prompt(), tokenizer=mt)\n",
    "\n",
    "patched_attn_info = get_attention_matrices(\n",
    "    input=clean_tokenized,\n",
    "    mt=mt,\n",
    "    # patches=[ln_patch],\n",
    "    patches = [head_q_patch],\n",
    "    # patches = [rep_patch]\n",
    "    # patch_interface=patch_with_nnsight\n",
    "    patch_interface=patch_with_baukit\n",
    ")\n",
    "\n",
    "patched_attn_matrix = patched_attn_info.attention_matrices[layer_idx, head_idx].squeeze()\n",
    "visualize_attn_matrix(\n",
    "    attn_matrix=patched_attn_matrix,\n",
    "    tokens=[mt.tokenizer.decode(t) for t in clean_tokenized.input_ids[0]],\n",
    "    q_index=-1,\n",
    ")\n",
    "\n",
    "interpret_logits(\n",
    "    tokenizer=mt,\n",
    "    logits=patched_attn_info.logits,\n",
    "    interested_tokens=[patch_sample.obj_token_id, clean_sample.obj_token_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98815bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
