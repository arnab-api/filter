{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ec34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfae7235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 11:45:46 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-08-07 11:45:46 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-08-07 11:45:46 __main__ INFO     transformers.__version__='4.54.1'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7720ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 11:45:49 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-07 11:45:49 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-07 11:45:49 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-08-07 11:45:49 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683855df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 11:45:50 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-08-07 11:45:50 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-08-07 11:45:50 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-08-07 11:45:50 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-08-07 11:45:50 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/models/meta-llama/Llama-3.3-70B-Instruct/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68f70ba3f5b4fe4ae58de4410467609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-07 11:46:37 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-08-07 11:46:37 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n",
      "2025-08-07 11:46:37 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a62d97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectOneTask: (profession of a famous person)\n",
      "Categories: actor(20), singer(20), comedian(20), director(20), basketball player(20), football player(20), soccer player(20), tennis player(20), golfer(20), boxer(20), news anchor(20), journalist(20), author(20), fashion designer(20), entrepreneur(19), politician(20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOneTask\n",
    "\n",
    "select_prof = SelectOneTask.load(\n",
    "    path=os.path.join(env_utils.DEFAULT_DATA_DIR, \"selection\", \"profession.json\")\n",
    ")\n",
    "\n",
    "print(select_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45087a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al Pacino -> Meryl Streep (1): ['Lee Child', 'Meryl Streep', 'Ellen DeGeneres', 'Matt Fitzpatrick', 'Erling Haaland', 'Chuck Todd']\n"
     ]
    }
   ],
   "source": [
    "sample = select_prof.get_random_sample(\n",
    "    mt = mt,\n",
    "    prompt_template_idx=3,\n",
    "    option_style=\"numbered\",\n",
    "    category=\"actor\"\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66399b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1. Lee Child\n",
      "2. Meryl Streep\n",
      "3. Ellen DeGeneres\n",
      "4. Matt Fitzpatrick\n",
      "5. Erling Haaland\n",
      "6. Chuck Todd\n",
      "Who among these people mentioned above is a actor by profession?\n",
      "Answer:\" >> Meryl Streep\n"
     ]
    }
   ],
   "source": [
    "# sample.prompt_template = select_prof.prompt_templates[3]\n",
    "\n",
    "print(f'\"{sample.prompt()}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57d67c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: Lee Child, Meryl Streep, Ellen DeGeneres, Matt Fitzpatrick, Erling Haaland, Chuck Todd.\n",
      "Who among these people mentioned above is a actor by profession?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(sample.prompt(option_style=\"single_line\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea0499b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Meryl Streep\n",
      "Meryl Streep is an American actress known for her versatility and range in\" >> Meryl Streep\n"
     ]
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")[0]\n",
    "print(f'\"{gen}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85a9a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patches to ablate possible answer information from options\n",
      "2025-08-07 11:46:51 src.experiments.utils DEBUG    Predictions: ['\" M\"[386] (p=0.637, logit=20.625)', '\" \"[220] (p=0.067, logit=18.375)', '\" Among\"[22395] (p=0.052, logit=18.125)', '\" (\"[320] (p=0.046, logit=18.000)', '\" Option\"[7104] (p=0.041, logit=17.875)']\n",
      "2025-08-07 11:46:51 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-b2c76975-2ec6\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-b2c76975-2ec6\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Lee\", \" Child\", \"\\n\", \"2\", \".\", \" M\", \"eryl\", \" Stre\", \"ep\", \"\\n\", \"3\", \".\", \" Ellen\", \" De\", \"Gener\", \"es\", \"\\n\", \"4\", \".\", \" Matt\", \" Fitz\", \"patrick\", \"\\n\", \"5\", \".\", \" Erl\", \"ing\", \" Ha\", \"aland\", \"\\n\", \"6\", \".\", \" Chuck\", \" Todd\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" a\", \" actor\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.01872405968606472, 0.004210281185805798, 0.004742431454360485, 0.005170726682990789, 0.01082763634622097, 0.00923767127096653, 0.004442977719008923, 0.012873172760009766, 0.0904693603515625, 0.08980407565832138, 0.134521484375, 0.19916991889476776, 0.003402137663215399, 0.01838226243853569, 0.012088012881577015, 0.003026592778041959, 0.011725997552275658, 0.02701873704791069, 0.02459564246237278, 0.005332755856215954, 0.004425048828125, 0.002434158232063055, 0.0009949803352355957, 0.0003676064370665699, 0.000912848103325814, 0.002177619840949774, 0.003197097685188055, 0.0010566950077190995, 0.0007881998899392784, 0.0003323793353047222, 0.0016299604903906584, 0.002560138702392578, 0.0022661208640784025, 0.0028747557662427425, 0.0009555816650390625, 0.0028465748764574528, 0.0037109374534338713, 0.008783722296357155, 0.006028699688613415, 0.007844734005630016, 0.002881813095882535, 0.0036344050895422697, 0.00352555513381958, 0.0028430460952222347, 0.0014871597522869706, 0.01913299597799778, 0.0021376609802246094, 0.0012123584747314453, 0.02564544603228569, 0.0031631470192223787, 0.02789611741900444]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f907408ed10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.subj,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fe333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
