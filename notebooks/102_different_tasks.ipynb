{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc88b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa440433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 12:50:48 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-09-10 12:50:49 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-09-10 12:50:49 __main__ INFO     transformers.__version__='4.55.3'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8766a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 12:50:52 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-09-10 12:50:52 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-09-10 12:50:52 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-09-10 12:50:52 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310827ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 12:50:53 src.models WARNING  meta-llama/Llama-3.2-3B not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-09-10 12:50:53 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 12:50:53 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-09-10 12:50:53 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-09-10 12:50:53 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/models/meta-llama/Llama-3.2-3B/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d8eb754b834a89955ee0362dbd931f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 12:50:58 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-09-10 12:50:58 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n",
      "2025-09-10 12:50:58 src.models INFO     loaded model <meta-llama/Llama-3.2-3B> | size: 6127.834 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17359f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YesNoTask: (different objects)\n",
      "Categories: fruit(15), vehicle(15), furniture(15), animal(15), music instrument(15), clothing(15), electronics(15), sport equipment(15), kitchen appliance(15), vegetable(14), building(15), office supply(15), bathroom item(15), flower(15), tree(15), jewelry(15)\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOneTask, CountingTask, YesNoTask, SelectFirstTask\n",
    "from src.selection.data import SelectionSample, CountingSample, YesNoSample\n",
    "\n",
    "#################################################################################\n",
    "# TASK_CLS = SelectOneTask\n",
    "# TASK_CLS = CountingTask\n",
    "TASK_CLS = YesNoTask\n",
    "prompt_template_idx = 0\n",
    "N_DISTRACTORS = 5\n",
    "OPTION_STYLE = \"single_line\"\n",
    "#################################################################################\n",
    "\n",
    "select_task = TASK_CLS.load(\n",
    "    path=os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR, \n",
    "        \"selection\", \n",
    "        # \"profession.json\"\n",
    "        # \"nationality.json\"\n",
    "        \"objects.json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(select_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f921352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "You have set `compile_config`, but we are unable to meet the criteria for compilation. Compilation will be skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Dress, Cauliflower, Charm, Daffodil, Ottoman\n",
      "Answer 'Yes' or 'No': Is there a fruit in the list above?\n",
      "Answer:\" >>  No\n",
      "Generation:\" Yes, there is a fruit in the list above.\n",
      "The fruit is the cauliflower.\n",
      "The cauliflower is\"\n"
     ]
    }
   ],
   "source": [
    "from src.functional import predict_next_token, generate_with_patch\n",
    "\n",
    "sample = select_task.get_random_sample(\n",
    "    mt=mt, \n",
    "    category=\"fruit\",\n",
    "    filter_by_lm_prediction=False,\n",
    "    prompt_template_idx=1,\n",
    "    n_options=5,\n",
    ")\n",
    "print(f'\"{sample.prompt()}\" >> {mt.tokenizer.decode(sample.ans_token_id)}')\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt=mt, \n",
    "    inputs=sample.prompt(),\n",
    "    n_gen_per_prompt=1,\n",
    "    remove_prefix=True,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(f\"Generation:\\\"{gen[0]}\\\"\") # expect sample.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764d792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d0e4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_counterfactual_samples_within_yes_no_task at 0x7eff6412d800>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Skirt, Keyboard, Lettuce, Dumbbell, Cherry, Scooter\n",
      "Answer 'Yes' or 'No': Is there a fruit in the list above?\n",
      "Answer: >>  Yes\n",
      "Museum, Hockey stick, Bus, Chair, Tractor, Food processor\n",
      "Answer 'Yes' or 'No': Is there a vehicle in the list above?\n",
      "Answer: >>  Yes\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import get_counterfactual_samples_interface\n",
    "\n",
    "counterfactual_sampler = get_counterfactual_samples_interface[select_task.task_name]\n",
    "\n",
    "print(counterfactual_sampler)\n",
    "\n",
    "patch_sample, clean_sample = counterfactual_sampler(\n",
    "    mt=mt,\n",
    "    task=select_task,\n",
    "    patch_category=\"fruit\",\n",
    "    clean_category=\"vehicle\",\n",
    "    filter_by_lm_prediction=False,\n",
    "    prompt_template_idx=1,\n",
    "    option_style=OPTION_STYLE,\n",
    "    # distinct_options=True,\n",
    "    n_options=6\n",
    ")\n",
    "\n",
    "# patch_sample.default_option_style = \"single_line\"\n",
    "# clean_sample.default_option_style = \"numbered\"\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(patch_sample.prompt(), \">>\", mt.tokenizer.decode(patch_sample.ans_token_id))\n",
    "print(clean_sample.prompt(), \">>\", mt.tokenizer.decode(clean_sample.ans_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "568ebee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([(9220,\n",
       "               (3,\n",
       "                PredictedToken(token=' Two', prob=0.2333984375, logit=21.5, token_id=9220, metadata=None))),\n",
       "              (14853,\n",
       "               (8,\n",
       "                PredictedToken(token=' Three', prob=0.003326416015625, logit=17.25, token_id=14853, metadata=None))),\n",
       "              (13625,\n",
       "               (17,\n",
       "                PredictedToken(token=' Four', prob=0.00045013427734375, logit=15.25, token_id=13625, metadata=None))),\n",
       "              (3861,\n",
       "               (19,\n",
       "                PredictedToken(token=' One', prob=0.000423431396484375, logit=15.1875, token_id=3861, metadata=None))),\n",
       "              (21594,\n",
       "               (42,\n",
       "                PredictedToken(token=' Five', prob=7.82012939453125e-05, logit=13.5, token_id=21594, metadata=None))),\n",
       "              (19198,\n",
       "               (75,\n",
       "                PredictedToken(token=' Six', prob=2.8848648071289062e-05, logit=12.5, token_id=19198, metadata=None))),\n",
       "              (18811,\n",
       "               (117,\n",
       "                PredictedToken(token=' Zero', prob=1.0609626770019531e-05, logit=11.5, token_id=18811, metadata=None))),\n",
       "              (36944,\n",
       "               (150,\n",
       "                PredictedToken(token=' Eight', prob=6.4373016357421875e-06, logit=11.0, token_id=36944, metadata=None))),\n",
       "              (31048,\n",
       "               (158,\n",
       "                PredictedToken(token=' Seven', prob=6.0498714447021484e-06, logit=10.9375, token_id=31048, metadata=None))),\n",
       "              (18165,\n",
       "               (198,\n",
       "                PredictedToken(token=' Ten', prob=3.904104232788086e-06, logit=10.5, token_id=18165, metadata=None))),\n",
       "              (38166,\n",
       "               (274,\n",
       "                PredictedToken(token=' Nine', prob=2.086162567138672e-06, logit=9.875, token_id=38166, metadata=None)))]),\n",
       " OrderedDict([(13625,\n",
       "               (3,\n",
       "                PredictedToken(token=' Four', prob=0.10498046875, logit=19.5, token_id=13625, metadata=None))),\n",
       "              (21594,\n",
       "               (4,\n",
       "                PredictedToken(token=' Five', prob=0.026611328125, logit=18.125, token_id=21594, metadata=None))),\n",
       "              (14853,\n",
       "               (7,\n",
       "                PredictedToken(token=' Three', prob=0.00860595703125, logit=17.0, token_id=14853, metadata=None))),\n",
       "              (19198,\n",
       "               (17,\n",
       "                PredictedToken(token=' Six', prob=0.00102996826171875, logit=14.875, token_id=19198, metadata=None))),\n",
       "              (3861,\n",
       "               (54,\n",
       "                PredictedToken(token=' One', prob=0.00013065338134765625, logit=12.8125, token_id=3861, metadata=None))),\n",
       "              (9220,\n",
       "               (62,\n",
       "                PredictedToken(token=' Two', prob=0.0001087188720703125, logit=12.625, token_id=9220, metadata=None))),\n",
       "              (31048,\n",
       "               (64,\n",
       "                PredictedToken(token=' Seven', prob=0.0001087188720703125, logit=12.625, token_id=31048, metadata=None))),\n",
       "              (36944,\n",
       "               (114,\n",
       "                PredictedToken(token=' Eight', prob=2.276897430419922e-05, logit=11.0625, token_id=36944, metadata=None))),\n",
       "              (38166,\n",
       "               (206,\n",
       "                PredictedToken(token=' Nine', prob=6.5267086029052734e-06, logit=9.8125, token_id=38166, metadata=None))),\n",
       "              (18165,\n",
       "               (265,\n",
       "                PredictedToken(token=' Ten', prob=3.9637088775634766e-06, logit=9.3125, token_id=18165, metadata=None))),\n",
       "              (18811,\n",
       "               (311,\n",
       "                PredictedToken(token=' Zero', prob=2.8908252716064453e-06, logit=9.0, token_id=18811, metadata=None)))]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_sample.prediction, clean_sample.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33958ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72131711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9272b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
