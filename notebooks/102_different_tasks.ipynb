{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc88b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa440433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 13:51:15 __main__ INFO     torch.__version__='2.8.0+cu126', torch.version.cuda='12.6'\n",
      "2025-09-10 13:51:15 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2025-09-10 13:51:15 __main__ INFO     transformers.__version__='4.56.1'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8766a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 13:51:18 git.cmd DEBUG    Popen(['git', 'version'], cwd=/share/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-09-10 13:51:18 git.cmd DEBUG    Popen(['git', 'version'], cwd=/share/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-09-10 13:51:18 wandb.docker.auth DEBUG    Trying paths: ['/share/u/arnab/.docker/config.json', '/share/u/arnab/.dockercfg']\n",
      "2025-09-10 13:51:18 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310827ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 13:51:19 src.models WARNING  google/gemma-2-27b-it not found in /share/u/models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-09-10 13:51:19 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-09-10 13:51:19 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /google/gemma-2-27b-it/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 13:51:20 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /google/gemma-2-27b-it/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-09-10 13:51:20 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/models/google/gemma-2-27b-it/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 13:51:20 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:06<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 13:51:27 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /google/gemma-2-27b-it/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-09-10 13:51:27 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /google/gemma-2-27b-it/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 13:51:27 accelerate.big_modeling WARNING  Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "2025-09-10 13:51:27 src.models INFO     loaded model <google/gemma-2-27b-it> | size: 51931.626 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17359f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'different objects', 'prompt_templates': ['Which object from the following list shares its category with <_pivot_entity_>?\\n<_options_>\\nAnswer:', '<_options_>\\nWhich among these objects mentioned above share the same category as <_pivot_entity_>?\\nAnswer:', 'Which object from the following list is a <_category_>?\\n<_options_>\\nAnswer:', '<_options_>\\nWhich among these objects mentioned above is a <_category_>?\\nAnswer:'], 'odd_one_prompt_templates': ['Which object from the following list does not belong to the same category as <_pivot_entity_>?\\n<_options_>\\nAnswer:', '<_options_>\\nWhich among these objects mentioned above does not belong to the same category as <_pivot_entity_>?\\nAnswer:', 'Which object from the following list is not a <_category_>?\\n<_options_>\\nAnswer:', '<_options_>\\nWhich among these objects mentioned above is not a <_category_>?\\nAnswer:', 'Detect the odd one out from the following list of objects:\\n<_options_>\\nAnswer:', '<_options_>\\nWhich object in this list does not fit in with others?\\nAnswer:'], 'order_prompt_templates': ['What is the <_order_> object in the following list?\\n<_options_>\\nAnswer:', '<_options_>\\nWhat is the <_order_> object from the list above?\\nAnswer:'], 'count_prompt_templates': ['How many objects are there in the following are <_category_>?\\nItems: <_options_>\\nAnswer:', 'Items: <_options_>\\nHow many <_category_>s are in this list?\\nAnswer:', 'How many objects are there in the following are <_category_>?\\nItems: <_options_>\\nAnswer: ', 'Items: <_options_>\\nHow many <_category_>s are in this list?\\nAnswer: '], 'yes_no_prompt_templates': [\"Answer 'Yes' or 'No': Is there a <_category_> in the following list?\\n<_options_>\\nAnswer:\", \"<_options_>\\nAnswer 'Yes' or 'No': Is there a <_category_> in the list above?\\nAnswer:\", \"Answer 'Yes' or 'No': Do you see a <_category_> in this list?\\n<_options_>\\nAnswer:\", \"<_options_>\\nAnswer 'Yes' or 'No': Do you see a <_category_> in the list above?\\nAnswer:\"], 'first_item_in_cat_prompt_templates': ['Identify the first <_category_> in this list:\\n<_options_>\\nAnswer:', '<_options_>\\nIdentify the first <_category_> from the list above?\\nAnswer:', 'What is the first <_category_> in the following list?\\n<_options_>\\nAnswer:', '<_options_>\\nWhat is the first <_category_> from the list above?\\nAnswer:'], 'categories': {'fruit': ['Apple', 'Banana', 'Orange', 'Grape', 'Pear', 'Strawberry', 'Watermelon', 'Mango', 'Pineapple', 'Kiwi', 'Peach', 'Plum', 'Cherry', 'Blueberry', 'Raspberry'], 'vehicle': ['Car', 'Bike', 'Bus', 'Truck', 'Motorcycle', 'Train', 'Airplane', 'Helicopter', 'Boat', 'Submarine', 'Scooter', 'Van', 'Tractor', 'Ambulance', 'Yacht'], 'furniture': ['Chair', 'Table', 'Sofa', 'Bed', 'Desk', 'Bookshelf', 'Wardrobe', 'Dresser', 'Nightstand', 'Ottoman', 'Bench', 'Cabinet', 'Stool', 'Recliner', 'Coffee table'], 'animal': ['Dog', 'Cat', 'Elephant', 'Tiger', 'Lion', 'Bear', 'Rabbit', 'Horse', 'Cow', 'Sheep', 'Giraffe', 'Zebra', 'Monkey', 'Dolphin', 'Eagle'], 'music instrument': ['Guitar', 'Piano', 'Drum', 'Violin', 'Flute', 'Saxophone', 'Trumpet', 'Cello', 'Clarinet', 'Harp', 'Accordion', 'Harmonica', 'Trombone', 'Ukulele', 'Xylophone'], 'clothing': ['Shirt', 'Pants', 'Jacket', 'Dress', 'Skirt', 'Sweater', 'Coat', 'Shorts', 'Jeans', 'Suit', 'Tie', 'Scarf', 'Gloves', 'Socks', 'Hat'], 'electronics': ['Phone', 'Laptop', 'Tablet', 'Camera', 'Headphones', 'Television', 'Smartwatch', 'Printer', 'Router', 'Speaker', 'Monitor', 'Keyboard', 'Mouse', 'Microphone', 'Projector'], 'sport equipment': ['Basketball', 'Football', 'Tennis ball', 'Baseball', 'Golf ball', 'Racket', 'Bat', 'Helmet', 'Skateboard', 'Surfboard', 'Skis', 'Hockey stick', 'Boxing gloves', 'Dumbbell', 'Yoga mat'], 'kitchen appliance': ['Refrigerator', 'Microwave', 'Oven', 'Dishwasher', 'Blender', 'Toaster', 'Coffee maker', 'Mixer', 'Food processor', 'Air fryer', 'Slow cooker', 'Rice cooker', 'Juicer', 'Kettle', 'Pressure cooker'], 'vegetable': ['Carrot', 'Broccoli', 'Spinach', 'Tomato', 'Potato', 'Onion', 'Cucumber', 'Lettuce', 'Pepper', 'Cauliflower', 'Celery', 'Mushroom', 'Asparagus', 'Zucchini'], 'building': ['House', 'Apartment', 'Skyscraper', 'Hospital', 'School', 'Library', 'Museum', 'Church', 'Temple', 'Mosque', 'Factory', 'Warehouse', 'Stadium', 'Theater', 'Mall'], 'office supply': ['Pen', 'Pencil', 'Paper', 'Stapler', 'Scissors', 'Tape', 'Ruler', 'Eraser', 'Paperclip', 'Folder', 'Binder', 'Calculator', 'Marker', 'Highlighter', 'Notebook'], 'bathroom item': ['Toothbrush', 'Toothpaste', 'Soap', 'Shampoo', 'Towel', 'Toilet paper', 'Mirror', 'Bathtub', 'Shower', 'Sink', 'Toilet', 'Razor', 'Comb', 'Hairdryer', 'Lotion'], 'flower': ['Rose', 'Tulip', 'Sunflower', 'Daisy', 'Lily', 'Orchid', 'Carnation', 'Daffodil', 'Iris', 'Peony', 'Lavender', 'Jasmine', 'Marigold', 'Chrysanthemum', 'Violet'], 'tree': ['Oak', 'Pine', 'Maple', 'Birch', 'Willow', 'Elm', 'Cedar', 'Spruce', 'Redwood', 'Bamboo', 'Eucalyptus', 'Magnolia', 'Palm', 'Ash', 'Hickory'], 'jewelry': ['Ring', 'Necklace', 'Bracelet', 'Earring', 'Watch', 'Pendant', 'Brooch', 'Anklet', 'Chain', 'Charm', 'Cufflink', 'Tiara', 'Locket', 'Bangle', 'Pin']}}\n",
      "SelectFirstTask: (different objects)\n",
      "Categories: fruit(15), vehicle(15), furniture(15), animal(15), music instrument(15), clothing(15), electronics(15), sport equipment(15), kitchen appliance(15), vegetable(14), building(15), office supply(15), bathroom item(15), flower(15), tree(15), jewelry(15)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOneTask, CountingTask, YesNoTask, SelectFirstTask\n",
    "from src.selection.data import SelectionSample, CountingSample, YesNoSample\n",
    "\n",
    "#################################################################################\n",
    "# TASK_CLS = SelectOneTask\n",
    "# TASK_CLS = CountingTask\n",
    "# TASK_CLS = YesNoTask\n",
    "TASK_CLS = SelectFirstTask\n",
    "prompt_template_idx = 0\n",
    "N_DISTRACTORS = 5\n",
    "OPTION_STYLE = \"single_line\"\n",
    "#################################################################################\n",
    "\n",
    "select_task = TASK_CLS.load(\n",
    "    path=os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR,\n",
    "        \"selection\",\n",
    "        # \"profession.json\"\n",
    "        # \"nationality.json\"\n",
    "        \"objects.json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(select_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f921352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Options: Scissors, Watermelon, Pineapple, Bed, Carrot, Strawberry, Apple.\n",
      "What is the first fruit from the list above?\n",
      "Answer:\" >>  Watermelon\n",
      "Generation:\" Watermelon\n",
      "\n",
      "Explanation:\n",
      "The first fruit in the list is watermelon. \n",
      "\n",
      "Let me know if\"\n"
     ]
    }
   ],
   "source": [
    "from src.functional import predict_next_token, generate_with_patch\n",
    "\n",
    "sample = select_task.get_random_sample(\n",
    "    mt=mt, \n",
    "    category=\"fruit\",\n",
    "    filter_by_lm_prediction=True,\n",
    "    prompt_template_idx=3,\n",
    "    # n_options=5,\n",
    "    n_distractors=3,\n",
    ")\n",
    "print(f'\"{sample.prompt()}\" >> {mt.tokenizer.decode(sample.ans_token_id)}')\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt=mt, \n",
    "    inputs=sample.prompt(),\n",
    "    n_gen_per_prompt=1,\n",
    "    remove_prefix=True,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(f\"Generation:\\\"{gen[0]}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "764d792e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Watermelon', prob=0.97265625, logit=23.125, token_id=197201, metadata=None),\n",
       " PredictedToken(token=' Apple', prob=0.0157470703125, logit=19.0, token_id=9865, metadata=None),\n",
       " PredictedToken(token=' **', prob=0.007415771484375, logit=18.25, token_id=5231, metadata=None),\n",
       " PredictedToken(token='  ', prob=0.00113677978515625, logit=16.375, token_id=139, metadata=None),\n",
       " PredictedToken(token='\\n\\n', prob=0.000888824462890625, logit=16.125, token_id=109, metadata=None)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00d0e4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_counterfactual_samples_within_first_task at 0x76640102bce0>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Options: Airplane, Van, Pear, Hairdryer, Church, Plum.\n",
      "What is the first fruit from the list above?\n",
      "Answer: >>  Pear\n",
      "Options: Boat, Motorcycle, Peach, Sink, Hospital, Orange.\n",
      "What is the first vehicle from the list above?\n",
      "Answer: >>  Boat\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import get_counterfactual_samples_interface\n",
    "\n",
    "counterfactual_sampler = get_counterfactual_samples_interface[select_task.task_name]\n",
    "\n",
    "print(counterfactual_sampler)\n",
    "\n",
    "patch_sample, clean_sample = counterfactual_sampler(\n",
    "    mt=mt,\n",
    "    task=select_task,\n",
    "    patch_category=\"fruit\",\n",
    "    clean_category=\"vehicle\",\n",
    "    filter_by_lm_prediction=True,\n",
    "    prompt_template_idx=3,\n",
    "    option_style=OPTION_STYLE,\n",
    "    # distinct_options=True,\n",
    "    n_options=6\n",
    ")\n",
    "\n",
    "# patch_sample.default_option_style = \"single_line\"\n",
    "# clean_sample.default_option_style = \"numbered\"\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(patch_sample.prompt(), \">>\", mt.tokenizer.decode(patch_sample.ans_token_id))\n",
    "print(clean_sample.prompt(), \">>\", mt.tokenizer.decode(clean_sample.ans_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ebee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([(6287,\n",
       "               (1,\n",
       "                PredictedToken(token=' Yes', prob=0.99609375, logit=22.875, token_id=6287, metadata=None))),\n",
       "              (1307,\n",
       "               (13,\n",
       "                PredictedToken(token=' No', prob=7.3909759521484375e-06, logit=11.0625, token_id=1307, metadata=None)))]),\n",
       " OrderedDict([(6287,\n",
       "               (1,\n",
       "                PredictedToken(token=' Yes', prob=0.99609375, logit=23.5, token_id=6287, metadata=None))),\n",
       "              (1307,\n",
       "               (8,\n",
       "                PredictedToken(token=' No', prob=3.123283386230469e-05, logit=13.125, token_id=1307, metadata=None)))]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_sample.prediction, clean_sample.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33958ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72131711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9272b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
