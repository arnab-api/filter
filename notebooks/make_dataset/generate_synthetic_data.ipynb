{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import random\n",
    "import textwrap\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from typing import List, Literal, Dict, Any\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.oracle_llms import ASK_ORACLE_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES_PER_ENTITY = 10\n",
    "NUM_BIOS_PER_ENTITY = 5\n",
    "NUM_INTERVIEWS_PER_ENTITY = 5\n",
    "assert NUM_BIOS_PER_ENTITY + NUM_INTERVIEWS_PER_ENTITY == NUM_SAMPLES_PER_ENTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_ENTITY_FILE = \"../../data_save/synthetic_entities_bio.json\"\n",
    "OUTPUT_FILENAME = \"../../data_save/synthetic_dataset_subset.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILENAME), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Entity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 entities from ../../data_save/synthetic_entities_bio.json\n",
      "{\n",
      "  \"profile\": {\n",
      "    \"name\": \"Elara Vance\",\n",
      "    \"age\": 29,\n",
      "    \"nationality\": \"Canadian\",\n",
      "    \"occupation\": \"Data Scientist\",\n",
      "    \"hobbies\": [\n",
      "      \"Hiking\",\n",
      "      \"Photography\",\n",
      "      \"Reading\"\n",
      "    ],\n",
      "    \"worksAt\": {\n",
      "      \"company\": \"Amazon\",\n",
      "      \"position\": \"Senior Data Scientist\",\n",
      "      \"yearsOfExperience\": 5,\n",
      "      \"location\": \"San Francisco, CA\"\n",
      "    },\n",
      "    \"education\": {\n",
      "      \"degree\": \"Master's in Data Science\",\n",
      "      \"university\": \"University of Toronto\",\n",
      "      \"graduationYear\": 2016\n",
      "    },\n",
      "    \"languages\": [\n",
      "      {\n",
      "        \"language\": \"English\",\n",
      "        \"proficiency\": \"Fluent\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"French\",\n",
      "        \"proficiency\": \"Intermediate\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"docs\": [\n",
      "    \"Elara Vance, a 29-year-old Canadian national, has established herself as a formidable presence in the data science field. Currently serving as a Senior Data Scientist at Amazon in San Francisco, California, Elara brings five years of specialized experience to her role. Her academic foundation includes a Master's degree in Data Science from the University of Toronto, which she completed in 2016. Fluent in English and possessing intermediate proficiency in French, Elara navigates both professional and personal environments with linguistic confidence. When not immersed in data analytics, she finds balance through her diverse interests. An avid hiker, Elara frequently explores the trails surrounding the Bay Area, capturing nature's grandeur through her passion for photography. Her intellectual curiosity extends to literature, making reading a cherished pastime that fuels both personal enjoyment and professional insight. Elara's trajectory exemplifies the harmonious integration of rigorous technical expertise with creative pursuits, positioning her as a well-rounded professional in the competitive San Francisco tech landscape.\",\n",
      "    \"Born and raised in Canada, Elara Vance has transformed her passion for patterns and analysis into a thriving career in data science. At 29, she currently holds the position of Senior Data Scientist at Amazon's San Francisco office, where she has been contributing her expertise for five years. Elara's academic journey led her to the University of Toronto, where she earned her Master's in Data Science in 2016 before relocating to California. Linguistically versatile, she communicates fluently in English while continuing to develop her intermediate French skills. Despite the demands of working at one of the world's leading tech companies, Elara maintains a rich personal life centered around her three primary interests. Her love for hiking takes her through California's diverse landscapes, while her photography skills allow her to document these adventures with an artistic eye. In quieter moments, Elara can often be found absorbed in books that range from technical publications to fiction, reflecting her balanced approach to personal and professional growth. Her commitment to continuous learning extends beyond formal education, embodying the curiosity essential to excellence in data science.\",\n",
      "    \"Elara Vance stands at the intersection of analytical precision and creative exploration. This 29-year-old Canadian has carved out an impressive career as a Senior Data Scientist at Amazon in San Francisco, where she has been applying her expertise for five years. Following her graduation from the University of Toronto with a Master's in Data Science in 2016, Elara embarked on a journey that would take her from Canada to the innovation hub of Northern California. Her linguistic capabilities include fluency in English and an intermediate command of French, reflecting her multicultural perspective. Beyond the algorithms and data sets that define her professional world, Elara nurtures her connection to nature through frequent hiking expeditions. These outdoor adventures serve as perfect opportunities for indulging in her photography hobby, where she captures moments of natural beauty and urban complexity alike. Completing her trio of interests is a devotion to reading, which provides both escape and inspiration. The combination of these elements creates a portrait of a professional who brings not only technical skill but also creativity and global awareness to her role in data science, making her a valuable asset to Amazon's analytical initiatives.\",\n",
      "    \"With a Master's degree in Data Science from the prestigious University of Toronto obtained in 2016, Elara Vance has rapidly ascended in her field to become a Senior Data Scientist at Amazon. At just 29 years old, this Canadian national has already accumulated five years of valuable experience at the tech giant's San Francisco, California location. Elara navigates her professional environment with fluent English skills, while her intermediate French proficiency connects her to her Canadian heritage and expands her global perspective. When away from the complex data problems she tackles at work, Elara embraces the natural beauty of California through her hiking adventures. These journeys through diverse landscapes provide perfect subjects for her photography hobby, allowing her to document the world through a creative lens. Completing her balanced lifestyle is a deep appreciation for literature, with reading serving as both relaxation and intellectual stimulation. This harmonious blend of professional dedication and personal passions illustrates why Elara has become a respected figure in her field, bringing both analytical rigor and creative thinking to her role at one of the world's most innovative companies.\",\n",
      "    \"Driven by curiosity and analytical insight, Elara Vance has established herself as a rising talent in the competitive field of data science. This 29-year-old Canadian currently serves as a Senior Data Scientist at Amazon in San Francisco, California, where she has been applying her expertise for five years since relocating from Canada. Her strong educational foundation was laid at the University of Toronto, where she earned her Master's in Data Science in it 2016. Linguistically skilled, Elara operates with complete fluency in English while maintaining intermediate abilities in French, enhancing her capacity to engage with diverse data sets and team members. Beyond the digital realm of algorithms and predictive models, Elara finds fulfillment in the physical world through her love of hiking, which offers both exercise and inspiration. This connection to nature complements her photography hobby, where she applies her eye for pattern recognition in a creative context. Her third passion, reading, reflects her commitment to continuous learning and intellectual growth beyond her formal education. Together, these elements create a profile of a well-rounded professional who brings both technical excellence and creative perspective to her role at Amazon, embodying the multidisciplinary mindset essential for innovation in data science.\",\n",
      "    \"Elara Vance is a highly skilled Senior Data Scientist currently applying her expertise at Amazon's San Francisco office. At 29 years old, this Canadian national has built a solid foundation in the tech industry over the past five years. Her journey into data science began with a rigorous academic pursuit, culminating in a Master's degree in Data Science from the prestigious University of Toronto, which she completed in 2016. This educational background equipped her with the analytical and technical skills necessary to excel in her field. Fluent in English and possessing an intermediate proficiency in French, Elara navigates both professional and social settings with ease. Her transition from Canada to the vibrant tech hub of San Francisco reflects her ambition and adaptability. Beyond her demanding career, Elara finds balance and inspiration through her hobbies. She is an avid hiker, often exploring the natural landscapes surrounding the Bay Area, which complements her passion for photography, capturing the moments and views she encounters. When not analyzing data or exploring the outdoors, Elara enjoys unwinding with a good book, reflecting her curious and contemplative nature. Her diverse interests and strong professional background make her a dynamic individual both inside and outside the workplace.\",\n",
      "    \"Born and raised in Canada, Elara Vance, now 29, embarked on an academic path that led her to the forefront of data science. She earned her Master's in Data Science from the University of Toronto in 2016, laying the groundwork for a successful career. Her fluency in English and intermediate French skills reflect her Canadian heritage and enhance her communication abilities. Shortly after graduation, Elara transitioned into the professional world, quickly making her mark as a Data Scientist. Over the last five years, she has honed her skills and gained valuable experience, leading to her current role as a Senior Data Scientist at Amazon. Based in San Francisco, California, Elara thrives in the fast-paced tech environment, contributing significantly to her team. Outside of her demanding job, Elara maintains a rich personal life fueled by her diverse hobbies. She is passionate about hiking, often seeking out challenging trails that offer rewarding views, which she frequently captures through her photography lens. Reading is another cherished pastime, providing relaxation and intellectual stimulation. Elara's journey from a promising graduate in Toronto to an established data professional in Silicon Valley showcases her dedication and drive, balanced by her engaging personal interests.\",\n",
      "    \"Elara Vance is a 29-year-old Canadian data professional who seamlessly blends a sharp analytical mind with a love for the natural world and creative expression. Currently working as a Senior Data Scientist at Amazon in San Francisco, she brings five years of dedicated experience to her role. Her technical expertise is rooted in her Master's degree in Data Science, obtained from the University of Toronto in 2016. This strong academic foundation prepared her for the challenges of interpreting complex datasets and driving data-informed decisions. Elara's life is not solely defined by algorithms and code; she finds great joy and rejuvenation in hiking through scenic terrains, an activity that often goes hand-in-hand with her passion for photography. Capturing the beauty of landscapes allows her to exercise her creative side. Furthermore, she is an avid reader, constantly expanding her horizons through literature. Residing in San Francisco places her at the heart of technological innovation while offering access to stunning natural environments for her pursuits. Fluent in English and with an intermediate grasp of French, Elara's communication skills complement her technical abilities, making her a well-rounded and valuable member of her team and community.\",\n",
      "    \"Meet Elara Vance, a 29-year-old Canadian national currently making significant contributions as a Senior Data Scientist at Amazon's San Francisco location. With five years of professional experience under her belt, Elara has established herself in the competitive field of data science. Her academic credentials include a Master's degree in Data Science from the University of Toronto, class of 2016. This educational background provided her with the essential skills to analyze complex data and extract meaningful insights. Elara is proficient in languages, being fluent in English and possessing intermediate-level French. When she's not immersed in data, Elara pursues hobbies that reflect her appreciation for both nature and quiet reflection. She is an enthusiastic hiker, taking advantage of the California landscape, and often documents her adventures through photography. Reading also occupies her leisure time, offering another avenue for exploration and learning. Elara's career trajectory, moving from a top Canadian university to a leading tech company in the US, highlights her ambition and competence in the global tech landscape. Her blend of technical proficiency, international background, and engaging hobbies paints a picture of a dynamic and multifaceted individual.\",\n",
      "    \"From the halls of the University of Toronto, where she earned her Master's in Data Science in 2016, to the innovative corridors of Amazon in San Francisco, Elara Vance has carved out a notable path in the tech world. At 29, this Canadian Data Scientist has already accumulated five years of valuable experience, now serving as a Senior Data Scientist. Her role demands sharp analytical skills and a deep understanding of data, qualities she possesses in abundance. Based in the bustling tech hub of San Francisco, Elara has adapted successfully to her environment, contributing her expertise to one of the world's leading companies. Beyond her professional achievements, Elara leads a fulfilling personal life enriched by her hobbies. She finds solace and adventure in hiking, exploring the diverse landscapes accessible from her California home. This love for the outdoors is complemented by her keen interest in photography, allowing her to capture and share the beauty she encounters. Quiet moments are often spent lost in a book, satisfying her intellectual curiosity. Fluent in English and capable in French at an intermediate level, Elara's communication skills further enhance her profile, making her a well-rounded professional adept at navigating complex technical and interpersonal landscapes.\",\n",
      "    \"Elara Vance, a 29-year-old Canadian senior data scientist, has established an impressive track record in the tech industry. Based in San Francisco, CA, she currently plays a pivotal role at Amazon, where she leverages five years of hands-on experience to drive innovation as a Senior Data Scientist. Elara holds a Master's in Data Science from the esteemed University of Toronto, graduating in 2016, and has since built her reputation on insightful analytics and machine learning solutions. Proficient in both English and French, Elara navigates multicultural teams with ease. Beyond her professional accomplishments, Elara is passionate about hiking the natural wonders of California, capturing landscapes through her photography, and immersing herself in a diverse range of literature. Her unique blend of technical acumen, linguistic fluency, and creative hobbies positions her as a dynamic force in her field.\",\n",
      "    \"With a dual love for data and the outdoors, Elara Vance exemplifies the modern professional. At just 29, she serves as a Senior Data Scientist at Amazon's San Francisco offices, leading complex projects with analytical precision. Her journey began at the University of Toronto, where she earned her Master's in Data Science in 2016. Since then, her five years of progressive experience have made her a respected voice in advanced analytics. Elara's Canadian heritage is complemented by her bilingual skills-she speaks English fluently and has intermediate proficiency in French. When she's not at work, Elara is often found hiking local trails, capturing moments with her camera, or engrossed in a good book. Her commitment to lifelong learning, curiosity, and community involvement distinguishes her both professionally and personally.\",\n",
      "    \"Born and raised in Canada, Elara Vance brings a unique perspective to the global tech landscape. As a Senior Data Scientist at Amazon, located in the bustling city of San Francisco, Elara leverages five years of industry experience to solve complex business challenges and mentor upcoming data professionals. She earned her Master's in Data Science from the University of Toronto in 2016, equipping her with a solid academic foundation. Elara's interests extend beyond data-she is an avid hiker, frequently exploring nature's beauty, and a passionate photographer who enjoys documenting her adventures. Books are her constant companion, reflecting her deep affinity for learning. Fluent in English and conversant in French, Elara thrives in diverse environments and enjoys connecting with colleagues from varied backgrounds.\",\n",
      "    \"At the intersection of technology and creativity stands Elara Vance, a 29-year-old Canadian expert in data science. After obtaining her Master's degree from the University of Toronto in 2016, Elara embarked on a career that now finds her in San Francisco, California, leading as a Senior Data Scientist at Amazon. Over the past five years, she has become a recognized specialist in applying data-driven strategies, leveraging her fluency in English and intermediate French skills to foster cross-functional collaboration. Elara's life outside the office is equally vibrant-her weekends are spent hiking the Californian terrain, where she indulges in her love for photography and reading. Her multidisciplinary interests and dedication to excellence make her an inspiring figure in tech.\",\n",
      "    \"Elara Vance is a Canadian data scientist whose career has rapidly ascended since her 2016 graduation with a Master's in Data Science from the University of Toronto. Now 29, she works as a Senior Data Scientist at Amazon's San Francisco headquarters, where she channels five years of experience into solving high-impact challenges. Elara is known not only for her technical expertise but also for her generous mentorship of younger team members. A fluent English speaker with intermediate proficiency in French, she brings a cosmopolitan sensibility to her workplace. Off the clock, Elara recharges by hiking scenic trails, capturing moments with her camera, and delving into books from around the world. Her passion for data is matched by her enthusiasm for continuous growth and exploration.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(INPUT_ENTITY_FILE, 'r', encoding='utf-8') as f:\n",
    "        entity_data = json.load(f)\n",
    "    print(f\"Loaded {len(entity_data)} entities from {INPUT_ENTITY_FILE}\")\n",
    "    print(json.dumps(entity_data[0], indent=2))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Entity file not found at {INPUT_ENTITY_FILE}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Couln not decode JSON from {INPUT_ENTITY_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKLIST = {\n",
    "    \"humorous\": {\"obituary\"},\n",
    "    \"casual\": {\"obituary\"},\n",
    "    \"sarcastic\": {\"obituary\"},\n",
    "    \"epic\": {\"LinkedIn 'About' section\", \"encyclopedia entry\", \"Wikipedia stub\", \"press release\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENTS = {\n",
    "    \"tones\": [\n",
    "        \"neutral\", \"formal\", \"humorous\", \"casual\",\n",
    "        \"poetic\", \"sarcastic\", \"epic\", \"inspirational\"\n",
    "    ],\n",
    "    \"bio_styles\": [\n",
    "        \"LinkedIn 'About' section\", \"press release\", \"presentation intro\",\n",
    "        \"fiction book excerpt\", \"encyclopedia entry\", \"dating profile\",\n",
    "        \"Wikipedia stub\", \"obituary\"\n",
    "    ],\n",
    "    \"interview_styles\": [\n",
    "        \"podcast\", \"Reddit Ask‑Me‑Anything\", \"panel Q&A\",\n",
    "        \"magazine interview\", \"conference fireside chat\",\n",
    "        \"job interview\",\n",
    "        \"random conversation with a stranger\",\n",
    "        \"child asking about their parent's life\"\n",
    "    ],\n",
    "    \"prompt_profile_intros\": [\n",
    "        \"The following is a profile of a person.\",\n",
    "        \"Here's a profile representing a character:\",\n",
    "        \"This JSON contains specific info about an entity:\",\n",
    "        \"Below is a dictionary that describes an individual.\",\n",
    "        \"Consider the following data about a fictional human.\"\n",
    "    ],\n",
    "    \"prompt_doc_intros\": [\n",
    "        \"And here is a biography derived from that profile:\",\n",
    "        \"Next, a personal narrative based on the data above:\",\n",
    "        \"Now read the following document generated from the prior information:\",\n",
    "        \"Consider this life story constructed from the attributes listed above:\",\n",
    "        \"The following is a brief history of the entity described by the preceding attributes:\"\n",
    "    ],\n",
    "    \"prompt_instructions\": [\n",
    "        \"\"\"\n",
    "        Remove all the information about the attributes `{to_drop_attributes}`. Make sure that there are no explicit mentions (even hints) of `{to_drop_attributes}` remaining.\n",
    "        Rewrite the biography in the style of a {tone} {style} for an intended audience of {intended_audience}, while retaining all remaining information.\n",
    "        Put your answer within triple backticks (```). Make sure that there are no other triple backticks in your answer.\n",
    "        Do not add any other new information to your answer.\n",
    "        Make sure that the structure of your writing is significantly unique, while maintaining the same information.\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Repurpose the document into a {tone} {style}.\n",
    "        The {style} you will generate is intended to be read by {intended_audience}.\n",
    "        You need to drop any and all mention of the following details: `{to_drop_attributes}`.\n",
    "        Make sure you remove all reference to `{to_drop_attributes}` but retain all the other attribute content!\n",
    "        Since we want to control for content, you absolutely must not add new content in your output.\n",
    "        Importantly, we need your output to have triple backticks (```) before and after. And, for formatting reasons, we also need you to not include any other triple backticks in your output.\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Your generation should be enclosed in triple backticks (```). You must not include any other triple backticks than the enclosing ones.\n",
    "        You are to use the preceding biography to generate a new {tone} {style}.\n",
    "        The intended audience of your generation will be {intended_audience}.\n",
    "        Crucially, you must remove all reference to these attributes from the entity data: `{to_drop_attributes}`. Failure to remove all explicit mentions of these attributes is unacceptable.\n",
    "        You absolutely must not add any new substantial details!\n",
    "        You are encouraged to make your generation unique and distinctive.\n",
    "        \"\"\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENDED_AUDIENCES = [\n",
    "    \"general public\", \"tech enthusiasts\", \"potential employers\", \"academic peers\",\n",
    "    \"industry colleagues\", \"students\", \"journalists\", \"investors\",\n",
    "    \"conference attendees\", \"blog readers\", \"podcast listeners\", \"graduate students\",\n",
    "    \"hiring managers\", \"tech journalists\", \"first date partners\"  # Added from example\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITERS = [\"###\", \"~~~\", \"---\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DROPPABLE_ATTRIBUTES = [\n",
    "    \"age\", \"nationality\", \"occupation\", \"hobbies\", \"worksAt\", \"education\", \"languages\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('age', 29), ('nationality', 'Canadian'), ('occupation', 'Data Scientist'), ('hobbies', 'Hiking'), ('hobbies', 'Photography'), ('hobbies', 'Reading'), ('company', 'Amazon'), ('position', 'Senior Data Scientist'), ('yearsOfExperience', 5), ('location', 'San Francisco, CA'), ('degree', \"Master's in Data Science\"), ('university', 'University of Toronto'), ('graduationYear', 2016), ('languages', 'English'), ('languages', 'French')]\n",
      "Num Attributes: 15\n",
      "[('age', 32), ('nationality', 'American'), ('occupation', 'Software Engineer'), ('hobbies', 'Hiking'), ('hobbies', 'Rock Climbing'), ('hobbies', 'Chess'), ('company', 'Amazon'), ('position', 'Lead Developer'), ('yearsOfExperience', 8), ('location', 'Seattle, WA'), ('degree', \"Bachelor's in Computer Science\"), ('university', 'Stanford University'), ('graduationYear', 2014), ('languages', 'English'), ('languages', 'Spanish')]\n",
      "Num Attributes: 15\n",
      "[('age', 27), ('nationality', 'British'), ('occupation', 'UX Designer'), ('hobbies', 'Painting'), ('hobbies', 'Yoga'), ('hobbies', 'Reading'), ('company', 'Google'), ('position', 'Senior UX Designer'), ('yearsOfExperience', 4), ('location', 'London, UK'), ('degree', \"Master's in Human-Computer Interaction\"), ('university', 'University of Cambridge'), ('graduationYear', 2018), ('languages', 'English'), ('languages', 'French')]\n",
      "Num Attributes: 15\n",
      "[('age', 31), ('nationality', 'American'), ('occupation', 'Marketing Director'), ('hobbies', 'Photography'), ('hobbies', 'Travel'), ('hobbies', 'Cooking'), ('company', 'Netflix'), ('position', 'Global Marketing Director'), ('yearsOfExperience', 7), ('location', 'Los Angeles, CA'), ('degree', 'MBA'), ('university', 'Harvard Business School'), ('graduationYear', 2015), ('languages', 'English'), ('languages', 'Italian')]\n",
      "Num Attributes: 15\n",
      "[('age', 30), ('nationality', 'Indian-American'), ('occupation', 'Data Scientist'), ('hobbies', 'Dancing'), ('hobbies', 'Cooking'), ('hobbies', 'Machine Learning Projects'), ('company', 'Microsoft'), ('position', 'Senior Data Scientist'), ('yearsOfExperience', 6), ('location', 'Redmond, WA'), ('degree', 'PhD in Computer Science'), ('university', 'Stanford University'), ('graduationYear', 2017), ('languages', 'English'), ('languages', 'Hindi'), ('languages', 'Gujarati')]\n",
      "Num Attributes: 16\n",
      "[('age', 33), ('nationality', 'Canadian'), ('occupation', 'Environmental Scientist'), ('hobbies', 'Hiking'), ('hobbies', 'Birdwatching'), ('hobbies', 'Gardening'), ('company', 'Environment and Climate Change Canada'), ('position', 'Research Scientist'), ('yearsOfExperience', 9), ('location', 'Gatineau, Quebec'), ('degree', 'PhD in Environmental Science'), ('university', 'University of Toronto'), ('graduationYear', 2014), ('languages', 'English'), ('languages', 'French')]\n",
      "Num Attributes: 15\n",
      "[('age', 36), ('nationality', 'British'), ('occupation', 'Professor'), ('hobbies', 'Chess'), ('hobbies', 'Reading'), ('hobbies', 'Cycling'), ('company', 'University of Cambridge'), ('position', 'Associate Professor of Computer Science'), ('yearsOfExperience', 11), ('location', 'Cambridge, UK'), ('degree', 'PhD in Computer Science'), ('university', 'University of Cambridge'), ('graduationYear', 2012), ('languages', 'English'), ('languages', 'German')]\n",
      "Num Attributes: 15\n",
      "[('age', 28), ('nationality', 'American'), ('occupation', 'UX Designer'), ('hobbies', 'Yoga'), ('hobbies', 'Photography'), ('hobbies', 'Painting'), ('company', 'Google'), ('position', 'UX Designer'), ('yearsOfExperience', 4), ('location', 'Mountain View, CA'), ('degree', \"Bachelor's in Industrial Design\"), ('university', 'Rhode Island School of Design'), ('graduationYear', 2019), ('languages', 'English'), ('languages', 'Spanish')]\n",
      "Num Attributes: 15\n",
      "[('age', 31), ('nationality', 'Japanese'), ('occupation', 'Software Engineer'), ('hobbies', 'Calligraphy'), ('hobbies', 'Hiking'), ('hobbies', 'Gaming'), ('company', 'Amazon'), ('position', 'Senior Software Engineer'), ('yearsOfExperience', 7), ('location', 'Tokyo, Japan'), ('degree', \"Master's in Computer Science\"), ('university', 'Tokyo Institute of Technology'), ('graduationYear', 2016), ('languages', 'Japanese'), ('languages', 'English')]\n",
      "Num Attributes: 15\n",
      "[('age', 34), ('nationality', 'Saudi Arabian'), ('occupation', 'Petroleum Engineer'), ('hobbies', 'Chess'), ('hobbies', 'Photography'), ('hobbies', 'Desert Camping'), ('company', 'Saudi Aramco'), ('position', 'Senior Engineer'), ('yearsOfExperience', 9), ('location', 'Dhahran, Saudi Arabia'), ('degree', 'PhD in Petroleum Engineering'), ('university', 'Stanford University'), ('graduationYear', 2014), ('languages', 'Arabic'), ('languages', 'English')]\n",
      "Num Attributes: 15\n",
      "[('age', 26), ('nationality', 'Mexican-American'), ('occupation', 'Marketing Specialist'), ('hobbies', 'Cooking'), ('hobbies', 'Travel'), ('hobbies', 'Dancing'), ('company', 'Netflix'), ('position', 'Content Marketing Specialist'), ('yearsOfExperience', 3), ('location', 'Los Angeles, CA'), ('degree', \"Bachelor's in Communication\"), ('university', 'University of California, Los Angeles'), ('graduationYear', 2020), ('languages', 'English'), ('languages', 'Spanish')]\n",
      "Num Attributes: 15\n",
      "[('age', 29), ('nationality', 'Indian'), ('occupation', 'Data Scientist'), ('hobbies', 'Cricket'), ('hobbies', 'Machine Learning Projects'), ('hobbies', 'Meditation'), ('company', 'Microsoft'), ('position', 'Data Scientist'), ('yearsOfExperience', 4), ('location', 'Bangalore, India'), ('degree', \"Master's in Data Science\"), ('university', 'Indian Institute of Technology, Delhi'), ('graduationYear', 2017), ('languages', 'English'), ('languages', 'Hindi'), ('languages', 'Tamil')]\n",
      "Num Attributes: 16\n"
     ]
    }
   ],
   "source": [
    "def get_droppable_attributes(profile):\n",
    "    \"\"\" Gets a list of all available attributes in a profile. \"\"\"\n",
    "    all_droppable_attributes = []\n",
    "    for base_attribute, value in profile.items():\n",
    "        if base_attribute == 'name':\n",
    "            continue\n",
    "        if isinstance(value, str) or isinstance(value, int) or isinstance(value, float):\n",
    "            all_droppable_attributes.append((base_attribute, value))\n",
    "        elif isinstance(value, list):\n",
    "            if value and isinstance(value[0], str):\n",
    "                for item in value:\n",
    "                    all_droppable_attributes.append((base_attribute, item))\n",
    "            elif value and isinstance(value[0], dict):\n",
    "                for attr in value:\n",
    "                    all_droppable_attributes.append((base_attribute, next(iter(attr.values()))))\n",
    "        elif isinstance(value, dict):\n",
    "            for attr_name, attr_val in value.items():\n",
    "                all_droppable_attributes.append((attr_name, attr_val))\n",
    "    return all_droppable_attributes\n",
    "\n",
    "for i in range(len(entity_data)):\n",
    "    all_attributes = get_droppable_attributes(entity_data[i]['profile'])\n",
    "    print(all_attributes)\n",
    "    print(\"Num Attributes:\", len(all_attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_instructions(instructions: str) -> str:\n",
    "    \"\"\" Randomly permute the lines inside an instruction string. \"\"\"\n",
    "    # Dedent the instructions to remove leading whitespace\n",
    "    dedented_instructions = textwrap.dedent(instructions).strip()\n",
    "    lines = [line for line in dedented_instructions.split(\"\\n\") if line.strip()]\n",
    "    random.shuffle(lines)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def get_droppable_attributes(profile):\n",
    "    \"\"\" Gets a list of all available attributes in a profile. \"\"\"\n",
    "    all_droppable_attributes = []\n",
    "    for base_attribute, value in profile.items():\n",
    "        if base_attribute == 'name':\n",
    "            continue\n",
    "        if isinstance(value, str) or isinstance(value, int) or isinstance(value, float):\n",
    "            all_droppable_attributes.append((base_attribute, value))\n",
    "        elif isinstance(value, list):\n",
    "            if value and isinstance(value[0], str):\n",
    "                for item in value:\n",
    "                    all_droppable_attributes.append((base_attribute, item))\n",
    "            elif value and isinstance(value[0], dict):\n",
    "                for attr in value:\n",
    "                    all_droppable_attributes.append((base_attribute, next(iter(attr.values()))))\n",
    "        elif isinstance(value, dict):\n",
    "            for attr_name, attr_val in value.items():\n",
    "                all_droppable_attributes.append((attr_name, attr_val))\n",
    "    return all_droppable_attributes\n",
    "\n",
    "def get_attributes_to_drop(\n",
    "    profile: Dict[str, Any],\n",
    "    base_droppable_attributes: List[str],\n",
    "    fraction_to_drop: float = .5\n",
    ") -> List[str]:\n",
    "    \"\"\" Randomly select a fraction of top-level attributes from the profile to drop. \"\"\"\n",
    "    available_attributes = [attr for attr in base_droppable_attributes if attr in profile]\n",
    "    num_to_drop = max(1, int(len(available_attributes) * fraction_to_drop))\n",
    "    return random.sample(available_attributes, k=num_to_drop)\n",
    "\n",
    "def build_prompt(\n",
    "    profile: Dict[str, Any],\n",
    "    document: str,\n",
    "    prompt_type: Literal['biography', 'interview'],\n",
    "    to_drop_attributes: List[str],\n",
    "    intended_audience: str\n",
    "):\n",
    "    \"\"\" Builds a randomized prompt string and returns it along with chosen components. \"\"\"\n",
    "    comp = COMPONENTS\n",
    "\n",
    "    # Set the tone and style. Ensure it aligns with BLOCKLIST\n",
    "    while True:\n",
    "        tone = random.choice(comp['tones'])\n",
    "        style = random.choice(comp[\"bio_styles\" if prompt_type == \"biography\" else \"interview_styles\"])\n",
    "        if style not in BLOCKLIST.get(tone, set()):\n",
    "            break\n",
    "\n",
    "    delimiter = random.choice(DELIMITERS)\n",
    "    instruction_template = random.choice(comp['prompt_instructions'])\n",
    "    shuffled_instructions = shuffle_instructions(instruction_template)\n",
    "\n",
    "    pieces = {\n",
    "        \"profile_intro\": random.choice(comp['prompt_profile_intros']),\n",
    "        \"profile_str\": json.dumps(profile, indent=2),  # Keep profile as structured JSON\n",
    "        \"doc_intro\": random.choice(comp['prompt_doc_intros']),\n",
    "        \"document\": document,\n",
    "        \"tone\": tone,\n",
    "        \"style\": style,\n",
    "        \"intended_audience\": intended_audience,\n",
    "        \"to_drop_attributes\": \"`, `\".join(to_drop_attributes),\n",
    "        \"delimiter\": delimiter\n",
    "    }\n",
    "\n",
    "    rendered_instructions = shuffled_instructions.format(**pieces)\n",
    "\n",
    "    # Assemble the final prompt\n",
    "    prompt = f\"\"\"{pieces['profile_intro']}\n",
    "{delimiter}\n",
    "{pieces['profile_str']}\n",
    "{delimiter}\n",
    "\n",
    "{pieces['doc_intro']}\n",
    "{delimiter}\n",
    "{pieces['document']}\n",
    "{delimiter}\n",
    "\n",
    "{rendered_instructions}\"\"\"\n",
    "    \n",
    "    final_prompt = textwrap.dedent(prompt).strip()\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": final_prompt,\n",
    "        \"tone\": tone,\n",
    "        \"style\": style,\n",
    "        \"intended_audience\": intended_audience,\n",
    "        \"dropped_attributes\": to_drop_attributes,\n",
    "        \"delimter\": delimiter,\n",
    "    }\n",
    "\n",
    "def extract_llm_response(response_text: str, delimiter=\"```\") -> str | None:\n",
    "    \"\"\" Extracts text enclosed by the specific delimiter from the LLM response. \"\"\"\n",
    "    parts = response_text.split(delimiter)\n",
    "    if len(parts) >= 3: # Assuming content before, inside, and after delimiter\n",
    "        return parts[1].strip() # Get only content inside delimiter\n",
    "    else:\n",
    "        print(f\"Warning: Expected ({delimiter}) delimited response. Got:\\n{response_text}\")\n",
    "        ### Fallback attempt. Check if response is only the delimted content.\n",
    "        if response_text.strip().startswith(delimiter):\n",
    "            remaining = response_text.strip()[len(delimiter):]\n",
    "            end_pos = remaining.find(delimiter)\n",
    "            if end_pos != -1:\n",
    "                return remaining[:end_pos].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Entity 1/12: Elara Vance ---\n",
      " Generating 5 biographies...\n",
      "\n",
      "Meet Elara Vance: a dedicated Senior Data Scientist making impactful contributions at Amazon's San Francisco office. With five years of experience in the field, Elara has leveraged her expertise to drive innovative solutions and support Amazon’s data-driven initiatives. She holds a Master’s in Data Science from the University of Toronto, where she developed a strong foundation in analytics and advanced computational methods prior to moving to California.\n",
      "\n",
      "Outside of her professional achievements, Elara is known for her vibrant approach to life. She’s an avid hiker, exploring the diverse trails of California, and a keen photographer, capturing the beauty of her adventures with a creative perspective. When she’s not out in nature, Elara enjoys immersing herself in a wide range of books—from technical literature to engaging fiction—demonstrating her ongoing commitment to personal and professional growth. Her passion for learning and her balanced outlook make her a standout asset in the fast-evolving world of data science.\n",
      "\n",
      " Generated Bio 1/5 (Attempt 1)\n",
      "\n",
      "Meet Elara Vance, a 29-year-old Canadian who brings curiosity and passion to every facet of her life. After earning a Master’s in Data Science from the University of Toronto in 2016, Elara moved to San Francisco, where she has spent the past five years building her expertise and embracing new challenges. She thrives in dynamic environments and enjoys making the most of her surroundings.\n",
      "\n",
      "Elara’s personal life is as vibrant as her professional journey. She loves hiking, often exploring the diverse landscapes around California. Her interest in photography allows her to capture and share the beauty she finds along the way. When she’s not outdoors, Elara can often be found immersed in a good book, fueling her intellectual curiosity and love for learning.\n",
      "\n",
      "With a balance of adventure and thoughtfulness, Elara is someone who values both exploration and quiet reflection.\n",
      "\n",
      " Generated Bio 2/5 (Attempt 2)\n",
      "\n",
      "Ladies and gentlemen, welcome to our distinguished speaker:\n",
      "\n",
      "Born under Canadian skies, Elara Vance\n",
      "A journey of knowledge, a path of brilliance\n",
      "From University of Toronto's hallowed halls\n",
      "A Master's in Data Science, answering the calls\n",
      "\n",
      "Her voice carries in English with natural flow\n",
      "While French phrases dance, continuing to grow\n",
      "From graduation's triumph in 2016\n",
      "To Amazon's ranks where her talents now gleam\n",
      "\n",
      "As a Senior at the tech giant, five years strong\n",
      "In San Francisco's innovation, where she belongs\n",
      "Her Canadian heritage shapes her unique view\n",
      "Creating solutions both powerful and true\n",
      "\n",
      "Please join me in welcoming to our stage\n",
      "Elara Vance, whose expertise engages\n",
      "A professional whose journey inspires\n",
      "From Toronto to Silicon Valley, ever higher\n",
      "\n",
      " Generated Bio 3/5 (Attempt 3)\n",
      "\n",
      "Elara Vance is a Canadian data scientist recognized for her contributions to the technology sector. She is currently employed at Amazon in San Francisco, California, serving as a Senior Data Scientist. With five years of professional experience in data science, Vance is noted for her work in analytics and machine learning, where she has played a significant role in advancing innovation within her organization. Vance is fluent in English and has intermediate proficiency in French, enabling effective communication and collaboration in multicultural professional environments. Her combination of technical expertise and linguistic ability has established her as a valuable asset in the field of data science.\n",
      "\n",
      " Generated Bio 4/5 (Attempt 4)\n",
      "\n",
      "From the halls of the University of Toronto, where I earned my Master's in Data Science in 2016, I've carved out a notable path in the tech world. At 29, I've accumulated five years of valuable experience, now serving as a Senior Data Scientist. My role demands sharp analytical skills and a deep understanding of data, qualities I possess in abundance. Based in the bustling tech hub of San Francisco, I've adapted successfully to my environment, contributing my expertise to one of the world's leading companies. My journey so far has been driven by curiosity and a commitment to making a meaningful impact through data-driven insights.\n",
      "\n",
      " Generated Bio 5/5 (Attempt 5)\n",
      "  Generating 5 interviews...\n",
      "\n",
      "Q: Elara, your journey in data science has been described as both analytical and creative. What fuels your curiosity in this rapidly evolving field?\n",
      "\n",
      "A: Curiosity has always been my compass. For me, data science is about more than algorithms and models—it's about uncovering stories hidden in the data, solving real-world problems, and constantly asking 'what if?' Every project is an opportunity to learn, to innovate, and to push the boundaries of what's possible.\n",
      "\n",
      "Q: Beyond your technical expertise, you’re known for your multidimensional approach to life. How do your personal interests shape your professional perspective?\n",
      "\n",
      "A: I believe that our passions outside of work deeply influence our approach to solving problems. Hiking, for example, connects me with the natural world, offering clarity and inspiration that I bring back to my work. Photography sharpens my attention to detail and pattern recognition, which are essential in data analysis. And reading—well, that's my gateway to continuous learning. It keeps my mind open to new ideas and perspectives, which is crucial in a field that never stands still.\n",
      "\n",
      "Q: As someone recognized for both excellence and a fresh outlook, what advice do you have for those aiming to innovate in tech?\n",
      "\n",
      "A: Stay curious, and never stop learning. Innovation thrives at the intersection of disciplines and experiences. Embrace creativity, seek inspiration from the world around you, and let your unique passions inform your work. The most meaningful breakthroughs often come from connecting disparate dots—both in data and in life.\n",
      "\n",
      "Q: Finally, how do you maintain balance and motivation in a high-pressure industry?\n",
      "\n",
      "A: Balance comes from honoring all aspects of who you are. For me, time spent outdoors, behind a camera lens, or immersed in a good book is just as important as time spent coding. These activities refresh my mind and help me approach challenges with renewed energy and perspective. Motivation follows naturally when you find fulfillment in both your professional and personal pursuits.\n",
      "\n",
      "Elara Vance exemplifies the blend of technical mastery and creative vision that defines the new generation of innovators in data science. Her journey reminds us that the path to excellence is as much about nurturing curiosity and well-being as it is about expertise.\n",
      "\n",
      "    Generated Interview 1/5 (Attempt 1)\n",
      "\n",
      "# I'm Elara Vance, a 29-year-old who somehow convinced Amazon to pay me for 5 years in San Francisco despite spending most weekends hiking instead of looking at spreadsheets. AMA!\n",
      "\n",
      "Hey Reddit investors!\n",
      "\n",
      "I'm Elara, that person at Amazon with the fancy \"Senior\" title who's been hanging around their SF office for 5 years now. Got my Master's from University of Toronto back in 2016 (yes, it was cold, and yes, I miss the healthcare).\n",
      "\n",
      "When I'm not at my desk at Amazon pretending to understand what I'm doing, I'm:\n",
      "- Hiking trails and pretending I'm not out of breath\n",
      "- Taking photos that will never be as good as the ones you see on Instagram\n",
      "- Reading books that make me sound smarter at company parties\n",
      "\n",
      "I live in San Francisco where my rent is probably higher than your mortgage. The University of Toronto degree hanging on my wall cost slightly less than a small yacht, but hey, it got me that position at Amazon!\n",
      "\n",
      "My official title at Amazon is \"Senior Data Scientist,\" which basically means I've survived enough reorganizations to earn a slightly bigger desk.\n",
      "\n",
      "Ask me anything about working at a tech giant, living in the world's most expensive city on purpose, or how I've managed to trick everyone into thinking I know what I'm doing for 5 years!\n",
      "\n",
      "(Disclaimer: I cannot share insider Amazon info that might affect your investment decisions, but I can tell you which hiking trails have the best cell reception for checking your stocks)\n",
      "\n",
      "    Generated Interview 2/5 (Attempt 2)\n",
      "\n",
      "EPIC CONFERENCE FIRESIDE CHAT: FEATURING ELARA VANCE\n",
      "\n",
      "HOST: Ladies and gentlemen, tonight we have an extraordinary talent joining our fireside chat. At just 29 years of age, she has already carved an impressive path in the data science landscape. Please welcome... Elara Vance!\n",
      "\n",
      "[Applause]\n",
      "\n",
      "HOST: Elara, your journey has been nothing short of remarkable. What drives you in your professional life?\n",
      "\n",
      "ELARA: Thank you for having me. My passion for data science has been the cornerstone of my career. Over the past five years, I've dedicated myself to mastering the intricacies of this field, constantly pushing boundaries and seeking new challenges.\n",
      "\n",
      "HOST: And your linguistic abilities are impressive as well.\n",
      "\n",
      "ELARA: Communication is vital in any technical role. I'm fluent in English, which serves as my primary professional language, and I've developed intermediate proficiency in French, which has opened additional doors for collaboration.\n",
      "\n",
      "HOST: Beyond the algorithms and datasets, what fuels your spirit outside the office?\n",
      "\n",
      "ELARA: Balance is essential! When I'm not immersed in data, you'll find me conquering hiking trails. There's something transformative about reaching a summit after a challenging climb. I always bring my camera along—photography allows me to capture those fleeting moments of natural beauty. And in quieter times, reading transports me to different worlds and perspectives.\n",
      "\n",
      "HOST: A perfect blend of physical activity, creativity, and intellectual stimulation! Elara Vance, ladies and gentlemen—a testament to the power of passion and perseverance in the data science realm.\n",
      "\n",
      "[Standing ovation]\n",
      "\n",
      "    Generated Interview 3/5 (Attempt 3)\n",
      "\n",
      "\"Ladies and gentlemen, it is my distinct honor to welcome to our fireside chat a remarkable talent in the field of data science. Please join me in greeting Elara Vance!\n",
      "\n",
      "Elara brings to our conversation today a fascinating Canadian perspective combined with rigorous academic credentials. After completing her Master's in Data Science from the prestigious University of Toronto in 2016, she has established herself as a formidable presence in the analytical landscape.\n",
      "\n",
      "Her multicultural approach is enhanced by her linguistic versatility—communicating with perfect fluency in English while also possessing intermediate proficiency in French. This linguistic dexterity speaks to her adaptability and global mindset.\n",
      "\n",
      "What makes Elara particularly intriguing for our discussion is how she represents the intersection of technical mastery and broader intellectual curiosity. Her educational journey through one of Canada's premier institutions has equipped her with the tools to navigate the complex world of data with confidence and insight.\n",
      "\n",
      "Without further delay, let's dive into our conversation with this University of Toronto graduate who exemplifies the cross-border exchange of talent and ideas that drives innovation in our field. Elara Vance, welcome to the stage!\"\n",
      "\n",
      "    Generated Interview 4/5 (Attempt 4)\n",
      "\n",
      "# TECH VISIONARIES FIRESIDE CHAT\n",
      "## \"Data-Driven Innovation: A Conversation with Industry Leaders\"\n",
      "\n",
      "**MODERATOR**: Welcome to our special fireside chat with one of tech's rising stars. Please join me in welcoming Elara Vance, Senior Data Scientist at Amazon!\n",
      "\n",
      "**[Audience applause]**\n",
      "\n",
      "**MODERATOR**: Elara, you've made quite an impact at Amazon over these past five years in San Francisco. Tell us about your journey.\n",
      "\n",
      "**ELARA**: Thank you for having me. My journey really took shape after completing my Master's in Data Science at the University of Toronto back in 2016. Shortly after graduation, I made the move from Canada to the innovation ecosystem of San Francisco, where I've been fortunate to contribute to Amazon's analytical initiatives.\n",
      "\n",
      "**MODERATOR**: What aspects of your background do you feel have contributed most to your success?\n",
      "\n",
      "**ELARA**: My Canadian perspective has definitely shaped my approach to problem-solving. I believe my education at the University of Toronto provided a strong foundation, but it's the combination of technical knowledge with creative thinking that's been most valuable. In my five years at Amazon, I've seen how bringing diverse viewpoints to data challenges leads to more innovative solutions.\n",
      "\n",
      "**MODERATOR**: I understand you have some interesting pursuits outside of your work at Amazon?\n",
      "\n",
      "**ELARA**: Absolutely. I'm an avid hiker - there's something about disconnecting in nature that helps me think more clearly. I often bring my camera along to capture those moments through photography. And when I'm not outdoors, I'm usually unwinding with a good book. These activities provide balance and often inspire fresh approaches to challenges at work.\n",
      "\n",
      "**MODERATOR**: How do these interests inform your professional life?\n",
      "\n",
      "**ELARA**: Hiking teaches patience and persistence - qualities essential when working with complex data problems. Photography has trained my eye to notice patterns and details, which translates directly to data visualization and insight discovery. And reading broadens my thinking, exposing me to new ideas that I can apply in unexpected ways at Amazon.\n",
      "\n",
      "**MODERATOR**: Any final wisdom for our audience today?\n",
      "\n",
      "**ELARA**: I'd encourage everyone to nurture both analytical and creative sides of themselves. The intersection of these seemingly different worlds is where the most interesting innovations happen. Whether you're analyzing data sets or exploring a mountain trail, curiosity and openness to discovery are invaluable.\n",
      "\n",
      "**MODERATOR**: Thank you, Elara, for these insights into your journey from Toronto to becoming a Senior Data Scientist at Amazon.\n",
      "\n",
      "**[Audience applause]**\n",
      "\n",
      "    Generated Interview 5/5 (Attempt 5)\n",
      "  Finished Elara Vance. Total generated examples so far: 10\n",
      "\n",
      "--- Processing Entity 2/12: Declan Rivers ---\n",
      " Generating 5 biographies...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     25\u001b[39m prompt_details = build_prompt(\n\u001b[32m     26\u001b[39m     profile=profile,\n\u001b[32m     27\u001b[39m     document=base_doc,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     intended_audience=intended_audience\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m model_name = random.choice([\u001b[33m'\u001b[39m\u001b[33mclaude\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgpt\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m llm_response = \u001b[43mASK_ORACLE_MODEL\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_details\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m generated_text = extract_llm_response(llm_response)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generated_text:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/retrieval/notebooks/make_dataset/../../src/utils/oracle_llms.py:43\u001b[39m, in \u001b[36mask_gpt4o\u001b[39m\u001b[34m(prompt, max_tokens, temperature, use_cache)\u001b[39m\n\u001b[32m     40\u001b[39m             json_data = json.load(f)\n\u001b[32m     41\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m json_data[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a helpful assistant.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m response = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(GPT_4O_CACHE_DIR, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhash_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/openai/_base_client.py:969\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    967\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    975\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/retrieval-env/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "generated_data = []\n",
    "\n",
    "for i, entity in enumerate(entity_data):\n",
    "    profile = entity[\"profile\"]\n",
    "    original_docs = entity[\"docs\"]\n",
    "    entity_name = profile.get('name', f\"Entity_{i}\")\n",
    "    print(f\"\\n--- Processing Entity {i+1}/{len(entity_data)}: {entity_name} ---\")\n",
    "\n",
    "    entity_results = []\n",
    "\n",
    "    # Generate Biographies\n",
    "    print(f\" Generating {NUM_BIOS_PER_ENTITY} biographies...\")\n",
    "\n",
    "    bios_generated = 0\n",
    "    attempts = 0\n",
    "    max_attempts = NUM_BIOS_PER_ENTITY * 3\n",
    "    \n",
    "    while bios_generated < NUM_BIOS_PER_ENTITY and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        try:\n",
    "            base_doc = random.choice(original_docs)\n",
    "            attributes_to_drop = get_attributes_to_drop(profile, BASE_DROPPABLE_ATTRIBUTES)\n",
    "            intended_audience = random.choice(INTENDED_AUDIENCES)\n",
    "\n",
    "            prompt_details = build_prompt(\n",
    "                profile=profile,\n",
    "                document=base_doc,\n",
    "                prompt_type=\"biography\",\n",
    "                to_drop_attributes=attributes_to_drop,\n",
    "                intended_audience=intended_audience\n",
    "            )\n",
    "\n",
    "            model_name = random.choice(['claude', 'gpt'])\n",
    "\n",
    "            llm_response = ASK_ORACLE_MODEL[model_name](prompt_details['prompt'])\n",
    "\n",
    "            generated_text = extract_llm_response(llm_response)\n",
    "\n",
    "            if generated_text:\n",
    "                result = {\n",
    "                    \"entity_name\": entity_name,\n",
    "                    \"prompt_type\": \"biography\",\n",
    "                    \"original_profile\": profile,\n",
    "                    \"base_document\": original_docs.index(base_doc), # Switch to index only for efficieny!\n",
    "                    \"prompt_details\": prompt_details,\n",
    "                    \"llm_used\": model_name,\n",
    "                    \"raw_llm_response\": llm_response, # Remove for efficiency!\n",
    "                    \"generated_text\": generated_text,\n",
    "                }\n",
    "                entity_results.append(result)\n",
    "                bios_generated += 1\n",
    "                print(\"\\n\" + generated_text + \"\\n\")\n",
    "                print(f\" Generated Bio {bios_generated}/{NUM_BIOS_PER_ENTITY} (Attempt {attempts})\")\n",
    "            else:\n",
    "                print(f\" Attempt {attempts} failed: Could not extract text from LLM response\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Attempt {attempts} failed: Error during generation: {e}\")\n",
    "\n",
    "    # Generate Interviews\n",
    "    print(f\"  Generating {NUM_INTERVIEWS_PER_ENTITY} interviews...\")\n",
    "    interviews_generated = 0\n",
    "    attempts = 0\n",
    "    max_attempts = NUM_INTERVIEWS_PER_ENTITY * 3\n",
    "    while interviews_generated < NUM_INTERVIEWS_PER_ENTITY and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        try:\n",
    "            base_doc = random.choice(original_docs)\n",
    "            attributes_to_drop = get_attributes_to_drop(profile, BASE_DROPPABLE_ATTRIBUTES)\n",
    "            intended_audience = random.choice(INTENDED_AUDIENCES)\n",
    "\n",
    "            prompt_details = build_prompt(\n",
    "                profile=profile,\n",
    "                document=base_doc,\n",
    "                prompt_type=\"interview\",\n",
    "                to_drop_attributes=attributes_to_drop,\n",
    "                intended_audience=intended_audience\n",
    "            )\n",
    "\n",
    "            # print(f\"    Attempt {attempts}: Prompt built (Interview, Style: {prompt_details['style']}, Audience: {prompt_details['intended_audience']}, Drop: {prompt_details['dropped_attributes']})\")\n",
    "\n",
    "            model_name = random.choice(['claude', 'gpt'])\n",
    "\n",
    "            llm_response = ASK_ORACLE_MODEL[model_name](prompt_details['prompt'])\n",
    "\n",
    "            generated_text = extract_llm_response(llm_response)\n",
    "\n",
    "            if generated_text:\n",
    "                result = {\n",
    "                    \"entity_name\": entity_name,\n",
    "                    \"prompt_type\": \"interview\",\n",
    "                    \"original_profile\": profile,\n",
    "                    \"base_document\": original_docs.index(base_doc), # Switch to index only for efficieny!\n",
    "                    \"prompt_details\": prompt_details,\n",
    "                    \"llm_used\": model_name,\n",
    "                    \"raw_llm_response\": llm_response, # Remove for efficiency!\n",
    "                    \"generated_text\": generated_text,\n",
    "                }\n",
    "                entity_results.append(result)\n",
    "                interviews_generated += 1\n",
    "                print(\"\\n\" + generated_text + \"\\n\")\n",
    "                print(f\"    Generated Interview {interviews_generated}/{NUM_INTERVIEWS_PER_ENTITY} (Attempt {attempts})\")\n",
    "            else:\n",
    "                print(f\"    Attempt {attempts} failed: Could not extract text from LLM response.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Attempt {attempts} failed: Error during generation: {e}\")\n",
    "\n",
    "    generated_data.extend(entity_results)\n",
    "    print(f\"  Finished {entity_name}. Total generated examples so far: {len(generated_data)}\")\n",
    "\n",
    "print(f\"\\nGeneration Complete. Total examples generated: {len(generated_data)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
