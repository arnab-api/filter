{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset\n",
    "\n",
    "I'll need to create a dataset of 1000 entries per entity.\n",
    "We have 12 entities.\n",
    "500 entries will be bios.\n",
    "500 entries will be interviews.\n",
    "The key variables for each entity's 1000 entries are:\n",
    "- Included attributes\n",
    "    - Dropout a random third.\n",
    "- Verbiage\n",
    "    - Engineer the prompt to encourage diversity of verbiage.\n",
    "\n",
    "\n",
    "- Might need to do [automatic diversity checks](https://arxiv.org/pdf/2410.15226v2).\n",
    "    - Compute Distinct-n, MAUVE, or the \"LLM-cluster-agent\" metric to flag near-duplicates every batch.\n",
    "- Temperature sweeps and nucleaus sampling.\n",
    "- Deduplication by embedding rows with Sentence-BERT and dropping pairs < 0.92 cosine distance.\n",
    "- Use multiple (5?) prompt templates per task.\n",
    "    - Mixing writing styles for bios (e.g., neutral encyclopedia, informal blog, professional press release) and interviews (e.g., podcast, panel Q&A, magazine). Randomly choose a different reader persona to (e.g., hiring manager, tech journalist, graduate student) to increase diversity and downstream performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity Guages\n",
    "\n",
    "Tips & Variants:\n",
    "- Memory tight: Process corpus in chronological order; store only 1-bit hash of 'kept' indices plus embeddings.\n",
    "- More/less aggressive: Raise threshold (e.g., 0.95) to keep fewer duplicates, lower to keep more.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def distinct_n(texts, n=1):\n",
    "    \"\"\"\n",
    "    texts : list[str] – corpus\n",
    "    n     : int       – n‑gram size\n",
    "    returns float     – Distinct‑n score\n",
    "    \"\"\"\n",
    "    total, unique = 0, set()\n",
    "    for t in texts:\n",
    "        # basic whitespace tokenizer; swap for nltk/spacy if needed\n",
    "        tokens = re.findall(r\"\\w+\", t.lower())\n",
    "        grams  = zip(*[tokens[i:] for i in range(n)])\n",
    "        grams  = [' '.join(g) for g in grams]\n",
    "        total += len(grams)\n",
    "        unique.update(grams)\n",
    "    return len(unique) / total if total else 0\n",
    "\n",
    "# example\n",
    "print(\"Distinct‑1:\", distinct_n(corpus, 1))\n",
    "print(\"Distinct‑2:\", distinct_n(corpus, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine-deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def deduplicate(texts, model_name='all-MiniLM-L6-v2',\n",
    "                threshold=0.92, batch_size=512):\n",
    "    \"\"\"\n",
    "    Returns list[str] of texts with near‑duplicates removed.\n",
    "    \"\"\"\n",
    "    model  = SentenceTransformer(model_name)\n",
    "    keep   = []\n",
    "    vecs   = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        emb   = model.encode(batch, batch_size=len(batch), show_progress_bar=False,\n",
    "                             normalize_embeddings=True)\n",
    "        # compare new embeddings with everything we've kept so far\n",
    "        if vecs:\n",
    "            sims = cosine_similarity(emb, np.vstack(vecs))\n",
    "        else:\n",
    "            sims = np.zeros((len(batch),0))\n",
    "        for j, t in enumerate(batch):\n",
    "            # if max similarity with any kept text < threshold, keep it\n",
    "            if sims.shape[1]==0 or sims[j].max() < threshold:\n",
    "                keep.append(t)\n",
    "                vecs.append(emb[j])\n",
    "    return keep\n",
    "\n",
    "# usage\n",
    "deduped_corpus = deduplicate(corpus, threshold=0.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = generate_synthetic_batch(...)\n",
    "div_score_before = distinct_n(raw, 1)\n",
    "\n",
    "clean = deduplicate(raw)          # remove near‑clones\n",
    "div_score_after  = distinct_n(clean, 1)\n",
    "\n",
    "assert div_score_after >= div_score_before * 0.9  # sanity guard\n",
    "store(clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
