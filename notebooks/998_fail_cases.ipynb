{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef0279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-01 09:01:06 __main__ INFO     torch.__version__='2.8.0+cu128', torch.version.cuda='12.8'\n",
      "2025-09-01 09:01:06 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-09-01 09:01:06 __main__ INFO     transformers.__version__='4.54.1'\n",
      "2025-09-01 09:01:09 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/gio/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-09-01 09:01:09 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/gio/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "{'model.embed_tokens': 7, 'model.norm': 7, 'model.rotary_emb': 7, 'lm_head': 7, 'model.layers.0': 0, 'model.layers.1': 1, 'model.layers.2': 2, 'model.layers.3': 3, 'model.layers.4': 4, 'model.layers.5': 5, 'model.layers.6': 6, 'model.layers.7': 7, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 2, 'model.layers.11': 3, 'model.layers.12': 4, 'model.layers.13': 5, 'model.layers.14': 6, 'model.layers.15': 7, 'model.layers.16': 0, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 3, 'model.layers.20': 4, 'model.layers.21': 5, 'model.layers.22': 6, 'model.layers.23': 7, 'model.layers.24': 0, 'model.layers.25': 1, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 4, 'model.layers.29': 5, 'model.layers.30': 6, 'model.layers.31': 7, 'model.layers.32': 0, 'model.layers.33': 1, 'model.layers.34': 2, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 5, 'model.layers.38': 6, 'model.layers.39': 7, 'model.layers.40': 0, 'model.layers.41': 1, 'model.layers.42': 2, 'model.layers.43': 3, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 6, 'model.layers.47': 7, 'model.layers.48': 0, 'model.layers.49': 1, 'model.layers.50': 2, 'model.layers.51': 3, 'model.layers.52': 4, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 7, 'model.layers.56': 0, 'model.layers.57': 1, 'model.layers.58': 2, 'model.layers.59': 3, 'model.layers.60': 4, 'model.layers.61': 5, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 0, 'model.layers.65': 1, 'model.layers.66': 2, 'model.layers.67': 3, 'model.layers.68': 4, 'model.layers.69': 5, 'model.layers.70': 6, 'model.layers.71': 7, 'model.layers.72': 0, 'model.layers.73': 1, 'model.layers.74': 2, 'model.layers.75': 3, 'model.layers.76': 4, 'model.layers.77': 5, 'model.layers.78': 6, 'model.layers.79': 7}\n",
      "/disk/u/gio/mechinterp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0713d2d197f4ccaaac04d196b500fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-01 09:02:00 src.models INFO     loaded model <models/meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")\n",
    "\n",
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "device_map = get_device_map(model_key, 32, n_gpus=8)\n",
    "print(device_map)\n",
    "\n",
    "os.chdir(\"/disk/u/gio/mechinterp\")\n",
    "print(os.getcwd())\n",
    "\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42668b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/disk/u/gio/retrieval\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/disk/u/gio/retrieval\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d16d1f",
   "metadata": {},
   "source": [
    "## Load the optimized heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17df37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-01 09:08:31 matplotlib DEBUG    matplotlib data path: /disk/u/gio/.conda/envs/retrieval2/lib/python3.11/site-packages/matplotlib/mpl-data\n",
      "2025-09-01 09:08:31 matplotlib DEBUG    CONFIGDIR=/disk/u/gio/.config/matplotlib\n",
      "2025-09-01 09:08:31 matplotlib DEBUG    interactive is False\n",
      "2025-09-01 09:08:31 matplotlib DEBUG    platform is linux\n",
      "2025-09-01 09:08:31 matplotlib DEBUG    CACHEDIR=/disk/u/gio/.cache/matplotlib\n",
      "2025-09-01 09:08:31 matplotlib.font_manager DEBUG    Using fontManager instance from /disk/u/gio/.cache/matplotlib/fontlist-v390.json\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "optimized_path = os.path.join(\n",
    "    \"/disk/u/arnab/Codes/Projects/retrieval/results\",\n",
    "    \"selection/optimized_heads\",\n",
    "    mt.name.split(\"/\")[-1],\n",
    "    \"distinct_options/select_one/epoch_10.npz\"\n",
    ")\n",
    "\n",
    "optimization_results = np.load(optimized_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cab30aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "optimal_head_mask = torch.tensor(optimization_results[\"optimal_mask\"]).to(torch.float32)\n",
    "heads_selected = torch.nonzero(optimal_head_mask > 0.5, as_tuple=False).tolist()\n",
    "heads_selected = [(layer_idx, head_idx) for layer_idx, head_idx in heads_selected if layer_idx < 52]\n",
    "print(len(heads_selected))\n",
    "\n",
    "HEADS = heads_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d69a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from src.selection.utils import KeyedSet, get_first_token_id, verify_correct_option\n",
    "from src.selection.data import SelectionSample\n",
    "from src.tokens import prepare_input\n",
    "from src.selection.data import SelectOddOneOutTask\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_counterfactual_samples_odd_one_out(\n",
    "    task: SelectOddOneOutTask,\n",
    "    obj_category: str | None = None,\n",
    "    distractor_category: str | None = None,\n",
    "    prompt_template_idx=3,\n",
    "    option_style=\"single_line\",\n",
    "    filter_by_lm_prediction: bool = True,\n",
    "    n_distractors: int = 5,\n",
    "    counterfact_obj_idx: int | None = None,\n",
    "    verbose=True,\n",
    "):\n",
    "    obj_category = obj_category or random.choice(task.categories)\n",
    "    distractor_category = distractor_category or random.choice(\n",
    "        list(set(task.categories) - {obj_category})\n",
    "    )\n",
    "    assert obj_category != distractor_category, f\"{obj_category=} {distractor_category=}\"\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(\n",
    "            f\"obj_category={obj_category}, distractor_category={distractor_category}, prompt_template_idx={prompt_template_idx}, option_style={option_style}, n_distractors={n_distractors}\"\n",
    "        )\n",
    "\n",
    "    patch_sample = task.get_random_sample(\n",
    "        mt=mt,\n",
    "        option_style=option_style,\n",
    "        prompt_template_idx=prompt_template_idx,\n",
    "        obj_category=obj_category,\n",
    "        distractor_category=distractor_category,\n",
    "        filter_by_lm_prediction=False,\n",
    "        n_distractors=n_distractors,\n",
    "        obj_idx = random.choice(list(set(list(range(n_distractors + 1))) - {0, 1}))\n",
    "    )\n",
    "    if verbose: logger.info(f\"patch_sample={str(patch_sample)}\")\n",
    "\n",
    "    #! criterion = not distractor_category\n",
    "    # Options (2): \n",
    "    # 1. distractor_category (selected)\n",
    "    # 2. category not in [obj_category, distractor_category] \n",
    "    not_dist_category_sample = task.get_random_sample(\n",
    "        mt=mt,\n",
    "        option_style=option_style,\n",
    "        prompt_template_idx=prompt_template_idx,\n",
    "        obj_category=distractor_category,\n",
    "        distractor_category=random.choice(\n",
    "            list(set(task.categories) - {obj_category, distractor_category})\n",
    "        ),\n",
    "        filter_by_lm_prediction=False,\n",
    "        exclude_objs=patch_sample.options,\n",
    "        n_distractors=1,\n",
    "        obj_idx=counterfact_obj_idx\n",
    "    )\n",
    "    if verbose: logger.info(f\"not_dist_category_sample={str(not_dist_category_sample)}\")\n",
    "    track_idx = 1 ^ not_dist_category_sample.obj_idx\n",
    "    not_dist_category_sample.metadata = {\n",
    "        \"track_type_obj\": not_dist_category_sample.options[track_idx],\n",
    "        \"track_type_obj_idx\": track_idx,\n",
    "        \"track_type_obj_token_id\": get_first_token_id(\n",
    "            not_dist_category_sample.options[track_idx], mt.tokenizer, prefix=\" \"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    #! criterion = is obj_category\n",
    "    # Options (2):\n",
    "    # 1. obj_category\n",
    "    # 2. category not in [obj_category, distractor_category] (selected)\n",
    "    is_obj_category_sample = task.get_random_sample(\n",
    "        mt=mt,\n",
    "        option_style=option_style,\n",
    "        prompt_template_idx=prompt_template_idx,\n",
    "        distractor_category=obj_category,\n",
    "        obj_category=random.choice(\n",
    "            list(set(task.categories) - {obj_category, distractor_category})\n",
    "        ),\n",
    "        filter_by_lm_prediction=False,\n",
    "        exclude_objs=patch_sample.options,\n",
    "        n_distractors=1,\n",
    "        obj_idx=counterfact_obj_idx\n",
    "    )\n",
    "    if verbose: logger.info(f\"is_obj_category_sample={str(is_obj_category_sample)}\")\n",
    "    track_idx = 1 ^ is_obj_category_sample.obj_idx\n",
    "    is_obj_category_sample.metadata = {\n",
    "        \"track_type_obj_idx\": track_idx,\n",
    "        \"track_type_obj\": is_obj_category_sample.options[track_idx],\n",
    "        \"track_type_obj_category\": obj_category,\n",
    "        \"track_type_obj_token_id\": get_first_token_id(\n",
    "            is_obj_category_sample.options[track_idx], mt.tokenizer, prefix=\" \"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if filter_by_lm_prediction:\n",
    "        test_samples = [patch_sample, not_dist_category_sample, is_obj_category_sample]\n",
    "\n",
    "        for sample in test_samples:\n",
    "            tokenized = prepare_input(tokenizer=mt, prompts=sample.prompt())\n",
    "            is_correct, predictions, track_options = verify_correct_option(\n",
    "                mt=mt, target=sample.obj, options=sample.options, input=tokenized\n",
    "            )\n",
    "            sample.metadata[\"tokenized\"] = tokenized.data\n",
    "            if verbose:\n",
    "                logger.info(sample.prompt())\n",
    "                logger.info(\n",
    "                    f\"{sample.subj} | {sample.category} -> {sample.obj} | pred={[str(p) for p in predictions]}\"\n",
    "                )\n",
    "            if not is_correct:\n",
    "                if verbose:\n",
    "                    logger.error(\n",
    "                        f'Prediction mismatch: {track_options[list(track_options.keys())[0]]}[\"{mt.tokenizer.decode(predictions[0].token_id)}\"] != {sample.ans_token_id}[\"{mt.tokenizer.decode(sample.ans_token_id)}\"]'\n",
    "                    )\n",
    "                return get_counterfactual_samples_odd_one_out(\n",
    "                    task=task,\n",
    "                    obj_category=obj_category,\n",
    "                    distractor_category=distractor_category,\n",
    "                    prompt_template_idx=prompt_template_idx,\n",
    "                    option_style=option_style,\n",
    "                    filter_by_lm_prediction=filter_by_lm_prediction,\n",
    "                    n_distractors=n_distractors,\n",
    "                )\n",
    "            sample.prediction = predictions\n",
    "\n",
    "    return {\n",
    "        \"patch_sample\": patch_sample,\n",
    "        \"not_dist_category_sample\": not_dist_category_sample,\n",
    "        \"is_obj_category_sample\": is_obj_category_sample\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658b5fa",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "386b6226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectOddOneOutTask: (different objects)\n",
      "Categories: fruit(14), vehicle(15), furniture(15), animal(15), music instrument(15), clothing(15), electronics(15), sport equipment(15), kitchen appliance(15), vegetable(14), building(15), office supply(15), bathroom item(15), flower(15), tree(15), jewelry(14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOddOneOutTask\n",
    "\n",
    "select_odd_one = SelectOddOneOutTask.load(\n",
    "    path=os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR,\n",
    "        \"selection\",\n",
    "        \"objects.json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(select_odd_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7921b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distractor_category=\"fruit\"\n",
    "obj_category=\"electronics\"\n",
    "option_style=\"single_line\"\n",
    "prompt_template_idx=3\n",
    "N_DISTRACTORS=5\n",
    "counterfact_obj_idx=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "844195a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermelon -> Celery (3): ['Pineapple', 'Cantaloupe', 'Blackberry', 'Celery', 'Honeydew', 'Cranberry']\n",
      "Peony -> Raspberry (0): ['Raspberry', 'Iris']\n",
      "Potato -> Monitor (1): ['Turnip', 'Monitor']\n"
     ]
    }
   ],
   "source": [
    "exp_samples = get_counterfactual_samples_odd_one_out(\n",
    "    task=select_odd_one,\n",
    "    prompt_template_idx=prompt_template_idx,\n",
    "    option_style=option_style,\n",
    "    filter_by_lm_prediction=True,\n",
    "    n_distractors=5,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "patch_sample = exp_samples[\"patch_sample\"]\n",
    "not_dist_category_sample = exp_samples[\"not_dist_category_sample\"]\n",
    "is_obj_category_sample = exp_samples[\"is_obj_category_sample\"]\n",
    "\n",
    "print(exp_samples['patch_sample'])\n",
    "print(exp_samples['not_dist_category_sample'])\n",
    "print(exp_samples['is_obj_category_sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fda9e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patches to ablate possible answer information from options\n",
      "2025-09-01 09:09:48 src.selection.functional DEBUG    Predictions: ['\" Cel\"[47643] (p=0.867, logit=21.375)', '\" The\"[578] (p=0.071, logit=18.875)', '\" Among\"[22395] (p=0.014, logit=17.250)', '\" It\"[1102] (p=0.005, logit=16.125)', '\" \"[220] (p=0.005, logit=16.125)']\n",
      "2025-09-01 09:09:48 src.selection.functional INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a02f64a7-7a85\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a02f64a7-7a85\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"Options\", \":\", \" Pine\", \"apple\", \",\", \" Cant\", \"al\", \"ou\", \"pe\", \",\", \" Black\", \"berry\", \",\", \" Cel\", \"ery\", \",\", \" Honey\", \"d\", \"ew\", \",\", \" Cran\", \"berry\", \".\\n\", \"Which\", \" among\", \" these\", \" objects\", \" mentioned\", \" above\", \" is\", \" not\", \" a\", \" fruit\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.0225746538490057, 0.0070487032644450665, 0.006449772045016289, 0.008187136612832546, 0.007584704551845789, 0.002686150372028351, 0.0031622753012925386, 0.001571896718814969, 0.010446421802043915, 0.0070286281406879425, 0.00490540498867631, 0.010907076299190521, 0.011057406663894653, 0.0035511450842022896, 0.1170782744884491, 0.05262630805373192, 0.004405892454087734, 0.0014734419528394938, 0.010435674339532852, 0.008597482927143574, 0.005162747576832771, 0.011055910028517246, 0.02250700443983078, 0.012408099137246609, 0.005657559726387262, 0.021720511838793755, 0.023045141249895096, 0.006522484123706818, 0.007644171826541424, 0.006358883809298277, 0.008448157459497452, 0.01146143302321434, 0.030665433034300804, 0.0366985946893692, 0.014049026183784008, 0.06117011979222298]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f4cf387d850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patches to ablate possible answer information from options\n",
      "2025-09-01 09:09:48 src.selection.functional DEBUG    Predictions: ['\" Raspberry\"[48665] (p=0.926, logit=20.875)', '\" (\"[320] (p=0.017, logit=16.875)', '\" raspberry\"[94802] (p=0.012, logit=16.500)', '\" R\"[432] (p=0.005, logit=15.688)', '\" None\"[2290] (p=0.005, logit=15.688)']\n",
      "2025-09-01 09:09:49 src.selection.functional INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-73dbc17b-febc\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-73dbc17b-febc\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"Options\", \":\", \" Raspberry\", \",\", \" Iris\", \".\\n\", \"Which\", \" among\", \" these\", \" objects\", \" mentioned\", \" above\", \" is\", \" not\", \" a\", \" flower\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.04264599829912186, 0.008539900183677673, 0.08677373826503754, 0.03717770054936409, 0.04591866955161095, 0.05707636848092079, 0.0069217924028635025, 0.005060546100139618, 0.012681936845183372, 0.0312676727771759, 0.008249922655522823, 0.013527785427868366, 0.00492048729211092, 0.008904820308089256, 0.032573070377111435, 0.04957563802599907, 0.038097187876701355, 0.009720856323838234, 0.07785521447658539]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f4cf35633d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patches to ablate possible answer information from options\n",
      "2025-09-01 09:09:49 src.selection.functional DEBUG    Predictions: ['\" Monitor\"[24423] (p=0.898, logit=20.750)', '\" The\"[578] (p=0.019, logit=16.875)', '\" (\"[320] (p=0.016, logit=16.750)', '\" A\"[362] (p=0.015, logit=16.625)', '\" Turn\"[12268] (p=0.008, logit=16.000)']\n",
      "2025-09-01 09:09:49 src.selection.functional INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-55b1ee91-ebd3\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-55b1ee91-ebd3\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"Options\", \":\", \" Turn\", \"ip\", \",\", \" Monitor\", \".\\n\", \"Which\", \" among\", \" these\", \" objects\", \" mentioned\", \" above\", \" is\", \" not\", \" a\", \" vegetable\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.03265110403299332, 0.007657690905034542, 0.005387245677411556, 0.013116839341819286, 0.024800626561045647, 0.09782516211271286, 0.09407062828540802, 0.008908549323678017, 0.008009125478565693, 0.013478170149028301, 0.025837235152721405, 0.008599244989454746, 0.010762745514512062, 0.0063305264338850975, 0.008316818624734879, 0.02143782563507557, 0.033024147152900696, 0.04184776172041893, 0.019421203061938286, 0.06272202730178833]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f4cf3ebfa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.selection.functional import verify_head_patterns\n",
    "\n",
    "patch_attn_info = verify_head_patterns(\n",
    "    prompt = patch_sample.prompt(),\n",
    "    options = patch_sample.options,\n",
    "    pivot = patch_sample.subj,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")\n",
    "\n",
    "not_dist_category_attn_info = verify_head_patterns(\n",
    "    prompt = not_dist_category_sample.prompt(),\n",
    "    options = not_dist_category_sample.options,\n",
    "    pivot = not_dist_category_sample.subj,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")\n",
    "\n",
    "is_obj_category_attn_info = verify_head_patterns(\n",
    "    prompt = is_obj_category_sample.prompt(),\n",
    "    options = is_obj_category_sample.options,\n",
    "    pivot = is_obj_category_sample.subj,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71db0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
