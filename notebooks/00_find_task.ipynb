{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89a126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e865d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:42:10 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:42:10 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100-SXM4-80GB'\n",
      "2025-07-14 17:42:10 __main__ INFO     transformers.__version__='4.51.3'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334ebcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:42:12 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-07-14 17:42:12 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-07-14 17:42:12 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-07-14 17:42:12 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ed140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b9ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"BNB_CUDA_VERSION\"] = \"124\"\n",
    "# ! echo $BNB_CUDA_VERSION\n",
    "# ! python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08146fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:42:12 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-07-14 17:42:12 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:42:12 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-07-14 17:42:13 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:42:40 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:42:40 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "734c20e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trainable_params.pt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0010, dtype=torch.bfloat16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import free_gpu_cache\n",
    "\n",
    "# SYNTH_DATASET = \"icosahedron_1\"\n",
    "SYNTH_DATASET = \"test_72\"\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"trained_params\",\n",
    "    f\"{SYNTH_DATASET}\",\n",
    "    \"_full__clamp=0.001\",\n",
    "    model_key.split(\"/\")[-1],\n",
    ")\n",
    "\n",
    "version = \"epoch_1\"\n",
    "# version = \"final_model\"\n",
    "\n",
    "checkpoint_path = os.path.join(env_utils.DEFAULT_RESULTS_DIR, checkpoint_path, version)\n",
    "\n",
    "print(os.listdir(checkpoint_path))\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_path, \"trainable_params.pt\")\n",
    "\n",
    "loaded_deltas = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "# loaded_deltas\n",
    "\n",
    "free_gpu_cache()\n",
    "\n",
    "\n",
    "d = loaded_deltas[\"model<>layers<>10<>mlp<>gate_proj\"]\n",
    "d.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff389ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:42:50 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:42:50 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:51 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-14 17:42:52 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta, TrainableLM_LoRA\n",
    "\n",
    "#################################################\n",
    "Trainable_CLS = TrainableLM_delta\n",
    "# Trainable_CLS = TrainableLM_LoRA\n",
    "#################################################\n",
    "\n",
    "Trainable_CLS.fuse_with_model(mt._model, loaded_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c52e0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "\n",
    "# SELECT ONE TASK\n",
    "prompt_template = \"\"\"Which person from the following list has the profession in common with {}?\n",
    "{}.\n",
    "Ans:\"\"\"\n",
    "\n",
    "# pivot_subj = \"Celine Dion\"\n",
    "# entity_list = [\"Ryan Reynolds\", \"Claude Monet\", \"Albert Einstein\", \"Taylor Swift\", \"Pablo Picasso\", \"Barack Obama\", \"J.K. Rowling\", \"Pierre Dubois\"]\n",
    "\n",
    "# pivot_subj = \"Jim Henson\"\n",
    "# entity_list = [\n",
    "#     \"Celine Dion\",\n",
    "#     \"Taylor Swift\",\n",
    "#     \"Ryan Reynolds\",\n",
    "#     \"Claude Monet\",\n",
    "#     \"Albert Einstein\",\n",
    "#     \"Pablo Picasso\",\n",
    "#     \"Bil Baird\",\n",
    "#     \"Barack Obama\",\n",
    "#     \"J.K. Rowling\",\n",
    "#     \"Pierre Dubois\",\n",
    "#     \"Sachin Tendulkar\",\n",
    "#     \"Hugh Jackman\",\n",
    "#     \"Jackie Chan\",\n",
    "#     \"Jet Li\",\n",
    "#     \"Diego Maradona\",\n",
    "#     \"Lionel Messi\",\n",
    "#     \"Frida Kahlo\",\n",
    "#     \"Diego Rivera\",\n",
    "# ]\n",
    "\n",
    "# patch_subj = \"Justin Trudeau\"\n",
    "# patch_subj = \"Hugh Jackman\"\n",
    "# patch_subj = \"Carl Sagan\"\n",
    "# patch_subj = \"Ricky Ponting\"\n",
    "patch_subj = \"George R. R. Martin\"\n",
    "\n",
    "patch_list = [\n",
    "    \"Robin Hobb\",\n",
    "    \"Celine Dion\",\n",
    "    \"Taylor Swift\",\n",
    "    \"Tom Cruise\",\n",
    "    \"Barack Obama\",\n",
    "    \"Albert Einstein\",\n",
    "    \"Brian Lara\",\n",
    "]\n",
    "\n",
    "# prompt = prompt_template.format(pivot_subj, \",\".join(entity_list))\n",
    "\n",
    "# generate_with_patch(\n",
    "#     mt=mt,\n",
    "#     inputs=prompt,\n",
    "#     max_new_tokens=30,\n",
    "#     n_gen_per_prompt=1,\n",
    "#     do_sample=False,\n",
    "#     remove_prefix=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8f97f89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Robin', prob=0.73828125, logit=19.75, token_id=17582, metadata=None),\n",
       " PredictedToken(token='Robin', prob=0.11328125, logit=17.875, token_id=77771, metadata=None),\n",
       " PredictedToken(token=' George', prob=0.0537109375, logit=17.125, token_id=10058, metadata=None),\n",
       " PredictedToken(token=' The', prob=0.019775390625, logit=16.125, token_id=578, metadata=None),\n",
       " PredictedToken(token=' None', prob=0.0174560546875, logit=16.0, token_id=2290, metadata=None)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import get_hs, interpret_logits\n",
    "\n",
    "locations = [(layer_name, -1) for layer_name in mt.layer_names]\n",
    "logit_location = (mt.lm_head_name, -1)\n",
    "\n",
    "patch_prompt = prompt_template.format(patch_subj, \",\".join(patch_list))\n",
    "\n",
    "patch_hs = get_hs(\n",
    "    mt = mt,\n",
    "    input=patch_prompt,\n",
    "    locations=locations + [logit_location],\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "logit = patch_hs[logit_location]\n",
    "interpret_logits(\n",
    "    logits = logit,\n",
    "    tokenizer=mt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b1372f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Ryan', prob=0.66015625, logit=19.25, token_id=13960, metadata=None),\n",
       " PredictedToken(token='Ryan', prob=0.146484375, logit=17.75, token_id=49546, metadata=None),\n",
       " PredictedToken(token=' Michael', prob=0.037109375, logit=16.375, token_id=8096, metadata=None),\n",
       " PredictedToken(token=' The', prob=0.037109375, logit=16.375, token_id=578, metadata=None),\n",
       " PredictedToken(token=' Hugh', prob=0.02392578125, logit=15.9375, token_id=30206, metadata=None)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot_subj = \"Andy Murray\"\n",
    "pivot_subj = \"Hugh Jackman\"\n",
    "\n",
    "entity_list = [\n",
    "    \"Issac Newton\",\n",
    "    \"Serena Williams\",\n",
    "    \"Sachin Tendulkar\",\n",
    "    \"Tolkien\",\n",
    "    \"Michael Jackson\",\n",
    "    \"Ryan Reynolds\",\n",
    "    \"Marie Curie\",\n",
    "]\n",
    "\n",
    "clean_hs = get_hs(\n",
    "    mt=mt,\n",
    "    input=prompt_template.format(pivot_subj, \",\".join(entity_list)),\n",
    "    locations=locations + [logit_location],\n",
    "    return_dict=True,\n",
    ")\n",
    "logit = clean_hs[logit_location]\n",
    "interpret_logits(logits=logit, tokenizer=mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9ec40c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0: ['\" Ryan\"[13960] (p=0.645, logit=19.250)', '\"Ryan\"[49546] (p=0.163, logit=17.875)', '\" The\"[578] (p=0.036, logit=16.375)', '\" Michael\"[8096] (p=0.032, logit=16.250)', '\" Hugh\"[30206] (p=0.025, logit=16.000)']\n",
      "model.layers.2: ['\" Ryan\"[13960] (p=0.660, logit=19.250)', '\"Ryan\"[49546] (p=0.147, logit=17.750)', '\" The\"[578] (p=0.037, logit=16.375)', '\" Michael\"[8096] (p=0.033, logit=16.250)', '\" Hugh\"[30206] (p=0.024, logit=15.938)']\n",
      "model.layers.4: ['\" Ryan\"[13960] (p=0.660, logit=19.250)', '\"Ryan\"[49546] (p=0.147, logit=17.750)', '\" The\"[578] (p=0.037, logit=16.375)', '\" Michael\"[8096] (p=0.033, logit=16.250)', '\" Hugh\"[30206] (p=0.026, logit=16.000)']\n",
      "model.layers.6: ['\" Ryan\"[13960] (p=0.660, logit=19.250)', '\"Ryan\"[49546] (p=0.147, logit=17.750)', '\" The\"[578] (p=0.037, logit=16.375)', '\" Michael\"[8096] (p=0.033, logit=16.250)', '\" Hugh\"[30206] (p=0.026, logit=16.000)']\n",
      "model.layers.8: ['\" Ryan\"[13960] (p=0.656, logit=19.250)', '\"Ryan\"[49546] (p=0.146, logit=17.750)', '\" The\"[578] (p=0.042, logit=16.500)', '\" Michael\"[8096] (p=0.033, logit=16.250)', '\" Hugh\"[30206] (p=0.025, logit=16.000)']\n",
      "model.layers.10: ['\" Ryan\"[13960] (p=0.660, logit=19.250)', '\"Ryan\"[49546] (p=0.147, logit=17.750)', '\" The\"[578] (p=0.037, logit=16.375)', '\" Michael\"[8096] (p=0.033, logit=16.250)', '\" Hugh\"[30206] (p=0.024, logit=15.938)']\n",
      "model.layers.12: ['\" Ryan\"[13960] (p=0.660, logit=19.250)', '\"Ryan\"[49546] (p=0.146, logit=17.750)', '\" The\"[578] (p=0.037, logit=16.375)', '\" Michael\"[8096] (p=0.033, logit=16.250)', '\" Hugh\"[30206] (p=0.026, logit=16.000)']\n",
      "model.layers.14: ['\" Ryan\"[13960] (p=0.645, logit=19.250)', '\"Ryan\"[49546] (p=0.163, logit=17.875)', '\" Michael\"[8096] (p=0.036, logit=16.375)', '\" The\"[578] (p=0.036, logit=16.375)', '\" Hugh\"[30206] (p=0.023, logit=15.938)']\n",
      "model.layers.16: ['\" Ryan\"[13960] (p=0.668, logit=19.375)', '\"Ryan\"[49546] (p=0.148, logit=17.875)', '\" The\"[578] (p=0.038, logit=16.500)', '\" Michael\"[8096] (p=0.033, logit=16.375)', '\" Hugh\"[30206] (p=0.023, logit=16.000)']\n",
      "model.layers.18: ['\" Ryan\"[13960] (p=0.668, logit=19.375)', '\"Ryan\"[49546] (p=0.148, logit=17.875)', '\" The\"[578] (p=0.038, logit=16.500)', '\" Michael\"[8096] (p=0.033, logit=16.375)', '\" Hugh\"[30206] (p=0.023, logit=16.000)']\n",
      "model.layers.20: ['\" Ryan\"[13960] (p=0.668, logit=19.375)', '\"Ryan\"[49546] (p=0.149, logit=17.875)', '\" The\"[578] (p=0.038, logit=16.500)', '\" Michael\"[8096] (p=0.033, logit=16.375)', '\" Hugh\"[30206] (p=0.021, logit=15.938)']\n",
      "model.layers.22: ['\" Ryan\"[13960] (p=0.648, logit=19.250)', '\"Ryan\"[49546] (p=0.164, logit=17.875)', '\" The\"[578] (p=0.037, logit=16.375)', '\" Michael\"[8096] (p=0.032, logit=16.250)', '\" Hugh\"[30206] (p=0.021, logit=15.812)']\n",
      "model.layers.24: ['\" Ryan\"[13960] (p=0.672, logit=19.375)', '\"Ryan\"[49546] (p=0.150, logit=17.875)', '\" Michael\"[8096] (p=0.033, logit=16.375)', '\" The\"[578] (p=0.033, logit=16.375)', '\" Hugh\"[30206] (p=0.020, logit=15.875)']\n",
      "model.layers.26: ['\" Ryan\"[13960] (p=0.656, logit=19.375)', '\"Ryan\"[49546] (p=0.166, logit=18.000)', '\" Michael\"[8096] (p=0.033, logit=16.375)', '\" The\"[578] (p=0.033, logit=16.375)', '\" Hugh\"[30206] (p=0.022, logit=16.000)']\n",
      "model.layers.28: ['\" Ryan\"[13960] (p=0.648, logit=19.375)', '\"Ryan\"[49546] (p=0.164, logit=18.000)', '\" Michael\"[8096] (p=0.032, logit=16.375)', '\" The\"[578] (p=0.032, logit=16.375)', '\" Hugh\"[30206] (p=0.025, logit=16.125)']\n",
      "model.layers.30: ['\" Ryan\"[13960] (p=0.652, logit=19.125)', '\"Ryan\"[49546] (p=0.164, logit=17.750)', '\" None\"[2290] (p=0.032, logit=16.125)', '\" The\"[578] (p=0.027, logit=15.938)', '\" Hugh\"[30206] (p=0.025, logit=15.875)']\n",
      "model.layers.32: ['\" Ryan\"[13960] (p=0.445, logit=18.500)', '\" None\"[2290] (p=0.238, logit=17.875)', '\"Ryan\"[49546] (p=0.113, logit=17.125)', '\"None\"[4155] (p=0.068, logit=16.625)', '\" Tolkien\"[80403] (p=0.034, logit=15.938)']\n",
      "model.layers.34: ['\" None\"[2290] (p=0.318, logit=17.750)', '\" Ryan\"[13960] (p=0.220, logit=17.375)', '\" Tolkien\"[80403] (p=0.104, logit=16.625)', '\"None\"[4155] (p=0.104, logit=16.625)', '\"Ryan\"[49546] (p=0.063, logit=16.125)']\n",
      "model.layers.36: ['\" Ryan\"[13960] (p=0.439, logit=18.125)', '\"Ryan\"[49546] (p=0.143, logit=17.000)', '\" Hugh\"[30206] (p=0.098, logit=16.625)', '\" Iss\"[16314] (p=0.067, logit=16.250)', '\" The\"[578] (p=0.049, logit=15.938)']\n",
      "model.layers.38: ['\" Ryan\"[13960] (p=0.266, logit=17.250)', '\" Iss\"[16314] (p=0.161, logit=16.750)', '\" Hugh\"[30206] (p=0.126, logit=16.500)', '\"Ryan\"[49546] (p=0.086, logit=16.125)', '\"Iss\"[29316] (p=0.063, logit=15.812)']\n",
      "model.layers.40: ['\" Ryan\"[13960] (p=0.334, logit=17.625)', '\" Hugh\"[30206] (p=0.230, logit=17.250)', '\"Ryan\"[49546] (p=0.123, logit=16.625)', '\" Iss\"[16314] (p=0.051, logit=15.750)', '\" The\"[578] (p=0.045, logit=15.625)']\n",
      "model.layers.42: ['\" Ryan\"[13960] (p=0.237, logit=17.125)', '\" Hugh\"[30206] (p=0.237, logit=17.125)', '\" Iss\"[16314] (p=0.112, logit=16.375)', '\"Ryan\"[49546] (p=0.087, logit=16.125)', '\" The\"[578] (p=0.053, logit=15.625)']\n",
      "model.layers.44: ['\" Ryan\"[13960] (p=0.242, logit=17.125)', '\" Hugh\"[30206] (p=0.214, logit=17.000)', '\" Iss\"[16314] (p=0.114, logit=16.375)', '\"Ryan\"[49546] (p=0.089, logit=16.125)', '\" The\"[578] (p=0.054, logit=15.625)']\n",
      "model.layers.46: ['\" Hugh\"[30206] (p=0.237, logit=17.125)', '\" Ryan\"[13960] (p=0.210, logit=17.000)', '\" Iss\"[16314] (p=0.127, logit=16.500)', '\"Ryan\"[49546] (p=0.077, logit=16.000)', '\" None\"[2290] (p=0.056, logit=15.688)']\n",
      "model.layers.48: ['\" Hugh\"[30206] (p=0.283, logit=17.250)', '\" Ryan\"[13960] (p=0.172, logit=16.750)', '\" Iss\"[16314] (p=0.118, logit=16.375)', '\"Ryan\"[49546] (p=0.067, logit=15.812)', '\" The\"[578] (p=0.059, logit=15.688)']\n",
      "model.layers.50: ['\" Hugh\"[30206] (p=0.281, logit=17.250)', '\" Ryan\"[13960] (p=0.171, logit=16.750)', '\" Iss\"[16314] (p=0.133, logit=16.500)', '\"Ryan\"[49546] (p=0.063, logit=15.750)', '\" The\"[578] (p=0.059, logit=15.688)']\n",
      "model.layers.52: ['\" Ryan\"[13960] (p=0.350, logit=17.500)', '\" Hugh\"[30206] (p=0.165, logit=16.750)', '\"Ryan\"[49546] (p=0.146, logit=16.625)', '\" The\"[578] (p=0.061, logit=15.750)', '\" None\"[2290] (p=0.054, logit=15.625)']\n",
      "model.layers.54: ['\" Robin\"[17582] (p=0.307, logit=17.000)', '\" Hugh\"[30206] (p=0.093, logit=15.812)', '\" Ryan\"[13960] (p=0.078, logit=15.625)', '\" The\"[578] (p=0.078, logit=15.625)', '\" None\"[2290] (p=0.068, logit=15.500)']\n",
      "model.layers.56: ['\" Robin\"[17582] (p=0.408, logit=17.375)', '\"Robin\"[77771] (p=0.097, logit=15.938)', '\" The\"[578] (p=0.071, logit=15.625)', '\" None\"[2290] (p=0.062, logit=15.500)', '\" Hugh\"[30206] (p=0.059, logit=15.438)']\n",
      "model.layers.58: ['\" Robin\"[17582] (p=0.404, logit=17.375)', '\"Robin\"[77771] (p=0.102, logit=16.000)', '\" The\"[578] (p=0.075, logit=15.688)', '\" None\"[2290] (p=0.062, logit=15.500)', '\" Hugh\"[30206] (p=0.058, logit=15.438)']\n",
      "model.layers.60: ['\" Robin\"[17582] (p=0.350, logit=17.250)', '\" George\"[10058] (p=0.100, logit=16.000)', '\"Robin\"[77771] (p=0.088, logit=15.875)', '\" The\"[578] (p=0.073, logit=15.688)', '\" None\"[2290] (p=0.065, logit=15.562)']\n",
      "model.layers.62: ['\" Robin\"[17582] (p=0.377, logit=17.375)', '\"Robin\"[77771] (p=0.095, logit=16.000)', '\" George\"[10058] (p=0.089, logit=15.938)', '\" The\"[578] (p=0.074, logit=15.750)', '\" None\"[2290] (p=0.062, logit=15.562)']\n",
      "model.layers.64: ['\" Robin\"[17582] (p=0.633, logit=18.625)', '\"Robin\"[77771] (p=0.141, logit=17.125)', '\" The\"[578] (p=0.043, logit=15.938)', '\" None\"[2290] (p=0.031, logit=15.625)', '\" George\"[10058] (p=0.030, logit=15.562)']\n",
      "model.layers.66: ['\" Robin\"[17582] (p=0.652, logit=18.750)', '\"Robin\"[77771] (p=0.146, logit=17.250)', '\" The\"[578] (p=0.037, logit=15.875)', '\" None\"[2290] (p=0.029, logit=15.625)', '\" George\"[10058] (p=0.027, logit=15.562)']\n",
      "model.layers.68: ['\" Robin\"[17582] (p=0.699, logit=19.250)', '\"Robin\"[77771] (p=0.155, logit=17.750)', '\" The\"[578] (p=0.027, logit=16.000)', '\" None\"[2290] (p=0.024, logit=15.875)', '\" George\"[10058] (p=0.022, logit=15.812)']\n",
      "model.layers.70: ['\" Robin\"[17582] (p=0.699, logit=19.250)', '\"Robin\"[77771] (p=0.156, logit=17.750)', '\" The\"[578] (p=0.027, logit=16.000)', '\" None\"[2290] (p=0.024, logit=15.875)', '\" George\"[10058] (p=0.022, logit=15.812)']\n",
      "model.layers.72: ['\" Robin\"[17582] (p=0.680, logit=19.250)', '\"Robin\"[77771] (p=0.151, logit=17.750)', '\" George\"[10058] (p=0.043, logit=16.500)', '\" The\"[578] (p=0.026, logit=16.000)', '\" None\"[2290] (p=0.023, logit=15.875)']\n",
      "model.layers.74: ['\" Robin\"[17582] (p=0.703, logit=19.375)', '\"Robin\"[77771] (p=0.139, logit=17.750)', '\" George\"[10058] (p=0.045, logit=16.625)', '\" The\"[578] (p=0.024, logit=16.000)', '\" None\"[2290] (p=0.020, logit=15.812)']\n",
      "model.layers.76: ['\" Robin\"[17582] (p=0.711, logit=19.500)', '\"Robin\"[77771] (p=0.141, logit=17.875)', '\" George\"[10058] (p=0.046, logit=16.750)', '\" The\"[578] (p=0.020, logit=15.938)', '\" None\"[2290] (p=0.018, logit=15.812)']\n",
      "model.layers.78: ['\" Robin\"[17582] (p=0.719, logit=19.625)', '\"Robin\"[77771] (p=0.125, logit=17.875)', '\" George\"[10058] (p=0.059, logit=17.125)', '\" The\"[578] (p=0.019, logit=16.000)', '\" None\"[2290] (p=0.018, logit=15.938)']\n"
     ]
    }
   ],
   "source": [
    "from src.functional import PatchSpec\n",
    "\n",
    "clean_prompt = prompt_template.format(pivot_subj, \",\".join(entity_list))\n",
    "\n",
    "for layer_name in mt.layer_names[::2]:\n",
    "    int_logit = get_hs(\n",
    "        mt = mt,\n",
    "        input = clean_prompt,\n",
    "        patches = [PatchSpec(\n",
    "            location=(layer_name, -1),\n",
    "            patch=patch_hs[(layer_name, -1)],\n",
    "            strategy=\"replace\"\n",
    "        )],\n",
    "        locations = [logit_location],\n",
    "        return_dict=False\n",
    "    ).squeeze(0)\n",
    "\n",
    "    pred = interpret_logits(\n",
    "        logits=int_logit,\n",
    "        tokenizer=mt,\n",
    "    )\n",
    "    print(f'{layer_name}: {[str(p) for p in pred]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0993c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e22d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' snow.\\nThe color of fire extinguisher is red. The objects in the list that are red are stop sign, apple, tomato, and fire truck']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclusion\n",
    "prompt_template = \"\"\"Which person from the following list does not have the profession in common with {}?\n",
    "{}.\n",
    "Ans:\"\"\"\n",
    "\n",
    "# pivot_subj = \"Hugh Jackman\"\n",
    "# entity_list = [\n",
    "#     \"Tom Cruise\",\n",
    "#     \"Brad Pitt\",\n",
    "#     \"Ryan Reynolds\",\n",
    "#     \"Celine Dion\",\n",
    "#     \"Tom Hanks\",\n",
    "#     \"Scarlett Johansson\",\n",
    "# ]\n",
    "\n",
    "# pivot_subj = \"Michael Jordan\"\n",
    "# entity_list = [\n",
    "#     \"Sachin Tendulkar\",\n",
    "#     \"Roger Federer\",\n",
    "#     \"Lionel Messi\",\n",
    "#     \"Kobe Bryant\",\n",
    "#     \"Tom Brady\",\n",
    "#     \"Hugh Jackman\",\n",
    "#     \"Serena Williams\",\n",
    "#     \"Mohamed Salah\",\n",
    "#     \"Tiger Woods\",\n",
    "#     \"Jesse Owens\",\n",
    "#     \"Cristiano Ronaldo\",\n",
    "#     \"Mike Tyson\",\n",
    "#     \"Usain Bolt\",\n",
    "#     \"Michael Phelps\",\n",
    "#     \"LeBron James\",\n",
    "# ]\n",
    "\n",
    "# prompt_template = \"\"\"Which person from the following list didn't graduate from the same school as {}\n",
    "# {}.\n",
    "# Ans:\"\"\"\n",
    "\n",
    "# pivot_subj = \"Steve Martin\"\n",
    "# entity_list = [\n",
    "#    \"Francis Ford Coppola\",\n",
    "#    \"Kareem Abdul-Jabbar\",\n",
    "#    \"Jackie Robinson\",\n",
    "#    \"Jim Morrison\",\n",
    "#    \"Susan Wojcicki\",\n",
    "#    \"Carol Burnett\",\n",
    "#    \"George Lucas\",\n",
    "#    \"Arthur Ashe\",\n",
    "#    \"Mayim Bialik\",\n",
    "#    \"Troy Aikman\",\n",
    "#    \"Randy Schekman\"\n",
    "# ]\n",
    "\n",
    "prompt_template = \"\"\"Which object in this list is does not share the same color as {}?\n",
    "{}.\n",
    "Ans:\"\"\"\n",
    "\n",
    "# pivot_subj = \"banana\"\n",
    "# entity_list = [\"pineapple\", \"sunflower\", \"canary\", \"blood\", \"mustard\", \"taxi\", \"corn\"]\n",
    "pivot_subj = \"fire extinguisher\"\n",
    "patch_list = [\"stop sign\", \"apple\", \"tomato\", \"snow\", \"traffic light\", \"fire truck\"]\n",
    "\n",
    "\n",
    "prompt = prompt_template.format(pivot_subj, \",\".join(patch_list))\n",
    "generate_with_patch(\n",
    "    mt=mt,\n",
    "    inputs=prompt,\n",
    "    max_new_tokens=30,\n",
    "    n_gen_per_prompt=1,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06924b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOVELTY: Find the odd one out\n",
    "\n",
    "prompt_template = \"\"\"Which person from the following list is different from the others?\n",
    "{}.\n",
    "Ans:\"\"\"\n",
    "\n",
    "patch_list = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
