{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-08 13:08:07 __main__ INFO     torch.__version__='2.3.1', torch.version.cuda='12.1'\n",
      "2024-08-08 13:08:07 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2024-08-08 13:08:07 __main__ INFO     transformers.__version__='4.43.3'\n",
      "2024-08-08 13:08:07 httpx DEBUG    load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-08 13:08:07 httpx DEBUG    load_verify_locations cafile='/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\")\n",
    "logger.info(f\"{transformers.__version__=}\")\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_KEY\"),\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-08 13:08:08 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-08 13:08:13 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-8B-Instruct> | size: 15316.516 MB | dtype: torch.float16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.typing import TokenizerOutput\n",
    "from src.functional import get_module_nnsight, untuple, get_hs, PatchSpec\n",
    "from typing import Literal\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "#! the clean here actually stands for **corrupt* in the causal tracing\n",
    "def attribution_patching(\n",
    "    mt: ModelandTokenizer,\n",
    "    clean_inputs: TokenizerOutput,\n",
    "    patches: PatchSpec | list[PatchSpec],\n",
    "    interested_locations: list[tuple[str, int]],\n",
    "    ans_token_idx: int,\n",
    "    metric: Literal[\"logit\", \"proba\"] = \"proba\",\n",
    "    resolution: int = 10,\n",
    ") -> float:\n",
    "\n",
    "    if \"offset_mapping\" in clean_inputs:\n",
    "        clean_inputs.pop(\"offset_mapping\")\n",
    "    if isinstance(patches, PatchSpec):\n",
    "        patches = [patches]\n",
    "\n",
    "    clean_states = get_hs(\n",
    "        mt = mt, \n",
    "        input = clean_inputs,\n",
    "        locations = interested_locations,\n",
    "    )\n",
    "    patched_states = get_hs(\n",
    "        mt = mt, \n",
    "        input = clean_inputs,\n",
    "        locations = interested_locations,\n",
    "        patches = patches\n",
    "    )\n",
    "\n",
    "    grads = {}\n",
    "    scan = True\n",
    "    for alpha in torch.linspace(0, 1, resolution):\n",
    "        cur_grads = {}\n",
    "\n",
    "        with mt.trace(clean_inputs, scan=scan) as trace:\n",
    "            # patching\n",
    "            for patch in patches:\n",
    "                module_name, tok_idx = patch.location\n",
    "                patch_module = get_module_nnsight(mt, module_name)\n",
    "\n",
    "                assert isinstance(patch.clean, torch.Tensor) and patch.clean.shape == patch.patch.shape\n",
    "                h = alpha * patch.clean + (1 - alpha) * patch.patch\n",
    "                h = h.to(mt.device) if not h.device == mt.device else h\n",
    "                h.retain_grad = True\n",
    "\n",
    "                patch_module.output[0, tok_idx, :] = h\n",
    "\n",
    "            # cache the interested hidden states\n",
    "            for loc in interested_locations:\n",
    "                module_name, tok_idx = loc\n",
    "                module = get_module_nnsight(mt, module_name)\n",
    "                cur_output = (\n",
    "                    module.output.save()\n",
    "                    if \"mlp\" in module_name\n",
    "                    else module.output[0].save()\n",
    "                )   #! nnsight quirk => to get the grad of a reference tensor, you can't index it\n",
    "                cur_grads[loc] = cur_output.grad[0, tok_idx, :].save()\n",
    "                # cur_grads[loc] = (\n",
    "                #     module.output.grad[0, tok_idx, :].save()\n",
    "                #     if \"mlp\" in module_name\n",
    "                #     else module.output[0].grad[0, tok_idx, :].save()\n",
    "                # )\n",
    "\n",
    "                # initialize the grads\n",
    "                if scan:\n",
    "                    grads[loc] = torch.zeros_like(cur_output.grad[0, tok_idx, :]).to(mt.device).save()\n",
    "\n",
    "            #! nnsight quirk => backward() has to be called later than grad.save() to populate the proxies\n",
    "            if metric == \"logit\":\n",
    "                v = mt.output.logits[0][-1][ans_token_idx]\n",
    "            elif metric == \"proba\":\n",
    "                v = mt.output.logits[0][-1].softmax(dim=-1)[ans_token_idx]\n",
    "            else:\n",
    "                raise ValueError(f\"unknown {metric=}\")\n",
    "            v.backward()\n",
    "\n",
    "\n",
    "        for loc in interested_locations:\n",
    "            print(f\"{loc=}\")\n",
    "            print(f\"{grads[loc].shape=} | {grads[loc].norm()=}\")\n",
    "            print(f\"{cur_grads[loc].shape=} | {cur_grads[loc].norm()=}\")\n",
    "            module_name, tok_idx = loc\n",
    "            grads[loc] += cur_grads[loc]\n",
    "\n",
    "        mt._model.zero_grad()\n",
    "        scan = False\n",
    "\n",
    "    grads = {loc: grad / resolution for loc, grad in grads.items()}\n",
    "\n",
    "    approx_IE = {\n",
    "        loc: torch.dot(grad, patched_states[loc] - clean_states[loc]).item() \n",
    "        for loc, grad in grads.items()\n",
    "    } \n",
    "\n",
    "    return approx_IE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import prepare_input, guess_subject\n",
    "from src.functional import find_token_range, get_hs\n",
    "from typing import Optional\n",
    "\n",
    "def get_h_at_subj(\n",
    "    mt: ModelandTokenizer,\n",
    "    layer: str | list[str],\n",
    "    prompt: str | TokenizerOutput,\n",
    "    subj: Optional[str] = None,\n",
    "    input: Optional[TokenizerOutput] = None,\n",
    ") -> torch.Tensor:\n",
    "    if subj is None:\n",
    "        subj = guess_subject(prompt)\n",
    "        logger.warning(f\"no subj provided, guessed {subj=}\")\n",
    "    else:\n",
    "        assert subj in prompt, f\"{subj=} not in {prompt=}\"\n",
    "\n",
    "    skip_prepare_input = input is not None and \"offset_mapping\" in input\n",
    "    if not skip_prepare_input:\n",
    "        logger.debug(f\"preparing input for prompt: {prompt}\")\n",
    "        input = prepare_input(\n",
    "            prompts=prompt, \n",
    "            tokenizer=mt, \n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "    offset_mapping = input.pop(\"offset_mapping\")[0]\n",
    "    subj_range = find_token_range(string=prompt, substring=subj, tokenizer=mt.tokenizer, offset_mapping=offset_mapping)\n",
    "    subj_ends = subj_range[1] - 1\n",
    "\n",
    "    logger.debug(f\"{subj=} => {subj_ends=} | \\\"{mt.tokenizer.decode(input['input_ids'][0][subj_ends])}\\\"\")\n",
    "\n",
    "    return get_hs(\n",
    "        mt = mt, input = input, \n",
    "        locations = [(l, subj_ends) for l in layer]\n",
    "    )\n",
    "\n",
    "\n",
    "# prompt =  \"{} is located in the city of\"\n",
    "# clean_subj = \"Louvre\"\n",
    "# patch_subj = \"The Space Needle\"\n",
    "\n",
    "# clean_hs, patch_hs = [\n",
    "#     get_h_at_subj(\n",
    "#         mt = mt, \n",
    "#         layer = [mt.embedder_name],\n",
    "#         prompt = prompt.format(subj),\n",
    "#         subj = subj,\n",
    "#     ) for subj in [clean_subj, patch_subj]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-08 13:26:47 __main__ DEBUG    \" Seattle\" (p=0.987)\n",
      "2024-08-08 13:26:47 __main__ DEBUG    clean_subj_range=(1, 3) | vre\n",
      "2024-08-08 13:26:47 __main__ DEBUG    patched_subj_range=(1, 4) |  Needle\n",
      "idx=0 [1] | <|begin_of_text|>\n",
      "idx=1 [0] | <|begin_of_text|>\n",
      "idx=2 [1] | Lou*\n",
      "idx=3 [1] | vre*\n",
      "idx=4 [1] |  is\n",
      "idx=5 [1] |  located\n",
      "idx=6 [1] |  in\n",
      "idx=7 [1] |  the\n",
      "idx=8 [1] |  city\n",
      "idx=9 [1] |  of\n",
      "--------------------------------------------------\n",
      "idx=0 [1] | <|begin_of_text|>\n",
      "idx=1 [1] | The*\n",
      "idx=2 [1] |  Space*\n",
      "idx=3 [1] |  Needle*\n",
      "idx=4 [1] |  is\n",
      "idx=5 [1] |  located\n",
      "idx=6 [1] |  in\n",
      "idx=7 [1] |  the\n",
      "idx=8 [1] |  city\n",
      "idx=9 [1] |  of\n"
     ]
    }
   ],
   "source": [
    "from src.functional import predict_next_token\n",
    "from src.trace import insert_padding_before_subj\n",
    "\n",
    "prompt =  \"{} is located in the city of\"\n",
    "clean_subj = \"Louvre\"\n",
    "patch_subj = \"The Space Needle\"\n",
    "\n",
    "ans = predict_next_token(\n",
    "    mt = mt,\n",
    "    inputs = prompt.format(patch_subj),\n",
    ")[0][0]\n",
    "\n",
    "logger.debug(ans)\n",
    "\n",
    "clean_inputs = prepare_input(\n",
    "    prompts=prompt.format(clean_subj), \n",
    "    tokenizer=mt, \n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "clean_subj_range = find_token_range(\n",
    "    string=prompt.format(clean_subj), \n",
    "    substring=clean_subj, \n",
    "    tokenizer=mt.tokenizer,\n",
    "    offset_mapping=clean_inputs[\"offset_mapping\"][0]\n",
    ")\n",
    "logger.debug(f\"{clean_subj_range=} | {mt.tokenizer.decode(clean_inputs['input_ids'][0][clean_subj_range[1]-1])}\")\n",
    "\n",
    "patched_inputs = prepare_input(\n",
    "    prompts=prompt.format(patch_subj), \n",
    "    tokenizer=mt, \n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "patched_subj_range = find_token_range(\n",
    "    string=prompt.format(patch_subj), \n",
    "    substring=patch_subj, \n",
    "    tokenizer=mt.tokenizer,\n",
    "    offset_mapping=patched_inputs[\"offset_mapping\"][0]\n",
    ")\n",
    "logger.debug(f\"{patched_subj_range=} | {mt.tokenizer.decode(patched_inputs['input_ids'][0][patched_subj_range[1]-1])}\")\n",
    "\n",
    "subj_end = max(clean_subj_range[1], patched_subj_range[1])\n",
    "\n",
    "clean_inputs = insert_padding_before_subj(\n",
    "    inp = clean_inputs,\n",
    "    subj_range = clean_subj_range,\n",
    "    subj_ends = subj_end,\n",
    "    pad_id = mt.tokenizer.bos_token_id,\n",
    ")\n",
    "patched_inputs = insert_padding_before_subj(\n",
    "    inp = patched_inputs,\n",
    "    subj_range = patched_subj_range,\n",
    "    subj_ends = subj_end,\n",
    "    pad_id = mt.tokenizer.bos_token_id,\n",
    ")\n",
    "\n",
    "clean_subj_shift = subj_end - clean_subj_range[1]\n",
    "clean_subj_range = (clean_subj_range[0] + clean_subj_shift, subj_end)\n",
    "patched_subj_shift = subj_end - patched_subj_range[1]\n",
    "patched_subj_range = (patched_subj_range[0] + patched_subj_shift, subj_end)\n",
    "\n",
    "subj_start = min(clean_subj_range[0], patched_subj_range[0])\n",
    "\n",
    "for idx, (tok_id, attn_mask) in enumerate(zip(clean_inputs.input_ids[0], clean_inputs.attention_mask[0])):\n",
    "    is_subj = clean_subj_range[0] <= idx < clean_subj_range[1]\n",
    "    append = \"*\" if is_subj else \"\"\n",
    "    print(f\"{idx=} [{attn_mask}] | {mt.tokenizer.decode(tok_id)}\"+append)\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "for idx, (tok_id, attn_mask) in enumerate(zip(patched_inputs.input_ids[0], patched_inputs.attention_mask[0])):\n",
    "    is_subj = patched_subj_range[0] <= idx < patched_subj_range[1]\n",
    "    append = \"*\" if is_subj else \"\"\n",
    "    print(f\"{idx=} [{attn_mask}] | {mt.tokenizer.decode(tok_id)}\"+append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key=('model.embed_tokens', 1) | emb_clean[key].shape=torch.Size([4096]) | emb_patch[key].shape=torch.Size([4096])\n",
      "key=('model.embed_tokens', 2) | emb_clean[key].shape=torch.Size([4096]) | emb_patch[key].shape=torch.Size([4096])\n",
      "key=('model.embed_tokens', 3) | emb_clean[key].shape=torch.Size([4096]) | emb_patch[key].shape=torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "emb_clean = get_hs(\n",
    "    mt = mt, \n",
    "    input = clean_inputs,\n",
    "    locations = [(mt.embedder_name, tok_idx) for tok_idx in range(subj_start, subj_end)]\n",
    ")\n",
    "emb_patch = get_hs(\n",
    "    mt = mt, \n",
    "    input = patched_inputs,\n",
    "    locations = [(mt.embedder_name, tok_idx) for tok_idx in range(subj_start, subj_end)]\n",
    ")\n",
    "\n",
    "for key in emb_clean.keys():\n",
    "    print(f\"{key=} | {emb_clean[key].shape=} | {emb_patch[key].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_spec = [\n",
    "    PatchSpec(\n",
    "        location = location,\n",
    "        patch = emb_patch[location],\n",
    "        clean = emb_clean[location],\n",
    "    ) for location in emb_clean.keys()\n",
    "]\n",
    "\n",
    "test_h = get_hs(\n",
    "    mt=mt,\n",
    "    input=clean_inputs,\n",
    "    locations=[(mt.embedder_name, subj_end - 1)],\n",
    "    # patches=PatchSpec(\n",
    "    #     location=(mt.embedder_name, subj_end - 1),\n",
    "    #     patch=emb_patch[(mt.embedder_name, subj_end - 1)],  \n",
    "    # ),\n",
    "    patches=patch_spec,\n",
    ")\n",
    "\n",
    "torch.allclose(emb_patch[(mt.embedder_name, subj_end - 1)], test_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc=('model.layers.0', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.5522, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.3730, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.2252, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1580, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1185, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0903, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0723, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0627, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0552, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0467, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0432, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0394, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0365, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0266, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0205, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0166, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0057, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0042, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0040, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0040, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0039, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0030, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0025, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0023, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5522, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.3682, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3730, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.2377, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2252, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1375, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1580, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0944, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1185, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0701, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0903, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0542, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0723, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0432, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0627, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0376, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0552, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0332, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0467, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0280, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0432, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0258, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0394, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0235, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0365, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0218, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0266, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0157, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0205, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0120, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0166, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0097, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0057, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0034, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0042, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0025, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0040, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0024, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0040, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0024, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0039, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0023, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0030, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0025, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0015, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0023, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0012, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.9131, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.3486, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.6060, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.2140, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3601, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1176, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2510, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0780, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1876, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0555, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1440, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0420, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1152, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0327, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1002, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0282, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0883, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0247, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0746, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0208, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0689, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0191, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0629, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0175, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0582, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0161, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0422, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0116, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0324, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0088, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0263, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0072, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0091, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0026, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0067, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0063, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0064, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0062, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0048, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0039, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0037, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0030, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0009, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0027, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0022, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0016, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(1.2285, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(18.1250, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8013, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(10.3047, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.4685, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(5.1562, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3242, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(3.1348, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2405, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(1.8877, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1846, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(1.1455, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1470, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.7856, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1277, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.5664, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1125, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.4382, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0950, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.3142, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0877, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.2766, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0800, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.2251, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0741, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.2006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0536, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1357, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0410, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0971, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0334, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0818, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0116, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0264, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0086, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0212, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0080, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0204, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0081, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0208, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0079, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0208, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0061, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0164, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0050, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0144, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0048, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0141, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0039, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0100, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0034, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0088, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0028, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0064, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0023, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0055, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0046, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0039, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(18.5625, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.2781, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(10.5859, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1670, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(5.3242, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0918, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(3.2871, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0594, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(2.0195, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0393, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(1.2559, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0244, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8804, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0177, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.6602, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0135, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5259, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0127, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3926, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0119, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3525, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0117, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2983, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0108, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2700, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0097, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1860, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0087, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1355, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0084, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1133, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0081, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0368, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0048, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0287, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0048, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0274, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0048, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0279, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0047, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0278, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0047, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0215, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0045, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0186, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0044, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0181, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0042, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0132, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0034, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0116, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0031, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0086, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0026, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0074, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0065, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0055, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(18.6406, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0304, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(10.6328, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0181, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(5.3359, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0105, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(3.2969, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0068, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(2.0234, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0049, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(1.2559, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0035, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8804, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0024, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.6602, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5254, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0015, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3926, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3521, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2983, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2700, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0009, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1870, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1375, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1161, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0368, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0289, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0277, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0282, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0280, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0221, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0191, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0186, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0141, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0126, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0094, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0080, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0068, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(9.4473e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0055, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(3.8147e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(2.0862e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(18.6250, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0206, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(10.6250, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0130, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(5.3359, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0080, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(3.2949, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0057, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(2.0215, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0042, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(1.2539, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0032, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8794, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0023, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.6592, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5249, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0016, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3921, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3516, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2979, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2695, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1869, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1375, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1162, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0368, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0290, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0277, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0282, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0280, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0221, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0192, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0186, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0141, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0126, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0094, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0080, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0069, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0055, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(5.0783e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(3.1650e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(18.6250, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0149, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(10.6250, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0098, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(5.3320, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0064, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(3.2930, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0047, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(2.0195, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0036, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(1.2529, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0028, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8784, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.6587, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5239, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0015, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3914, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3508, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2976, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2693, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1868, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1375, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1164, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0368, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0290, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0277, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0283, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0280, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0222, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0192, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0186, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0141, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0126, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0094, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0080, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0069, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0055, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(5.0128e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(3.2783e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(18.6250, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0123, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(10.6250, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0085, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(5.3320, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0057, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(3.2930, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0043, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(2.0195, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0034, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(1.2520, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0027, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8779, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.6582, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0016, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5234, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3909, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3503, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2974, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2690, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0009, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1868, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1376, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1165, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0367, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0290, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0278, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0283, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0281, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0222, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0192, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0186, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0142, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0127, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0095, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0081, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0069, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0055, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(4.5300e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(3.0160e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(18.6250, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0105, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(10.6250, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0076, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(5.3320, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0052, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(3.2930, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0040, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(2.0195, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0032, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(1.2510, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0025, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8774, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.6577, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0015, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5229, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3904, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0012, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3499, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0012, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2969, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0009, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2688, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1866, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1377, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1167, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0367, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0291, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0278, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0283, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0281, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0222, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0193, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0187, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0142, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0127, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0095, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0081, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0069, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(8.7082e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0055, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(4.0054e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(2.7359e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 3)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "results = attribution_patching(\n",
    "    mt = mt,\n",
    "    clean_inputs = clean_inputs,\n",
    "    patches = patch_spec,\n",
    "    interested_locations=[\n",
    "        (mt.layer_name_format.format(l), subj_end - 1) \n",
    "        for l in range(mt.n_layer)\n",
    "    ],\n",
    "    ans_token_idx=ans.token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('model.layers.0', 3): 0.0035152435302734375,\n",
       " ('model.layers.1', 3): 0.003936767578125,\n",
       " ('model.layers.2', 3): 0.00331878662109375,\n",
       " ('model.layers.3', 3): 0.00189208984375,\n",
       " ('model.layers.4', 3): 0.00103759765625,\n",
       " ('model.layers.5', 3): 0.0005850791931152344,\n",
       " ('model.layers.6', 3): 0.0010442733764648438,\n",
       " ('model.layers.7', 3): 0.0008549690246582031,\n",
       " ('model.layers.8', 3): 0.0004711151123046875,\n",
       " ('model.layers.9', 3): -5.4836273193359375e-05,\n",
       " ('model.layers.10', 3): 0.00035953521728515625,\n",
       " ('model.layers.11', 3): 0.00047898292541503906,\n",
       " ('model.layers.12', 3): 0.00025582313537597656,\n",
       " ('model.layers.13', 3): 0.00022268295288085938,\n",
       " ('model.layers.14', 3): 0.0002803802490234375,\n",
       " ('model.layers.15', 3): 0.0002503395080566406,\n",
       " ('model.layers.16', 3): 0.0004260540008544922,\n",
       " ('model.layers.17', 3): 0.0003256797790527344,\n",
       " ('model.layers.18', 3): 0.0003790855407714844,\n",
       " ('model.layers.19', 3): 0.0003502368927001953,\n",
       " ('model.layers.20', 3): 0.0005354881286621094,\n",
       " ('model.layers.21', 3): 0.0003349781036376953,\n",
       " ('model.layers.22', 3): 0.0003876686096191406,\n",
       " ('model.layers.23', 3): 0.00037288665771484375,\n",
       " ('model.layers.24', 3): 0.00030684471130371094,\n",
       " ('model.layers.25', 3): 0.0005078315734863281,\n",
       " ('model.layers.26', 3): 0.0001900196075439453,\n",
       " ('model.layers.27', 3): 7.778406143188477e-05,\n",
       " ('model.layers.28', 3): 0.00015270709991455078,\n",
       " ('model.layers.29', 3): 0.0001552104949951172,\n",
       " ('model.layers.30', 3): 0.00012409687042236328,\n",
       " ('model.layers.31', 3): 0.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import untuple\n",
    "\n",
    "clean_inputs = prepare_input(\n",
    "    prompts=prompt.format(clean_subj), \n",
    "    tokenizer=mt, \n",
    "    return_offsets_mapping=False\n",
    ")\n",
    "\n",
    "\n",
    "cur_grads = {l: None for l in mt.layer_names}\n",
    "\n",
    "# module_name = mt.layer_name_format.format(10)\n",
    "# module_name = mt.embedder_name\n",
    "module_name = mt.mlp_module_name_format.format(10)\n",
    "# module_name = mt.attn_module_name_format.format(10)\n",
    "with mt.trace(clean_inputs, scan = True) as trace:\n",
    "    module = get_module_nnsight(mt, module_name)\n",
    "    h = module.output.save()\n",
    "    # h_grad = module.output.grad[0, 5, :].save()\n",
    "    h_grad = h.grad[0, 5, :].save()\n",
    "\n",
    "    m = mt.output.logits[0][-1].softmax(dim=-1)[ans.token_id]\n",
    "    m.backward()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # for l in cur_grads:\n",
    "    #     module = get_module_nnsight(mt, l)\n",
    "    #     cur_grads[l] = module.output[0].grad.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.1526e-06, -8.4639e-06, -1.0073e-05,  ...,  3.2187e-06,\n",
       "        -5.4240e-06, -1.4246e-05], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for module, grad in cur_grads.items():\n",
    "#     print(module, grad.shape)\n",
    "h_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untuple(h).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.0',\n",
       " 'model.layers.1',\n",
       " 'model.layers.2',\n",
       " 'model.layers.3',\n",
       " 'model.layers.4',\n",
       " 'model.layers.5',\n",
       " 'model.layers.6',\n",
       " 'model.layers.7',\n",
       " 'model.layers.8',\n",
       " 'model.layers.9',\n",
       " 'model.layers.10',\n",
       " 'model.layers.11',\n",
       " 'model.layers.12',\n",
       " 'model.layers.13',\n",
       " 'model.layers.14',\n",
       " 'model.layers.15',\n",
       " 'model.layers.16',\n",
       " 'model.layers.17',\n",
       " 'model.layers.18',\n",
       " 'model.layers.19',\n",
       " 'model.layers.20',\n",
       " 'model.layers.21',\n",
       " 'model.layers.22',\n",
       " 'model.layers.23',\n",
       " 'model.layers.24',\n",
       " 'model.layers.25',\n",
       " 'model.layers.26',\n",
       " 'model.layers.27',\n",
       " 'model.layers.28',\n",
       " 'model.layers.29',\n",
       " 'model.layers.30',\n",
       " 'model.layers.31']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
