{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-07 15:00:47 __main__ INFO     torch.__version__='2.3.1', torch.version.cuda='12.1'\n",
      "2024-08-07 15:00:47 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2024-08-07 15:00:47 __main__ INFO     transformers.__version__='4.43.3'\n",
      "2024-08-07 15:00:47 httpx DEBUG    load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-07 15:00:47 httpx DEBUG    load_verify_locations cafile='/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\")\n",
    "logger.info(f\"{transformers.__version__=}\")\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_KEY\"),\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-07 15:00:49 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-07 15:00:54 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-8B-Instruct> | size: 15316.516 MB | dtype: torch.float16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.typing import TokenizerOutput\n",
    "from src.functional import get_module_nnsight, untuple\n",
    "from typing import Literal\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "#! the clean here actually stands for **corrupt* in the causal tracing\n",
    "def attribution_patching(\n",
    "    mt: ModelandTokenizer,\n",
    "    clean_inputs: TokenizerOutput,\n",
    "    h_clean: torch.Tensor,\n",
    "    h_patch: torch.Tensor,\n",
    "    patch_location: tuple[str, int],\n",
    "    interested_locations: list[tuple[str, int]],\n",
    "    ans_token_idx: int,\n",
    "    metric: Literal[\"logit\", \"proba\"] = \"proba\",\n",
    "    resolution: int = 10,\n",
    ") -> float:\n",
    "\n",
    "    h_clean = h_clean.to(mt.device)\n",
    "    h_patch = h_patch.to(mt.device)\n",
    "\n",
    "    if \"offset_mapping\" in clean_inputs:\n",
    "        clean_inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    scan = True\n",
    "\n",
    "    for alpha in torch.linspace(0, 1, resolution):\n",
    "        cur_grads = {}\n",
    "\n",
    "        with mt.trace(clean_inputs, scan=scan) as trace:\n",
    "            # patching\n",
    "            h = alpha * h_clean + (1 - alpha) * h_patch\n",
    "            h.retain_grad = True\n",
    "            module_name, tok_idx = patch_location\n",
    "            patch_module = get_module_nnsight(mt, module_name)\n",
    "            patch_module.output[0, tok_idx, :] = h\n",
    "\n",
    "            # cache the interested hidden states\n",
    "            for loc in interested_locations:\n",
    "                module_name, tok_idx = loc\n",
    "                module = get_module_nnsight(mt, module_name)\n",
    "                cur_output = (\n",
    "                    module.output[0, tok_idx, :].save()\n",
    "                    if \"mlp\" in module_name\n",
    "                    else module.output[0][0, tok_idx, :].save()\n",
    "                )\n",
    "                # cur_grads[loc] = cur_output.grad.save() #! this doesn't work, weird nnsight issue\n",
    "                cur_grads[loc] = (\n",
    "                    module.output.grad[0, tok_idx, :].save()\n",
    "                    if \"mlp\" in module_name\n",
    "                    else module.output[0].grad[0, tok_idx, :].save()\n",
    "                )\n",
    "\n",
    "                # initialize the grads\n",
    "                if scan:\n",
    "                    grads[loc] = torch.zeros_like(cur_output).to(mt.device).save()\n",
    "\n",
    "            #! another nnsight quirk => backward() has to be called later than grad.save() to populate the proxies\n",
    "            if metric == \"logit\":\n",
    "                v = mt.output.logits[0][-1][ans_token_idx]\n",
    "            elif metric == \"proba\":\n",
    "                v = mt.output.logits[0][-1].softmax(dim=-1)[ans_token_idx]\n",
    "            else:\n",
    "                raise ValueError(f\"unknown {metric=}\")\n",
    "            v.backward()\n",
    "\n",
    "\n",
    "        for loc in interested_locations:\n",
    "            print(f\"{loc=} | {patch_location=}\")\n",
    "            print(f\"{grads[loc].shape=} | {grads[loc].norm()=}\")\n",
    "            print(f\"{cur_grads[loc].shape=} | {cur_grads[loc].norm()=}\")\n",
    "            module_name, tok_idx = loc\n",
    "            grads[loc] += cur_grads[loc]\n",
    "\n",
    "        mt._model.zero_grad()\n",
    "        scan = False\n",
    "\n",
    "    grads = {loc: grad / resolution for loc, grad in grads.items()}\n",
    "    approx_IE = {loc: torch.dot(grad, h_clean - h_patch) for loc, grad in grads.items()}\n",
    "\n",
    "    return approx_IE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-07 15:02:31 __main__ DEBUG    preparing input for prompt: Louvre is located in the city of\n",
      "2024-08-07 15:02:31 __main__ DEBUG    subj='Louvre' => subj_ends=2 | \"vre\"\n",
      "2024-08-07 15:02:32 __main__ DEBUG    preparing input for prompt: The Space Needle is located in the city of\n",
      "2024-08-07 15:02:32 __main__ DEBUG    subj='The Space Needle' => subj_ends=3 | \" Needle\"\n"
     ]
    }
   ],
   "source": [
    "from src.functional import prepare_input, guess_subject\n",
    "from src.functional import find_token_range, get_hs\n",
    "from typing import Optional\n",
    "\n",
    "def get_h_at_subj(\n",
    "    mt: ModelandTokenizer,\n",
    "    layer: str | list[str],\n",
    "    prompt: str | TokenizerOutput,\n",
    "    subj: Optional[str] = None,\n",
    "    input: Optional[TokenizerOutput] = None,\n",
    ") -> torch.Tensor:\n",
    "    if subj is None:\n",
    "        subj = guess_subject(prompt)\n",
    "        logger.warning(f\"no subj provided, guessed {subj=}\")\n",
    "    else:\n",
    "        assert subj in prompt, f\"{subj=} not in {prompt=}\"\n",
    "\n",
    "    skip_prepare_input = input is not None and \"offset_mapping\" in input\n",
    "    if not skip_prepare_input:\n",
    "        logger.debug(f\"preparing input for prompt: {prompt}\")\n",
    "        input = prepare_input(\n",
    "            prompts=prompt, \n",
    "            tokenizer=mt, \n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "    offset_mapping = input.pop(\"offset_mapping\")[0]\n",
    "    subj_range = find_token_range(string=prompt, substring=subj, tokenizer=mt.tokenizer, offset_mapping=offset_mapping)\n",
    "    subj_ends = subj_range[1] - 1\n",
    "\n",
    "    logger.debug(f\"{subj=} => {subj_ends=} | \\\"{mt.tokenizer.decode(input['input_ids'][0][subj_ends])}\\\"\")\n",
    "\n",
    "    return get_hs(\n",
    "        mt = mt, input = input, \n",
    "        layer_and_index = [(l, subj_ends) for l in layer]\n",
    "    )\n",
    "\n",
    "\n",
    "prompt =  \"{} is located in the city of\"\n",
    "clean_subj = \"Louvre\"\n",
    "patch_subj = \"The Space Needle\"\n",
    "\n",
    "clean_hs, patch_hs = [\n",
    "    get_h_at_subj(\n",
    "        mt = mt, \n",
    "        layer = [mt.embedder_name],\n",
    "        prompt = prompt.format(subj),\n",
    "        subj = subj,\n",
    "    ) for subj in [clean_subj, patch_subj]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-07 16:21:48 __main__ DEBUG    \" Seattle\" (p=0.987)\n"
     ]
    }
   ],
   "source": [
    "from src.functional import predict_next_token\n",
    "\n",
    "ans = predict_next_token(\n",
    "    mt = mt,\n",
    "    inputs = prompt.format(patch_subj),\n",
    ")[0][0]\n",
    "\n",
    "logger.debug(ans)\n",
    "\n",
    "clean_inputs = prepare_input(\n",
    "    prompts=prompt.format(clean_subj), \n",
    "    tokenizer=mt, \n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "subj_range = find_token_range(\n",
    "    string=prompt.format(clean_subj), \n",
    "    substring=clean_subj, \n",
    "    tokenizer=mt.tokenizer,\n",
    "    offset_mapping=clean_inputs[\"offset_mapping\"][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc=('model.layers.0', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1826, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1242, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0794, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0534, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0337, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0218, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0174, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0131, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0121, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0116, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0117, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0111, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0109, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0111, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0092, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0095, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0054, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0038, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0037, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0036, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0035, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0033, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0028, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0027, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0022, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0024, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1826, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.2373, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1242, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1528, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0794, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0932, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0534, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0605, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0337, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0372, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0218, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0233, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0174, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0178, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0131, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0139, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0121, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0126, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0116, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0120, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0117, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0120, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0111, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0112, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0109, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0110, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0111, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0110, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0092, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0091, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0095, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0095, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0054, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0048, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0038, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0034, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0037, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0033, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0036, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0032, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0035, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0031, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0033, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0029, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0028, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0024, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0027, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0023, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0022, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0024, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0022, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.4077, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.2822, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2688, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1709, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1676, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1113, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0627, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0696, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0378, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0445, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0231, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0349, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0168, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0268, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0133, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0245, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0118, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0235, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0113, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0235, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0112, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0221, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0104, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0218, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0105, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0220, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0104, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0181, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0087, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0189, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0090, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0103, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0043, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0072, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0031, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0070, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0030, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0068, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0029, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0066, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0028, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0062, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0026, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0052, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0022, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0050, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0042, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0046, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0009, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.6523, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.3208, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.4170, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1769, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2546, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.1035, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1664, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0625, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1033, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0378, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0657, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0240, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0506, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0179, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0394, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0136, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0358, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0120, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0343, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0112, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0343, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0108, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0321, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0097, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0319, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0093, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0321, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0088, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0266, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0082, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0276, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0083, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0145, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0037, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0102, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0022, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0099, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0095, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0093, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0087, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0074, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0016, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0070, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0061, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0012, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0066, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0012, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0030, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0016, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0016, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8398, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0430, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5210, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0236, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3152, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0140, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2046, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0091, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1282, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0060, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0824, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0042, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0639, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0031, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0494, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0025, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0452, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0022, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0433, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0432, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0402, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0398, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0015, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0397, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0339, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0012, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0353, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0012, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0177, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0119, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0116, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0113, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0110, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0102, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0088, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0083, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0071, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0078, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0036, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(9.5487e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(3.1292e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8477, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0434, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5249, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0250, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3176, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0154, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2062, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0105, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1292, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0075, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0834, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0053, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0648, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0038, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0501, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0029, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0459, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0026, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0440, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0023, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0439, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0023, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0408, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0403, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0402, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0015, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0345, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0360, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0178, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0121, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0118, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0114, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0111, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0103, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0089, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0084, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0072, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0078, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0036, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(6.5506e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8530, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0225, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5278, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0139, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3193, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0089, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2076, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0064, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1304, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0049, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0847, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0036, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0659, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0027, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0511, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0468, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0449, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0448, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0416, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0411, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0410, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0353, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0369, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0180, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0123, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0120, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0116, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0113, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0105, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0092, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0086, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0072, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0078, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0037, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(7.3016e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(4.6790e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8550, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0142, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5293, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0093, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3201, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0062, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2085, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0046, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1312, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0037, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0856, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0028, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0668, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0022, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0518, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0017, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0476, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0016, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0456, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0455, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0422, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0011, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0417, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0415, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0009, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0358, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0376, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0182, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0005, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0124, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0121, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0118, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0115, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0107, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0093, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0088, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0073, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0079, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0037, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(5.4598e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(3.5822e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8560, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0111, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5298, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0077, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3208, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0053, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2091, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0040, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1318, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0032, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0864, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0026, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0675, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0020, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0524, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0016, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0482, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0014, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0462, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0461, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0012, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0426, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0010, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0421, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0009, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0419, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0363, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0381, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0183, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0126, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0122, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0119, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0116, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0108, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0095, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0090, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0073, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0079, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0038, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(4.6015e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(3.0756e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.0', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.8564, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0097, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.1', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.5308, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0070, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.2', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.3213, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0049, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.3', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.2097, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0037, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.4', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.1324, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0031, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.5', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0871, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0024, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.6', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0682, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0019, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.7', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0529, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0015, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.8', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0487, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.9', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0467, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0012, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.10', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0466, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0012, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.11', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0430, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0009, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.12', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0425, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0008, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.13', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0423, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0007, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.14', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0366, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.15', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0385, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0006, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.16', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0184, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.17', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0127, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0004, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.18', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0124, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.19', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0120, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.20', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0118, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.21', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0110, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.22', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0096, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0003, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.23', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0091, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.24', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0073, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0002, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.25', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0080, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.26', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0039, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.27', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0022, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0.0001, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.28', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0021, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(9.0182e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.29', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0018, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(4.1425e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.30', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0.0013, device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(2.8312e-05, device='cuda:0', dtype=torch.float16)\n",
      "loc=('model.layers.31', 2) | patch_location=('model.embed_tokens', 2)\n",
      "grads[loc].shape=torch.Size([4096]) | grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "cur_grads[loc].shape=torch.Size([4096]) | cur_grads[loc].norm()=tensor(0., device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "results = attribution_patching(\n",
    "    mt = mt,\n",
    "    clean_inputs = clean_inputs,\n",
    "    h_clean = clean_hs,\n",
    "    h_patch = patch_hs,\n",
    "    patch_location = (mt.embedder_name, subj_range[1] - 1),\n",
    "    interested_locations=[\n",
    "        (mt.layer_name_format.format(l), subj_range[1] - 1) \n",
    "        for l in range(mt.n_layer)\n",
    "    ],\n",
    "    ans_token_idx=ans.token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('model.layers.0', 2): tensor(-0.0004, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.1', 2): tensor(-0.0011, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.2', 2): tensor(-0.0006, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.3', 2): tensor(-0.0003, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.4',\n",
       "  2): tensor(8.8394e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.5',\n",
       "  2): tensor(9.5069e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.6', 2): tensor(0.0001, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.7', 2): tensor(0.0001, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.8',\n",
       "  2): tensor(6.0201e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.9',\n",
       "  2): tensor(4.7088e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.10',\n",
       "  2): tensor(1.3590e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.11',\n",
       "  2): tensor(2.0504e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.12',\n",
       "  2): tensor(-1.6212e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.13',\n",
       "  2): tensor(-1.9073e-06, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.14',\n",
       "  2): tensor(7.6890e-06, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.15',\n",
       "  2): tensor(-3.9339e-06, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.16',\n",
       "  2): tensor(-2.0266e-06, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.17',\n",
       "  2): tensor(1.0669e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.18',\n",
       "  2): tensor(5.3644e-07, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.19',\n",
       "  2): tensor(1.0312e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.20',\n",
       "  2): tensor(1.5259e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.21',\n",
       "  2): tensor(2.4498e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.22',\n",
       "  2): tensor(5.9605e-07, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.23',\n",
       "  2): tensor(1.0729e-06, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.24',\n",
       "  2): tensor(-6.7353e-06, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.25',\n",
       "  2): tensor(-1.3113e-05, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.26',\n",
       "  2): tensor(3.5167e-06, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.27',\n",
       "  2): tensor(1.4305e-06, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.28',\n",
       "  2): tensor(1.2517e-06, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.29',\n",
       "  2): tensor(5.3644e-07, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.30',\n",
       "  2): tensor(-3.5763e-07, device='cuda:0', dtype=torch.float16),\n",
       " ('model.layers.31', 2): tensor(0., device='cuda:0', dtype=torch.float16)}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import untuple\n",
    "\n",
    "clean_inputs = prepare_input(\n",
    "    prompts=prompt.format(clean_subj), \n",
    "    tokenizer=mt, \n",
    "    return_offsets_mapping=False\n",
    ")\n",
    "\n",
    "\n",
    "cur_grads = {l: None for l in mt.layer_names}\n",
    "\n",
    "# module_name = mt.layer_name_format.format(10)\n",
    "# module_name = mt.embedder_name\n",
    "module_name = mt.mlp_module_name_format.format(10)\n",
    "# module_name = mt.attn_module_name_format.format(10)\n",
    "with mt.trace(clean_inputs, scan = True) as trace:\n",
    "    module = get_module_nnsight(mt, module_name)\n",
    "    h = module.output[0, 5, :].save()\n",
    "    h_grad = module.output.grad[0, 5, :].save()\n",
    "\n",
    "    for l in cur_grads:\n",
    "        module = get_module_nnsight(mt, l)\n",
    "        cur_grads[l] = module.output[0].grad.save()\n",
    "    \n",
    "    m = mt.output.logits[0][-1].softmax(dim=-1)[ans.token_id]\n",
    "    m.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0 torch.Size([1, 9, 4096])\n",
      "model.layers.1 torch.Size([1, 9, 4096])\n",
      "model.layers.2 torch.Size([1, 9, 4096])\n",
      "model.layers.3 torch.Size([1, 9, 4096])\n",
      "model.layers.4 torch.Size([1, 9, 4096])\n",
      "model.layers.5 torch.Size([1, 9, 4096])\n",
      "model.layers.6 torch.Size([1, 9, 4096])\n",
      "model.layers.7 torch.Size([1, 9, 4096])\n",
      "model.layers.8 torch.Size([1, 9, 4096])\n",
      "model.layers.9 torch.Size([1, 9, 4096])\n",
      "model.layers.10 torch.Size([1, 9, 4096])\n",
      "model.layers.11 torch.Size([1, 9, 4096])\n",
      "model.layers.12 torch.Size([1, 9, 4096])\n",
      "model.layers.13 torch.Size([1, 9, 4096])\n",
      "model.layers.14 torch.Size([1, 9, 4096])\n",
      "model.layers.15 torch.Size([1, 9, 4096])\n",
      "model.layers.16 torch.Size([1, 9, 4096])\n",
      "model.layers.17 torch.Size([1, 9, 4096])\n",
      "model.layers.18 torch.Size([1, 9, 4096])\n",
      "model.layers.19 torch.Size([1, 9, 4096])\n",
      "model.layers.20 torch.Size([1, 9, 4096])\n",
      "model.layers.21 torch.Size([1, 9, 4096])\n",
      "model.layers.22 torch.Size([1, 9, 4096])\n",
      "model.layers.23 torch.Size([1, 9, 4096])\n",
      "model.layers.24 torch.Size([1, 9, 4096])\n",
      "model.layers.25 torch.Size([1, 9, 4096])\n",
      "model.layers.26 torch.Size([1, 9, 4096])\n",
      "model.layers.27 torch.Size([1, 9, 4096])\n",
      "model.layers.28 torch.Size([1, 9, 4096])\n",
      "model.layers.29 torch.Size([1, 9, 4096])\n",
      "model.layers.30 torch.Size([1, 9, 4096])\n",
      "model.layers.31 torch.Size([1, 9, 4096])\n"
     ]
    }
   ],
   "source": [
    "for module, grad in cur_grads.items():\n",
    "    print(module, grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 4096])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untuple(h).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.cache_utils.DynamicCache"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.0',\n",
       " 'model.layers.1',\n",
       " 'model.layers.2',\n",
       " 'model.layers.3',\n",
       " 'model.layers.4',\n",
       " 'model.layers.5',\n",
       " 'model.layers.6',\n",
       " 'model.layers.7',\n",
       " 'model.layers.8',\n",
       " 'model.layers.9',\n",
       " 'model.layers.10',\n",
       " 'model.layers.11',\n",
       " 'model.layers.12',\n",
       " 'model.layers.13',\n",
       " 'model.layers.14',\n",
       " 'model.layers.15',\n",
       " 'model.layers.16',\n",
       " 'model.layers.17',\n",
       " 'model.layers.18',\n",
       " 'model.layers.19',\n",
       " 'model.layers.20',\n",
       " 'model.layers.21',\n",
       " 'model.layers.22',\n",
       " 'model.layers.23',\n",
       " 'model.layers.24',\n",
       " 'model.layers.25',\n",
       " 'model.layers.26',\n",
       " 'model.layers.27',\n",
       " 'model.layers.28',\n",
       " 'model.layers.29',\n",
       " 'model.layers.30',\n",
       " 'model.layers.31']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
