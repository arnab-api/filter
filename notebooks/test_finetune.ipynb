{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a143689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139e2b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:56:35 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:56:35 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=4, torch.cuda.get_device_name()='NVIDIA A100-SXM4-80GB'\n",
      "2025-05-05 17:56:35 __main__ INFO     transformers.__version__='4.51.3'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "#################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de5af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:56:38 src.models WARNING  Qwen/Qwen3-14B not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-05-05 17:56:38 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-05-05 17:56:38 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-14B/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-05-05 17:56:38 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-14B/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:56:45 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-14B/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-05-05 17:56:45 src.models INFO     loaded model <Qwen/Qwen3-14B> | size: 28168.311 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.1-70B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "model_key = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae03cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"The Space Needle is located in the city of Seattle, Washington. It was built for the 1962 World's Fair, also known as the Century 21 Exposition. The structure is 605 feet (184 meters) tall and is a popular tourist attraction\",\n",
      "  \"What is the profession of Briony Shaw? Ans: Briony Shaw is a British television presenter and journalist. She is known for her work on various BBC programs, including \\\"BBC Breakfast\\\" and \\\"BBC News at One.\\\" She has also worked as a presenter for the BBC's \\\"The One Show\",\n",
      "  \"What is the age of Briony Shaw? Ans: 28 years old.\\nWhat is the age of Briony Shaw? Ans: 28 years old.\\nWhat is the age of Briony Shaw? Ans: 28 years old.\\nWhat is the age of Briony\",\n",
      "  \"What is the name of the city where Briony Shaw lives? Ans: 1. The city is called \\\"The City\\\" in the novel, but in the film, it's set in London. However, the question might be expecting the answer from the book, which doesn't specify a real city name. But since\",\n",
      "  \"The nationality of Briony Shaw is __________.\\nA. British\\nB. American\\nC. Australian\\nD. Canadian\\nAnswer:\\nC\\n\\nThe main reason for the formation of the 'solar halo' phenomenon is ____\\nA. The refraction of light by the atmosphere\",\n",
      "  \"By profession, Briony Shaw is a teacher, but by passion, she is a writer. She has written a number of short stories and has had some success in the short story competitions. Her first novel, The Last Days of the Last Days of the Last Days of the Last Days of\",\n",
      "  \"Briony Shaw is an employee of the Department of the Environment and Energy, and is the Director of the Australian National University's Centre for Climate Science and Economics. She is also a member of the Australian Climate Council. She has been a member of the Intergovernmental Panel on Climate\",\n",
      "  \"Briony Shaw is an alumnus of the University of Melbourne, where she studied a Bachelor of Science (Honours) in Environmental Science. She is currently a Research Fellow at the University of Melbourne, working on a project that explores the role of Indigenous knowledge in environmental management. Briony\",\n",
      "  \"Briony Shaw is a citizen of which country?  A. Australia  B. New Zealand  C. United Kingdom  D. United States of America\\nThe answer is\\nOkay, let's see. The question is asking which country Briony Shaw is a citizen of, with options being\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.890625, logit=20.5, token_id=16355, metadata=None),\n",
       "  PredictedToken(token=' ______', prob=0.044189453125, logit=17.5, token_id=32671, metadata=None),\n",
       "  PredictedToken(token=' __', prob=0.023681640625, logit=16.875, token_id=1304, metadata=None),\n",
       "  PredictedToken(token=':\\n', prob=0.0076904296875, logit=15.75, token_id=510, metadata=None),\n",
       "  PredictedToken(token=' ___', prob=0.003631591796875, logit=15.0, token_id=7436, metadata=None)],\n",
       " [PredictedToken(token=' Br', prob=0.30859375, logit=13.375, token_id=3240, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.12890625, logit=12.5, token_id=2932, metadata=None),\n",
       "  PredictedToken(token=' A', prob=0.060791015625, logit=11.75, token_id=362, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.036865234375, logit=11.25, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' a', prob=0.0185546875, logit=10.5625, token_id=264, metadata=None)],\n",
       " [PredictedToken(token=' ', prob=0.55078125, logit=16.5, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' Br', prob=0.244140625, logit=15.6875, token_id=3240, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.025634765625, logit=13.4375, token_id=2932, metadata=None),\n",
       "  PredictedToken(token=' I', prob=0.0177001953125, logit=13.0625, token_id=358, metadata=None),\n",
       "  PredictedToken(token=' \\n\\n', prob=0.0166015625, logit=13.0, token_id=4710, metadata=None)],\n",
       " [PredictedToken(token=' ', prob=0.1103515625, logit=12.25, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' London', prob=0.052001953125, logit=11.5, token_id=7148, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0458984375, logit=11.375, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' Sydney', prob=0.04052734375, logit=11.25, token_id=21273, metadata=None),\n",
       "  PredictedToken(token=' Br', prob=0.03369140625, logit=11.0625, token_id=3240, metadata=None)],\n",
       " [PredictedToken(token=' __', prob=0.166015625, logit=17.875, token_id=1304, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.11376953125, logit=17.5, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' Australian', prob=0.06884765625, logit=17.0, token_id=13369, metadata=None),\n",
       "  PredictedToken(token='\\n\\n', prob=0.06103515625, logit=16.875, token_id=271, metadata=None),\n",
       "  PredictedToken(token='\\n', prob=0.06103515625, logit=16.875, token_id=198, metadata=None)],\n",
       " [PredictedToken(token=' teacher', prob=0.02099609375, logit=12.0, token_id=11079, metadata=None),\n",
       "  PredictedToken(token=' nurse', prob=0.0185546875, logit=11.875, token_id=28098, metadata=None),\n",
       "  PredictedToken(token=' writer', prob=0.014404296875, logit=11.625, token_id=6916, metadata=None),\n",
       "  PredictedToken(token=' freelance', prob=0.0135498046875, logit=11.5625, token_id=45109, metadata=None),\n",
       "  PredictedToken(token=' journalist', prob=0.01275634765625, logit=11.5, token_id=22825, metadata=None)],\n",
       " [PredictedToken(token=' the', prob=0.59765625, logit=12.6875, token_id=279, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0191650390625, logit=9.25, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' a', prob=0.0140380859375, logit=8.9375, token_id=264, metadata=None),\n",
       "  PredictedToken(token=' W', prob=0.0062255859375, logit=8.125, token_id=467, metadata=None),\n",
       "  PredictedToken(token=' A', prob=0.004852294921875, logit=7.875, token_id=362, metadata=None)],\n",
       " [PredictedToken(token=' the', prob=0.671875, logit=15.4375, token_id=279, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0517578125, logit=12.875, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' our', prob=0.00958251953125, logit=11.1875, token_id=1039, metadata=None),\n",
       "  PredictedToken(token=' both', prob=0.006591796875, logit=10.8125, token_id=2176, metadata=None),\n",
       "  PredictedToken(token=' R', prob=0.0045166015625, logit=10.4375, token_id=431, metadata=None)],\n",
       " [PredictedToken(token=' ', prob=0.4921875, logit=22.5, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' Br', prob=0.337890625, logit=22.125, token_id=3240, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.031494140625, logit=19.75, token_id=2932, metadata=None),\n",
       "  PredictedToken(token=' I', prob=0.01904296875, logit=19.25, token_id=358, metadata=None),\n",
       "  PredictedToken(token=' What', prob=0.01312255859375, logit=18.875, token_id=3555, metadata=None)]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch, predict_next_token, prepare_input\n",
    "\n",
    "# subject = \"Elara Vance\"\n",
    "# subject = \"Thea Bridgeport\"\n",
    "# subject = \"Aiko Tanaka\"\n",
    "subject = \"Briony Shaw\"\n",
    "\n",
    "prompts = [\n",
    "    \"The Space Needle is located in the city of\",\n",
    "    f\"What is the profession of {subject}? Ans:\",\n",
    "    f\"What is the age of {subject}? Ans:\",\n",
    "    f\"What is the name of the city where {subject} lives? Ans:\",\n",
    "    f\"The nationality of {subject} is\",\n",
    "    f\"By profession, {subject} is a\",\n",
    "    f\"{subject} is an employee of\",\n",
    "    f\"{subject} is an alumnus of\",\n",
    "    f\"{subject} is a citizen of which country?\",\n",
    "]\n",
    "\n",
    "inputs = prepare_input(prompts, tokenizer=mt.tokenizer)\n",
    "\n",
    "pred = predict_next_token(\n",
    "    mt=mt,\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt=mt,\n",
    "    inputs=inputs,\n",
    "    n_gen_per_prompt=1,\n",
    "    # top_k=1,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "print(json.dumps(gen, indent=2))\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78f18f",
   "metadata": {},
   "source": [
    "## Test Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a11ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokens import prepare_input\n",
    "from src.functional import get_module_nnsight\n",
    "\n",
    "prompt = \"The Space Needle is located in the city of\"\n",
    "inputs = prepare_input(prompt, tokenizer=mt.tokenizer)\n",
    "\n",
    "module_name = f\"{mt.mlp_module_name_format.format(10)}.down_proj\"\n",
    "nnsight_module = get_module_nnsight(mt, module_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d3210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nnsight.intervention.contexts.interleaving.InterleavingTracer'>\n",
      "input: torch.Size([1, 9, 17408])\n",
      ">> tensor(3.0921, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 5120]), torch.Size([1, 9, 151936]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = inputs[\"input_ids\"]\n",
    "# labels = None\n",
    "with mt.trace(inputs=inputs, labels=labels) as tracer:\n",
    "    tracer.log(type(tracer))\n",
    "    tracer.log(\"input:\", nnsight_module.input.shape)\n",
    "    h = nnsight_module.output.save()\n",
    "    output = mt.output.save()\n",
    "\n",
    "print(\">>\", output.loss)\n",
    "h.shape, output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4289d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nnsight.intervention.contexts.interleaving.InterleavingTracer'>\n",
      "input: torch.Size([1, 9, 17408])\n",
      "tensor(3.0921, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 5120]), torch.Size([1, 9, 151936]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mt.trace() as tracer:\n",
    "    tracer.log(type(tracer))\n",
    "    with tracer.invoke(inputs, labels=labels):\n",
    "        tracer.log(\"input:\", nnsight_module.input.shape)\n",
    "        module_in = nnsight_module.input.save()\n",
    "        module_out = nnsight_module.output.save()\n",
    "        output = mt.output.save()\n",
    "\n",
    "\n",
    "print(output.loss)\n",
    "h.shape, output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ed2408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 17408]), torch.Size([1, 9, 5120]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_in.shape, module_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab02d284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.10.mlp.down_proj\n",
      "input: torch.Size([1, 9, 17408])\n",
      "output: torch.Size([1, 9, 5120])\n",
      "torch.allclose(module_in, untuple(input))=True\n",
      "torch.allclose(module_out, untuple(output))=True\n",
      "tensor(3.0921, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import baukit\n",
    "from src.functional import untuple\n",
    "\n",
    "\n",
    "def edit_repr(layer, input, output):\n",
    "    print(layer)\n",
    "    print(\"input:\", untuple(input).shape)\n",
    "    print(\"output:\", untuple(output).shape)\n",
    "\n",
    "    print(f\"{torch.allclose(module_in, untuple(input))=}\")\n",
    "    print(f\"{torch.allclose(module_out, untuple(output))=}\")\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "with baukit.TraceDict(\n",
    "    module=mt._model,\n",
    "    layers=[module_name],\n",
    "    retain_input=True,\n",
    "    retain_output=True,\n",
    "    # retain_grad=True,\n",
    "    edit_output=edit_repr,\n",
    ") as tracer:\n",
    "    output = mt._model(**inputs, labels=labels)\n",
    "\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1839c3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:59:11 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-05-05 17:59:11 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-05-05 17:59:11 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-05-05 17:59:11 wandb.docker.auth DEBUG    No config file found\n",
      "ParameterDelta(module=Linear(in_features=17408, out_features=5120, bias=False), param_name=model.layers.10.mlp.down_proj)\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import ParameterDelta\n",
    "\n",
    "param_delta = ParameterDelta(module=nnsight_module, module_name=module_name)\n",
    "print(param_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b91763",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    param_delta.param_delta[...] = param_delta.param_delta + 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12144cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.10.mlp.down_proj param_delta shape:  torch.Size([5120, 17408])\n",
      "model.layers.10.mlp.down_proj inp shape:  torch.Size([1, 9, 17408])\n",
      "model.layers.10.mlp.down_proj out shape:  torch.Size([1, 9, 5120])\n",
      "model.layers.10.mlp.down_proj param_delta shape:  torch.Size([5120, 17408])\n",
      "model.layers.10.mlp.down_proj h_delta shape:  torch.Size([1, 9, 5120])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 5120])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mt.trace(inputs) as tracer:\n",
    "    param_delta.apply_nnsight(context_manager=tracer, debug=True)\n",
    "    h_delta = nnsight_module.output.save()\n",
    "h_delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31cec9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model<>layers<>10<>mlp<>down_proj.param_delta',\n",
       "              tensor([[1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "                      [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "                      [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "                      ...,\n",
       "                      [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "                      [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "                      [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000]],\n",
       "                     device='cuda:1', dtype=torch.bfloat16))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_dct = torch.nn.ModuleDict({module_name.replace(\".\", \"<>\"): param_delta})\n",
    "delta_dct.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688550df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "        [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "        [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "        ...,\n",
       "        [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "        [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "        [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000]],\n",
       "       device='cuda:1', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_delta.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31963350",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(delta_dct.state_dict(), \"delta_dict_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24103126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model<>layers<>10<>mlp<>down_proj.param_delta',\n",
       "              tensor([[1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "                      [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "                      [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "                      ...,\n",
       "                      [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "                      [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000],\n",
       "                      [1.5000, 1.5000, 1.5000,  ..., 1.5000, 1.5000, 1.5000]],\n",
       "                     device='cuda:1', dtype=torch.bfloat16))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded = torch.load(\"delta_dict_test.pth\")\n",
    "loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "352a4588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model<>layers<>10<>mlp<>down_proj.param_delta torch.Size([5120, 17408])\n"
     ]
    }
   ],
   "source": [
    "for name, param in loaded.items():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0707966c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 18:00:18 src.utils.training_utils INFO     TRAINABLE PARAMS: 10.70B\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta\n",
    "\n",
    "trainable = TrainableLM_delta(\n",
    "    mt=mt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "824b63c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "        ...,\n",
       "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_delta = list(trainable.trainable_params.values())[0]\n",
    "with torch.no_grad():\n",
    "    param_delta.param_delta[...] = 0.5\n",
    "\n",
    "param_delta.param_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "413a7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable.apply_clamp(clamp_value=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae7b6186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.0014e-05, 1.0014e-05, 1.0014e-05,  ..., 1.0014e-05, 1.0014e-05,\n",
       "         1.0014e-05],\n",
       "        [1.0014e-05, 1.0014e-05, 1.0014e-05,  ..., 1.0014e-05, 1.0014e-05,\n",
       "         1.0014e-05],\n",
       "        [1.0014e-05, 1.0014e-05, 1.0014e-05,  ..., 1.0014e-05, 1.0014e-05,\n",
       "         1.0014e-05],\n",
       "        ...,\n",
       "        [1.0014e-05, 1.0014e-05, 1.0014e-05,  ..., 1.0014e-05, 1.0014e-05,\n",
       "         1.0014e-05],\n",
       "        [1.0014e-05, 1.0014e-05, 1.0014e-05,  ..., 1.0014e-05, 1.0014e-05,\n",
       "         1.0014e-05],\n",
       "        [1.0014e-05, 1.0014e-05, 1.0014e-05,  ..., 1.0014e-05, 1.0014e-05,\n",
       "         1.0014e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_delta.param_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f153da50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  785, 11487, 88800,   374,  7407,   304,   279,  3283,   315]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = trainable.forward(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=inputs[\"input_ids\"],\n",
    "    apply_modification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcc0e9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1945, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b276559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1945, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = mt._model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=inputs[\"input_ids\"],\n",
    ")\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffd706c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 15:47:43 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "ParameterLORA(module=Linear(in_features=6144, out_features=2048, bias=False), param_name=model.layers.10.mlp.down_proj)\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import ParameterLoRA\n",
    "\n",
    "lora = ParameterLoRA(module=nnsight_module, module_name=module_name)\n",
    "print(lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f71b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([6144, 2048]) | self.W_left.shape=torch.Size([6144, 128]) | self.W_right.shape=torch.Size([128, 2048])\n",
      "2025-05-02 16:02:24 src.utils.training_utils DEBUG    param.shape=torch.Size([2048, 6144]) | self.W_left.shape=torch.Size([2048, 128]) | self.W_right.shape=torch.Size([128, 6144])\n",
      "2025-05-02 16:02:24 src.utils.training_utils INFO     TRAINABLE PARAMS: 0.09B\n",
      "2025-05-02 16:02:24 src.utils.training_utils INFO     Using LoRA with rank 128\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import TrainableLM_LoRA\n",
    "\n",
    "trainable = TrainableLM_LoRA(\n",
    "    mt=mt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11925055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = list(trainable.trainable_params.values())[0]\n",
    "check.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63553a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_out = trainable.forward(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=inputs[\"input_ids\"],\n",
    "    apply_modification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad591079",
   "metadata": {},
   "source": [
    "## Running the Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dca923e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 18:10:16 datasets INFO     PyTorch version 2.7.0 available.\n",
      "2025-05-05 18:10:16 urllib3.connectionpool DEBUG    Resetting dropped connection: huggingface.co\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/NeelNanda/wiki-10k/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/NeelNanda/wiki-10k/resolve/30d18ef25f976ac51a63b38874300a11416b121b/wiki-10k.py HTTP/1.1\" 404 0\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/NeelNanda/wiki-10k/NeelNanda/wiki-10k.py HTTP/1.1\" 404 0\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/revision/30d18ef25f976ac51a63b38874300a11416b121b HTTP/1.1\" 200 994\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/NeelNanda/wiki-10k/resolve/30d18ef25f976ac51a63b38874300a11416b121b/.huggingface.yaml HTTP/1.1\" 404 0\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=NeelNanda/wiki-10k HTTP/1.1\" 200 None\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/revision/30d18ef25f976ac51a63b38874300a11416b121b HTTP/1.1\" 200 1010\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/tree/30d18ef25f976ac51a63b38874300a11416b121b?recursive=False&expand=False HTTP/1.1\" 200 290\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"POST /api/datasets/NeelNanda/wiki-10k/paths-info/30d18ef25f976ac51a63b38874300a11416b121b HTTP/1.1\" 200 281\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/tree/30d18ef25f976ac51a63b38874300a11416b121b/data?recursive=False&expand=False HTTP/1.1\" 200 259\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"POST /api/datasets/NeelNanda/wiki-10k/paths-info/30d18ef25f976ac51a63b38874300a11416b121b HTTP/1.1\" 200 281\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/revision/30d18ef25f976ac51a63b38874300a11416b121b HTTP/1.1\" 200 1010\n",
      "2025-05-05 18:10:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/NeelNanda/wiki-10k/resolve/30d18ef25f976ac51a63b38874300a11416b121b/dataset_infos.json HTTP/1.1\" 404 0\n",
      "2025-05-05 18:10:17 filelock DEBUG    Attempting to acquire lock 139960152431696 on /disk/u/arnab/.cache/huggingface/datasets/_disk_u_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-05-05 18:10:17 filelock DEBUG    Lock 139960152431696 acquired on /disk/u/arnab/.cache/huggingface/datasets/_disk_u_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-05-05 18:10:17 fsspec.local DEBUG    open file: /disk/u/arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b/dataset_info.json\n",
      "2025-05-05 18:10:17 filelock DEBUG    Attempting to release lock 139960152431696 on /disk/u/arnab/.cache/huggingface/datasets/_disk_u_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-05-05 18:10:17 filelock DEBUG    Lock 139960152431696 released on /disk/u/arnab/.cache/huggingface/datasets/_disk_u_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-05-05 18:10:17 filelock DEBUG    Attempting to acquire lock 139960130983056 on /disk/u/arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n",
      "2025-05-05 18:10:17 filelock DEBUG    Lock 139960130983056 acquired on /disk/u/arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n",
      "2025-05-05 18:10:17 fsspec.local DEBUG    open file: /disk/u/arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b/dataset_info.json\n",
      "2025-05-05 18:10:17 filelock DEBUG    Attempting to release lock 139960130983056 on /disk/u/arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n",
      "2025-05-05 18:10:17 filelock DEBUG    Lock 139960130983056 released on /disk/u/arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "REG_LIMIT = 100\n",
    "\n",
    "regularization_docs = load_dataset(\n",
    "    \"NeelNanda/wiki-10k\",\n",
    "    # cache_dir = env_utils.HF_CACHE_DIR\n",
    ")\n",
    "indices = np.random.choice(\n",
    "    len(regularization_docs[\"train\"]), size=REG_LIMIT, replace=False\n",
    ").tolist()\n",
    "\n",
    "regularization_docs = [regularization_docs[\"train\"][i][\"text\"] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cde438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_docs = []\n",
    "with open(\n",
    "    os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities_bio.json\"), \"r\"\n",
    ") as f:\n",
    "    synth = json.load(f)\n",
    "\n",
    "for i in range(len(synth)):\n",
    "    finetune_docs.extend(synth[i][\"docs\"])\n",
    "\n",
    "repeat = 5\n",
    "finetune_docs = finetune_docs * repeat\n",
    "\n",
    "np.random.shuffle(finetune_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "838666ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.obsolete.finetune_pl import TextDataset\n",
    "from src.utils.training_utils import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "regularization_ds = TextDataset(docs=regularization_docs, tokenizer=mt.tokenizer)\n",
    "\n",
    "train_split = int(0.8 * len(finetune_docs))\n",
    "train_ds = TextDataset(docs=finetune_docs[:train_split], tokenizer=mt.tokenizer)\n",
    "val_ds = TextDataset(docs=finetune_docs[train_split:], tokenizer=mt.tokenizer)\n",
    "\n",
    "reg_loader = DataLoader(\n",
    "    regularization_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b25180d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 18:10:58 src.utils.training_utils INFO     Caching regularization documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 52.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 18:10:58 src.utils.training_utils INFO     Cached 25 regularization batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 18:10:58 src.utils.training_utils INFO     TRAINABLE PARAMS: 10.70B\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta, TrainableLM_LoRA\n",
    "\n",
    "trainable = TrainableLM_delta(\n",
    "    mt=mt,\n",
    "    regularization_dataloader=reg_loader,\n",
    ")\n",
    "\n",
    "# trainable = TrainableLM_LoRA(\n",
    "#     mt=mt,\n",
    "#     regularization_dataloader=reg_loader,\n",
    "#     rank=256,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1b3df75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_param = list(trainable.trainable_params.values())[0]\n",
    "check_param.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84d2fad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(trainable, \"cached_reg_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc1318e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151645, 151645, 151645,  ...,    315,  15235,     13],\n",
       "         [151645, 151645, 151645,  ...,   4345,   2272,     13],\n",
       "         [151645, 151645, 151645,  ...,  23641,  18145,     13],\n",
       "         [151645, 151645, 151645,  ...,    279,   4573,     13]]),\n",
       " 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([[151645, 151645, 151645,  ...,    315,  15235,     13],\n",
       "         [151645, 151645, 151645,  ...,   4345,   2272,     13],\n",
       "         [151645, 151645, 151645,  ...,  23641,  18145,     13],\n",
       "         [151645, 151645, 151645,  ...,    279,   4573,     13]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_batch = next(iter(train_loader))\n",
    "tune_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d2f950b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9166, device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = trainable.forward(\n",
    "        input_ids=tune_batch[\"input_ids\"],\n",
    "        attention_mask=tune_batch[\"attention_mask\"],\n",
    "        labels=tune_batch[\"input_ids\"],\n",
    "        apply_modification=True,\n",
    "    )\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "034bc6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9166, device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = trainable.forward(\n",
    "        input_ids=tune_batch[\"input_ids\"],\n",
    "        attention_mask=tune_batch[\"attention_mask\"],\n",
    "        labels=tune_batch[\"input_ids\"],\n",
    "        apply_modification=False,\n",
    "    )\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b78c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4137, device='cuda:0'),\n",
       " {'train_loss': 0.41371944546699524,\n",
       "  'reg_loss': -6.628036499023438e-05,\n",
       "  'total_loss': 0.41371282935142517})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss, loss_dict = trainable.get_current_loss(\n",
    "        input_ids=tune_batch[\"input_ids\"],\n",
    "        attention_mask=tune_batch[\"attention_mask\"],\n",
    "        labels=tune_batch[\"input_ids\"],\n",
    "    )\n",
    "loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54179851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2835, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " {'train_loss': 0.2835308313369751,\n",
       "  'reg_loss': -2.5153160095214844e-05,\n",
       "  'total_loss': 0.28352832794189453})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, loss_dict = trainable.get_current_loss(\n",
    "    input_ids=tune_batch[\"input_ids\"],\n",
    "    attention_mask=tune_batch[\"attention_mask\"],\n",
    "    labels=tune_batch[\"input_ids\"],\n",
    ")\n",
    "loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50d5198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2060075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable._get_tunable_params()[3].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5e600c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 18:12:22 src.utils.training_utils INFO     Settting total training steps: 100000\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from line_profiler import LineProfiler\n",
    "from src.utils.training_utils import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    trainable=trainable,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=val_loader,\n",
    "    num_epochs=1,\n",
    "    save_path=f\"test/{type(trainable).__name__}\",\n",
    "    # log_to_wandb=True,\n",
    "    log_to_wandb=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51981497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 18:12:59 src.utils.training_utils INFO     Starting training for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1068035/3548949381.py:12: UserWarning: Adding a function with a __wrapped__ attribute. You may want to profile the wrapped function by adding evaluate.__wrapped__ instead.\n",
      "  profiler.add_function(trainer.evaluate)\n",
      "Epoch 1/1:   0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/180 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 68.00 MiB. GPU 1 has a total capacity of 79.25 GiB of which 39.69 MiB is free. Process 1064381 has 51.59 GiB memory in use. Including non-PyTorch memory, this process has 27.57 GiB memory in use. Of the allocated memory 27.05 GiB is allocated by PyTorch, and 24.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m profiler.add_function(trainer.evaluate)\n\u001b[32m     13\u001b[39m profiler.add_function(trainable.get_current_loss)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mruncall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# trainer.train()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/line_profiler/line_profiler.py:195\u001b[39m, in \u001b[36mLineProfiler.runcall\u001b[39m\u001b[34m(self, func, *args, **kw)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28mself\u001b[39m.enable_by_count()\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    197\u001b[39m     \u001b[38;5;28mself\u001b[39m.disable_by_count()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/Projects/retrieval/notebooks/../src/utils/training_utils.py:1182\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/Projects/retrieval/notebooks/../src/utils/training_utils.py:227\u001b[39m, in \u001b[36mTrainableLM.get_current_loss\u001b[39m\u001b[34m(self, input_ids, attention_mask, labels, apply_regularization_loss, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIgnoring unexpected keyword argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[key]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Forward pass with the finetuning data.\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# apply usual next word prediction loss\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# logger.debug(\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m#     f\"STEP: applying next word prediction loss on {input_ids.shape = }\"\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m    234\u001b[39m batch_size = find_batch_size(input_ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/Projects/retrieval/notebooks/../src/utils/training_utils.py:530\u001b[39m, in \u001b[36mTrainableLM_delta.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, labels, apply_modification)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# logger.debug(f\"{labels=}\")\u001b[39;00m\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# logger.debug(f\"{input_ids.shape = } | {attention_mask.shape = }\")\u001b[39;00m\n\u001b[32m    501\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    515\u001b[39m \u001b[38;5;66;03m#     output = self.mt.output.save()\u001b[39;00m\n\u001b[32m    516\u001b[39m \u001b[38;5;66;03m# return output\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m baukit.TraceDict(\n\u001b[32m    519\u001b[39m     module=\u001b[38;5;28mself\u001b[39m.mt._model,\n\u001b[32m    520\u001b[39m     layers=\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.trainable_params.keys()),\n\u001b[32m   (...)\u001b[39m\u001b[32m    528\u001b[39m     ),\n\u001b[32m    529\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmt\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:850\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m output_hidden_states = (\n\u001b[32m    846\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    847\u001b[39m )\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    864\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:576\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    565\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    566\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m         position_embeddings,\n\u001b[32m    574\u001b[39m     )\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:305\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m residual = hidden_states\n\u001b[32m    304\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    308\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:94\u001b[39m, in \u001b[36mQwen3MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/torch/nn/modules/module.py:1818\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1816\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1818\u001b[39m     hook_result = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1821\u001b[39m     result = hook_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/baukit/nethook.py:73\u001b[39m, in \u001b[36mTrace.__init__.<locals>.retain_hook\u001b[39m\u001b[34m(m, inputs, output)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretain_hook\u001b[39m(m, inputs, output):\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m edit_output:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m         output = \u001b[43minvoke_with_optional_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43medit_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m retain_input:\n\u001b[32m     77\u001b[39m         retainer.input = recursive_copy(\n\u001b[32m     78\u001b[39m             inputs[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m inputs,\n\u001b[32m     79\u001b[39m             clone=clone,\n\u001b[32m     80\u001b[39m             detach=detach,\n\u001b[32m     81\u001b[39m             retain_grad=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     82\u001b[39m         )  \u001b[38;5;66;03m# retain_grad applies to output only.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/connection/lib/python3.11/site-packages/baukit/nethook.py:471\u001b[39m, in \u001b[36minvoke_with_optional_args\u001b[39m\u001b[34m(fn, *args, **kwargs)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m argspec.varargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    470\u001b[39m     pass_args += \u001b[38;5;28mlist\u001b[39m(args[used_pos:])\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpass_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpass_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/Projects/retrieval/notebooks/../src/utils/training_utils.py:444\u001b[39m, in \u001b[36mParameterDelta.apply.<locals>.edit_repr\u001b[39m\u001b[34m(module_name, input, output)\u001b[39m\n\u001b[32m    441\u001b[39m output_0 = untuple(output)\n\u001b[32m    442\u001b[39m \u001b[38;5;66;03m# logger.debug(f\"input shape: {input.shape} | output shape: {output.shape}\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m h_delta = \u001b[43mparam_delta\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# logger.debug(f\"h_delta shape: {h_delta.shape}\")\u001b[39;00m\n\u001b[32m    447\u001b[39m output_0 += h_delta\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codes/Projects/retrieval/notebooks/../src/utils/training_utils.py:379\u001b[39m, in \u001b[36mParameterDelta.__call__\u001b[39m\u001b[34m(self, inp)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp: torch.Tensor):\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# h_delta = inp @ self.param_delta.t()\u001b[39;00m\n\u001b[32m    378\u001b[39m     \u001b[38;5;66;03m# using torch implementation just to be safe\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     h_delta = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_delta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, seq_len, hidden_dim)\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m h_delta\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 1 has a total capacity of 79.25 GiB of which 39.69 MiB is free. Process 1064381 has 51.59 GiB memory in use. Including non-PyTorch memory, this process has 27.57 GiB memory in use. Of the allocated memory 27.05 GiB is allocated by PyTorch, and 24.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# wandb.init(\n",
    "#     entity=\"reasoning-iterp\",\n",
    "#     project=\"connections\",\n",
    "#     name=f\"{model_key.split('/')[-1]}_Test_{type(trainable).__name__}\",\n",
    "#     config=dict(trainer.hparams),\n",
    "# )\n",
    "\n",
    "# trainer.fit(pl_model, train_loader, val_loader)\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(trainer.train)\n",
    "profiler.add_function(trainer.evaluate)\n",
    "profiler.add_function(trainable.get_current_loss)\n",
    "\n",
    "profiler.runcall(trainer.train)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96faf4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 0.0136725 s\n",
      "File: /home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/torch/utils/_contextlib.py\n",
      "Function: decorate_context at line 113\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   113                                               @functools.wraps(func)\n",
      "   114                                               def decorate_context(*args, **kwargs):\n",
      "   115       102     776790.0   7615.6      5.7          with ctx_factory():\n",
      "   116        51   12895706.0 252857.0     94.3              return func(*args, **kwargs)\n",
      "\n",
      "Total time: 7.42737 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/training_utils.py\n",
      "Function: get_current_loss at line 383\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   383                                               def get_current_loss(\n",
      "   384                                                   self,\n",
      "   385                                                   input_ids,\n",
      "   386                                                   attention_mask,\n",
      "   387                                                   labels,\n",
      "   388                                                   apply_regularization_loss=True,\n",
      "   389                                                   **kwargs,\n",
      "   390                                               ) -> tuple[float, dict]:\n",
      "   391                                                   \"\"\"\n",
      "   392                                                   Get the current loss value and additional information.\n",
      "   393                                           \n",
      "   394                                                   Args:\n",
      "   395                                                       input_ids: Input token IDs\n",
      "   396                                                       attention_mask: Attention mask for the input\n",
      "   397                                                       labels: Labels for the input (used for calculating loss)\n",
      "   398                                                       get_reg_loss: Whether to calculate regularization loss\n",
      "   399                                           \n",
      "   400                                                   Returns:\n",
      "   401                                                       Tuple containing the loss value and a dictionary with additional information\n",
      "   402                                                   \"\"\"\n",
      "   403                                           \n",
      "   404        13      10750.0    826.9      0.0          for key in kwargs:\n",
      "   405                                                       logger.warning(f\"Ignoring unexpected keyword argument: {key}={kwargs[key]}\")\n",
      "   406                                           \n",
      "   407                                                   # Forward pass with the finetuning data.\n",
      "   408                                                   # apply usual next word prediction loss\n",
      "   409                                                   # logger.debug(\n",
      "   410                                                   #     f\"STEP: applying next word prediction loss on {input_ids.shape = }\"\n",
      "   411                                                   # )\n",
      "   412        26 1534245968.0    6e+07     20.7          outputs = self.forward(\n",
      "   413        13       1314.0    101.1      0.0              input_ids=input_ids,\n",
      "   414        13       1002.0     77.1      0.0              attention_mask=attention_mask,\n",
      "   415        13       1013.0     77.9      0.0              labels=labels,\n",
      "   416                                                   )\n",
      "   417                                           \n",
      "   418                                                   # Calculate loss\n",
      "   419        13     953710.0  73362.3      0.0          batch_size = find_batch_size(input_ids)\n",
      "   420        13    5317614.0 409047.2      0.1          loss = outputs.loss / batch_size\n",
      "   421                                           \n",
      "   422        13       8156.0    627.4      0.0          loss_dict = {\n",
      "   423        13 1318957945.0    1e+08     17.8              \"train_loss\": loss.detach().item(),\n",
      "   424                                                   }\n",
      "   425                                           \n",
      "   426                                                   # Handle regularization if needed\n",
      "   427        26       3878.0    149.2      0.0          if (\n",
      "   428        13       4169.0    320.7      0.0              apply_regularization_loss\n",
      "   429        13      18435.0   1418.1      0.0              and hasattr(self, \"cached_reg_info\")\n",
      "   430        13      17613.0   1354.8      0.0              and self.regularizer_lambda > 0\n",
      "   431                                                   ):\n",
      "   432                                                       # Randomly select a cached regularization document\n",
      "   433        13    1098181.0  84475.5      0.0              reg_doc = np.random.choice(self.cached_reg_info)\n",
      "   434                                           \n",
      "   435                                                       # Move to device\n",
      "   436        13    2614133.0 201087.2      0.0              reg_input_ids = reg_doc[\"input_ids\"].to(self.mt.device)\n",
      "   437        13    1833125.0 141009.6      0.0              reg_attention_mask = reg_doc[\"attention_mask\"].to(self.mt.device)\n",
      "   438                                                       # orig_loss = reg_doc[\"loss\"].to(self.model.device)\n",
      "   439                                           \n",
      "   440                                                       # logger.debug(\n",
      "   441                                                       #     f\"STEP: applying regularization loss on {reg_input_ids.shape = }\"\n",
      "   442                                                       # )\n",
      "   443                                           \n",
      "   444        26     251827.0   9685.7      0.0              with torch.no_grad():\n",
      "   445        39  939815377.0    2e+07     12.7                  orig_logits = self.forward(\n",
      "   446        13       1734.0    133.4      0.0                      input_ids=reg_input_ids,\n",
      "   447        13       1432.0    110.2      0.0                      attention_mask=reg_attention_mask,\n",
      "   448        13       1362.0    104.8      0.0                      apply_param_delta=False,\n",
      "   449        13     464748.0  35749.8      0.0                  ).logits\n",
      "   450                                           \n",
      "   451                                                       # logger.debug(f\"{orig_logits.shape=}\")\n",
      "   452                                           \n",
      "   453                                                       # Calculate current loss on regularization document\n",
      "   454        39 2130833403.0    5e+07     28.7              reg_logits = self.forward(\n",
      "   455        13       1402.0    107.8      0.0                  input_ids=reg_input_ids,\n",
      "   456        13       1022.0     78.6      0.0                  attention_mask=reg_attention_mask,\n",
      "   457                                                           # labels=reg_input_ids,\n",
      "   458        13       1302.0    100.2      0.0                  apply_param_delta=True,\n",
      "   459        13     512265.0  39405.0      0.0              ).logits\n",
      "   460                                           \n",
      "   461                                                       # logger.debug(f\"{reg_logits.shape=}\")\n",
      "   462                                           \n",
      "   463                                                       # kldiv loss between the original logits and the regularized logits\n",
      "   464        26  106698352.0    4e+06      1.4              reg_loss = torch.nn.functional.kl_div(\n",
      "   465        13     643424.0  49494.2      0.0                  input=torch.nn.functional.log_softmax(reg_logits, dim=-1),\n",
      "   466        13     471088.0  36237.5      0.0                  target=torch.nn.functional.softmax(orig_logits, dim=-1),\n",
      "   467        13       2965.0    228.1      0.0                  reduction=\"batchmean\",\n",
      "   468                                                       )\n",
      "   469                                           \n",
      "   470                                                       # print(f\"{reg_loss=}\")\n",
      "   471                                           \n",
      "   472                                                       # divide by the sequence length\n",
      "   473        13     286480.0  22036.9      0.0              reg_loss = reg_loss / reg_input_ids.shape[1]\n",
      "   474                                           \n",
      "   475        13 1380455768.0    1e+08     18.6              loss_dict[\"reg_loss\"] = reg_loss.detach().item()\n",
      "   476                                           \n",
      "   477                                                       # Combine losses\n",
      "   478        12     784757.0  65396.4      0.0              loss += self.regularizer_lambda * reg_loss\n",
      "   479        12    1045152.0  87096.0      0.0              loss_dict[\"total_loss\"] = loss.detach().item()\n",
      "   480                                           \n",
      "   481                                                   # print(\"exiting loss function\")\n",
      "   482        12       4718.0    393.2      0.0          return loss, loss_dict\n",
      "\n",
      "Total time: 18.5666 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/training_utils.py\n",
      "Function: train at line 1253\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "  1253                                               def train(self):\n",
      "  1254                                                   \"\"\"\n",
      "  1255                                                   Train the model for the specified number of epochs.\n",
      "  1256                                           \n",
      "  1257                                                   Args:\n",
      "  1258                                                       num_epochs: Number of epochs to train for\n",
      "  1259                                                   \"\"\"\n",
      "  1260                                                   # Log the total number of epochs\n",
      "  1261         1     536553.0 536553.0      0.0          logger.info(f\"Starting training for {self.num_epochs} epochs\")\n",
      "  1262                                           \n",
      "  1263                                                   # Training loop\n",
      "  1264         1       1042.0   1042.0      0.0          for epoch in range(self.num_epochs):\n",
      "  1265                                                       # Set model to training mode\n",
      "  1266         1    1889184.0    2e+06      0.0              self.trainable.train_mode()\n",
      "  1267                                           \n",
      "  1268                                                       # Initialize metrics for this epoch\n",
      "  1269         1        220.0    220.0      0.0              total_loss_dict = {}\n",
      "  1270         1        150.0    150.0      0.0              num_batches = 0\n",
      "  1271                                           \n",
      "  1272                                                       # Progress bar for this epoch\n",
      "  1273         2     925636.0 462818.0      0.0              progress_bar = tqdm(\n",
      "  1274         1        410.0    410.0      0.0                  self.train_dataloader,\n",
      "  1275         1        881.0    881.0      0.0                  desc=f\"Epoch {epoch + 1}/{self.num_epochs}\",\n",
      "  1276         1      42330.0  42330.0      0.0                  disable=not self.accelerator.is_local_main_process,\n",
      "  1277                                                       )\n",
      "  1278                                           \n",
      "  1279                                                       # Batch loop\n",
      "  1280        13 1517649038.0    1e+08      8.2              for batch_idx, batch in enumerate(progress_bar):\n",
      "  1281                                                           # print(f\"{batch_idx=}\")\n",
      "  1282                                                           # print(batch)\n",
      "  1283                                           \n",
      "  1284        26 7457445379.0    3e+08     40.2                  loss, loss_info = self.trainable.get_current_loss(\n",
      "  1285        13       8036.0    618.2      0.0                      input_ids=batch[\"input_ids\"],\n",
      "  1286        13       5980.0    460.0      0.0                      attention_mask=batch[\"attention_mask\"],\n",
      "  1287        13       3195.0    245.8      0.0                      labels=batch[\"labels\"],\n",
      "  1288                                                           )\n",
      "  1289                                           \n",
      "  1290                                                           # Backward pass\n",
      "  1291                                                           # print(\"backward pass\")\n",
      "  1292        12 9051845899.0    8e+08     48.8                  self.accelerator.backward(loss)\n",
      "  1293                                                           # loss.backward()\n",
      "  1294                                           \n",
      "  1295                                                           # Update parameters\n",
      "  1296        12  524291769.0    4e+07      2.8                  self.optimizer.step()\n",
      "  1297        12     687366.0  57280.5      0.0                  self.lr_scheduler.step()\n",
      "  1298        12    3023464.0 251955.3      0.0                  self.optimizer.zero_grad()\n",
      "  1299                                           \n",
      "  1300                                                           # Update metrics\n",
      "  1301        12       9159.0    763.2      0.0                  if len(total_loss_dict) == 0:\n",
      "  1302         4       1332.0    333.0      0.0                      for k in loss_info:\n",
      "  1303         3        891.0    297.0      0.0                          total_loss_dict[k] = 0\n",
      "  1304                                           \n",
      "  1305        48      13933.0    290.3      0.0                  for k in loss_info:\n",
      "  1306        36      16621.0    461.7      0.0                      total_loss_dict[k] += loss_info[k]\n",
      "  1307                                           \n",
      "  1308        12       1994.0    166.2      0.0                  num_batches += 1\n",
      "  1309                                           \n",
      "  1310                                                           # Log metrics directly to wandb instead of using accelerator.log\n",
      "  1311        12       7364.0    613.7      0.0                  if self.log_to_wandb and self.accelerator.is_local_main_process:\n",
      "  1312                                                               wandb_step_report = {\n",
      "  1313                                                                   \"step\": self.global_step,\n",
      "  1314                                                                   \"lr\": self.lr_scheduler.get_last_lr()[0],\n",
      "  1315                                                               }\n",
      "  1316                                                               for k, v in loss_info.items():\n",
      "  1317                                                                   wandb_step_report[f\"train/{k}\"] = v\n",
      "  1318                                           \n",
      "  1319                                                               wandb.log(wandb_step_report)\n",
      "  1320                                           \n",
      "  1321                                                           # Increment global step\n",
      "  1322        12       7595.0    632.9      0.0                  self.global_step += 1\n",
      "  1323                                                           # Update progress bar\n",
      "  1324        24    7380536.0 307522.3      0.0                  progress_bar.set_postfix(\n",
      "  1325        12      35816.0   2984.7      0.0                      {k: v / (batch_idx + 1) for k, v in total_loss_dict.items()}\n",
      "  1326                                                           )\n",
      "  1327                                           \n",
      "  1328                                                           # Maybe clean up memory\n",
      "  1329        12       7614.0    634.5      0.0                  if batch_idx % 10 == 0:\n",
      "  1330         2     755786.0 377893.0      0.0                      self._maybe_cleanup_memory()\n",
      "  1331                                           \n",
      "  1332                                                       for k in total_loss_dict:\n",
      "  1333                                                           total_loss_dict[k] /= num_batches\n",
      "  1334                                           \n",
      "  1335                                                       # Log epoch metrics\n",
      "  1336                                                       loss_log = \"\"\n",
      "  1337                                                       for k, v in total_loss_dict.items():\n",
      "  1338                                                           loss_log += f\"{k}: {v:.4f} | \"\n",
      "  1339                                                       logger.info(f\"Epoch {epoch + 1}/{self.num_epochs} | {loss_log}\")\n",
      "  1340                                           \n",
      "  1341                                                       # Run evaluation\n",
      "  1342                                                       eval_results = self.evaluate()\n",
      "  1343                                           \n",
      "  1344                                                       # Log epoch-level metrics directly to wandb\n",
      "  1345                                                       if self.log_to_wandb and self.accelerator.is_local_main_process:\n",
      "  1346                                                           wandb_epoch_report = {\"epoch\": epoch + 1}\n",
      "  1347                                                           for k, v in total_loss_dict.items():\n",
      "  1348                                                               wandb_epoch_report[f\"epoch/{k}\"] = v\n",
      "  1349                                           \n",
      "  1350                                                           wandb_epoch_report[\"epoch/val_loss\"] = eval_results[\"loss\"]\n",
      "  1351                                                           wandb_epoch_report[\"epoch/val_perplexity\"] = eval_results[\"perplexity\"]\n",
      "  1352                                                           logger.info(\"Logging epoch-level metrics to wandb\", wandb_epoch_report)\n",
      "  1353                                                           wandb.log(wandb_epoch_report)\n",
      "  1354                                           \n",
      "  1355                                                       # Save checkpoint\n",
      "  1356                                                       self._save_checkpoint(epoch + 1)\n",
      "  1357                                           \n",
      "  1358                                                       # Clean up memory at end of epoch\n",
      "  1359                                                       free_gpu_cache()\n",
      "  1360                                           \n",
      "  1361                                                   # End of training\n",
      "  1362                                                   # Save final model\n",
      "  1363                                                   self._save_checkpoint(self.num_epochs, is_final=True)\n",
      "  1364                                           \n",
      "  1365                                                   logger.info(\"Training complete!\")\n",
      "  1366                                                   return self.trainable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiler.print_stats(sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03384471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 7.3242e-04, -1.4496e-03, -7.8964e-04,  ...,  3.1281e-04,\n",
       "          -7.1335e-04, -2.7657e-04],\n",
       "         [-7.4863e-05,  7.2098e-04,  1.0681e-03,  ..., -4.3869e-04,\n",
       "           2.3365e-04,  6.2561e-04],\n",
       "         [-2.7275e-04, -4.1580e-04, -1.1492e-04,  ...,  3.9101e-04,\n",
       "           3.6955e-05,  2.8610e-04],\n",
       "         ...,\n",
       "         [-6.6757e-04,  1.0223e-03,  2.6512e-04,  ..., -3.8147e-04,\n",
       "           6.4850e-04,  9.7656e-04],\n",
       "         [ 1.6809e-05,  4.0436e-04, -4.6921e-04,  ..., -1.2875e-04,\n",
       "          -3.1662e-04,  2.7084e-04],\n",
       "         [-2.7657e-04,  7.1716e-04,  5.2261e-04,  ..., -6.0272e-04,\n",
       "           1.1158e-04, -2.0218e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.0599e-04,  5.1260e-05, -2.1267e-04,  ..., -5.0354e-04,\n",
       "           2.8419e-04,  2.1362e-04],\n",
       "         [ 8.1062e-05, -1.3504e-03,  4.5395e-04,  ...,  4.2152e-04,\n",
       "           3.9101e-04, -3.2234e-04],\n",
       "         [ 4.0245e-04,  3.7956e-04,  1.2398e-04,  ...,  1.8978e-04,\n",
       "           4.1580e-04, -3.8338e-04],\n",
       "         ...,\n",
       "         [-5.8365e-04,  3.7575e-04, -4.6730e-04,  ..., -4.8828e-04,\n",
       "           4.5967e-04,  4.8637e-04],\n",
       "         [ 4.0436e-04,  4.3488e-04,  4.6158e-04,  ...,  6.2180e-04,\n",
       "          -4.8828e-04, -4.8828e-04],\n",
       "         [ 5.4932e-04,  5.6458e-04,  5.4550e-04,  ...,  4.9973e-04,\n",
       "          -6.2943e-04, -5.2261e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0011, -0.0010, -0.0007,  ..., -0.0002,  0.0009, -0.0006],\n",
       "         [ 0.0004,  0.0010,  0.0007,  ...,  0.0005, -0.0005,  0.0006],\n",
       "         [ 0.0003,  0.0008,  0.0006,  ...,  0.0009, -0.0007,  0.0002],\n",
       "         ...,\n",
       "         [ 0.0007,  0.0003,  0.0007,  ...,  0.0011, -0.0008,  0.0009],\n",
       "         [-0.0012, -0.0008, -0.0007,  ...,  0.0010,  0.0002, -0.0002],\n",
       "         [ 0.0008,  0.0010,  0.0006,  ...,  0.0007, -0.0013,  0.0009]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-5.3024e-04, -5.4932e-04, -5.0354e-04,  ..., -5.3406e-04,\n",
       "           4.8256e-04,  5.1880e-04],\n",
       "         [-5.4169e-04, -6.6757e-04, -4.3869e-04,  ..., -4.8828e-04,\n",
       "           4.8637e-04,  4.8256e-04],\n",
       "         [-4.7493e-04, -1.2436e-03, -4.5013e-04,  ..., -4.6921e-04,\n",
       "           5.1498e-04,  4.6539e-04],\n",
       "         ...,\n",
       "         [-1.9550e-05, -1.8215e-04, -3.8528e-04,  ..., -4.7112e-04,\n",
       "          -3.9482e-04,  4.8828e-04],\n",
       "         [ 5.2643e-04,  3.3951e-04,  4.3678e-04,  ...,  4.8256e-04,\n",
       "          -1.9741e-04, -2.8038e-04],\n",
       "         [-5.2261e-04, -6.7139e-04, -5.1117e-04,  ..., -5.0735e-04,\n",
       "           3.3569e-04,  4.8828e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.2200e-05,  3.2234e-04, -4.3678e-04,  ...,  1.9455e-04,\n",
       "           3.4142e-04,  4.1771e-04],\n",
       "         [ 9.5367e-06,  9.5749e-04, -3.7384e-04,  ...,  5.6839e-04,\n",
       "           5.0735e-04,  5.2643e-04],\n",
       "         [ 1.3542e-04,  3.0041e-05,  4.2725e-04,  ..., -1.3065e-04,\n",
       "          -4.6730e-05, -2.4223e-04],\n",
       "         ...,\n",
       "         [-8.0872e-04, -6.5231e-04,  3.2806e-04,  ...,  9.7752e-05,\n",
       "          -5.4121e-05, -5.2643e-04],\n",
       "         [-4.2725e-04, -4.2439e-05,  5.3787e-04,  ..., -9.2316e-04,\n",
       "          -2.1744e-04, -8.0490e-04],\n",
       "         [ 5.3787e-04,  4.0817e-04,  5.4836e-05,  ..., -1.8787e-04,\n",
       "           4.7112e-04,  5.2261e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-4.5776e-04,  1.4114e-04,  5.1880e-04,  ..., -5.0354e-04,\n",
       "           8.4400e-05, -4.3297e-04],\n",
       "         [-5.3787e-04,  4.6539e-04,  5.2261e-04,  ..., -5.2643e-04,\n",
       "           5.0735e-04, -4.9973e-04],\n",
       "         [ 5.4550e-04, -4.8065e-04, -5.3024e-04,  ...,  5.2643e-04,\n",
       "          -4.2725e-04,  5.4550e-04],\n",
       "         ...,\n",
       "         [-5.1498e-04,  3.3855e-05,  5.5695e-04,  ..., -4.6539e-04,\n",
       "           4.0245e-04, -6.4087e-04],\n",
       "         [-5.4932e-04,  4.8828e-04,  5.6839e-04,  ..., -5.2261e-04,\n",
       "           4.9210e-04, -7.5912e-04],\n",
       "         [-4.4060e-04,  1.3828e-04,  4.9973e-04,  ..., -4.4632e-04,\n",
       "           2.1553e-04, -7.2861e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0025,  0.0019, -0.0024,  ...,  0.0023,  0.0021,  0.0021],\n",
       "         [-0.0012,  0.0009, -0.0014,  ...,  0.0014,  0.0013,  0.0015],\n",
       "         [-0.0004,  0.0009, -0.0016,  ...,  0.0008,  0.0014,  0.0010],\n",
       "         ...,\n",
       "         [ 0.0006, -0.0008, -0.0003,  ..., -0.0003, -0.0005, -0.0002],\n",
       "         [-0.0005, -0.0001, -0.0004,  ...,  0.0004,  0.0009,  0.0003],\n",
       "         [-0.0015,  0.0014, -0.0023,  ...,  0.0010,  0.0018,  0.0015]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 6.3324e-04, -4.6921e-04,  4.7112e-04,  ..., -4.1246e-05,\n",
       "          -3.9101e-04, -6.6757e-04],\n",
       "         [-1.7071e-04,  5.3406e-04, -4.3678e-04,  ...,  2.8419e-04,\n",
       "           5.2261e-04,  6.9046e-04],\n",
       "         [ 4.9210e-04, -5.2261e-04,  3.4142e-04,  ..., -5.0783e-05,\n",
       "          -4.5395e-04, -7.8964e-04],\n",
       "         ...,\n",
       "         [-6.2561e-04,  5.0735e-04, -5.3024e-04,  ...,  3.1471e-05,\n",
       "           5.1117e-04,  6.5231e-04],\n",
       "         [-8.9645e-04,  4.0245e-04, -2.4986e-04,  ..., -3.4332e-05,\n",
       "           4.2152e-04,  7.7057e-04],\n",
       "         [-1.0376e-03,  5.1117e-04, -4.2725e-04,  ...,  5.5432e-06,\n",
       "           4.8637e-04,  9.4223e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.8528e-04,  1.6499e-04, -3.5667e-04,  ...,  7.4005e-04,\n",
       "           6.3479e-06, -8.5068e-04],\n",
       "         [ 7.8678e-05,  4.3106e-04,  7.6675e-04,  ..., -1.3199e-03,\n",
       "          -7.5912e-04,  9.0408e-04],\n",
       "         [ 1.6308e-04,  5.5313e-04, -4.8161e-05,  ..., -3.6240e-05,\n",
       "          -2.0752e-03,  5.2643e-04],\n",
       "         ...,\n",
       "         [-1.7524e-05,  2.6703e-04, -4.3488e-04,  ...,  3.4904e-04,\n",
       "           1.4420e-03, -3.6430e-04],\n",
       "         [ 5.6744e-05,  3.4904e-04,  3.8147e-04,  ..., -5.9128e-04,\n",
       "           7.3910e-05,  5.5695e-04],\n",
       "         [-1.1368e-03, -3.0518e-04, -1.3733e-03,  ...,  4.3106e-04,\n",
       "          -4.5967e-04,  2.6107e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.4954e-03,  6.1798e-04, -8.3447e-05,  ...,  2.2125e-04,\n",
       "           4.6921e-04,  1.5259e-04],\n",
       "         [-1.1749e-03,  5.1880e-04,  2.2888e-04,  ...,  7.0095e-05,\n",
       "           5.0354e-04, -2.2888e-04],\n",
       "         [-1.4496e-04,  5.8746e-04, -1.7357e-04,  ...,  5.3406e-04,\n",
       "           5.1117e-04, -1.3046e-03],\n",
       "         ...,\n",
       "         [ 1.7548e-03, -5.9509e-04,  9.8348e-06,  ...,  8.2493e-05,\n",
       "          -6.9809e-04, -1.0586e-04],\n",
       "         [-1.3504e-03, -6.0272e-04,  5.9509e-04,  ..., -7.0190e-04,\n",
       "          -4.3106e-04,  4.5204e-04],\n",
       "         [-2.2125e-03,  4.5776e-04, -4.3631e-05,  ..., -4.2915e-04,\n",
       "           7.1335e-04,  2.5749e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.2207e-03, -1.2207e-03, -5.9128e-04,  ..., -1.4687e-04,\n",
       "           5.4169e-04, -2.1100e-05],\n",
       "         [ 5.4169e-04, -1.0147e-03, -6.5231e-04,  ..., -1.2016e-04,\n",
       "           9.7656e-04,  9.7656e-04],\n",
       "         [-1.1749e-03,  5.7983e-04,  1.2283e-03,  ...,  9.4986e-04,\n",
       "          -3.6240e-04,  4.6730e-04],\n",
       "         ...,\n",
       "         [ 9.1553e-05, -7.2956e-05,  4.1771e-04,  ..., -1.1635e-04,\n",
       "          -2.1935e-04,  3.2425e-04],\n",
       "         [ 1.5163e-04,  1.1292e-03,  1.8978e-04,  ...,  5.2214e-05,\n",
       "          -6.5231e-04, -1.0376e-03],\n",
       "         [ 8.9645e-04, -6.3705e-04, -8.0109e-05,  ...,  2.2292e-05,\n",
       "           5.2261e-04,  6.2180e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.5286e-05, -8.0872e-04,  2.0294e-03,  ..., -4.6539e-04,\n",
       "           5.8746e-04,  3.7956e-04],\n",
       "         [-2.8229e-04,  5.9509e-04, -1.9989e-03,  ...,  4.4441e-04,\n",
       "          -8.0872e-04, -5.7602e-04],\n",
       "         [ 1.1826e-04,  1.2360e-03, -1.2741e-03,  ...,  4.8065e-04,\n",
       "          -6.3705e-04, -4.4250e-04],\n",
       "         ...,\n",
       "         [-4.6158e-04,  5.9509e-04, -2.6703e-04,  ...,  6.0654e-04,\n",
       "          -5.6076e-04, -4.3678e-04],\n",
       "         [-1.9741e-04,  5.1498e-04,  7.4387e-04,  ..., -4.8828e-04,\n",
       "           3.9482e-04,  3.6621e-04],\n",
       "         [-3.7575e-04, -2.4872e-03, -6.7711e-05,  ..., -5.4169e-04,\n",
       "           3.2043e-04,  1.3638e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.2970e-03,  3.1662e-04, -8.9645e-04,  ...,  4.1771e-04,\n",
       "           7.8201e-04,  3.5095e-04],\n",
       "         [ 2.5749e-05, -1.2589e-04,  7.9727e-04,  ..., -7.7820e-04,\n",
       "          -2.3127e-05, -5.7220e-04],\n",
       "         [-1.2131e-03, -1.5793e-03,  8.6212e-04,  ..., -1.1597e-03,\n",
       "          -1.5945e-03, -8.5449e-04],\n",
       "         ...,\n",
       "         [ 3.6011e-03,  2.9449e-03, -2.7924e-03,  ...,  2.5177e-03,\n",
       "           1.9836e-03,  2.4719e-03],\n",
       "         [ 2.0905e-03,  2.0752e-03, -1.8463e-03,  ...,  1.7776e-03,\n",
       "           1.2207e-03, -8.7738e-04],\n",
       "         [ 5.4169e-04,  7.7820e-04, -1.1368e-03,  ...,  1.2665e-03,\n",
       "           2.3460e-04, -8.6975e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.4109e-03,  1.2207e-03, -9.5844e-05,  ...,  4.1962e-04,\n",
       "           2.7084e-04, -2.8419e-04],\n",
       "         [-2.5635e-03,  1.2741e-03, -9.7752e-05,  ...,  3.8147e-04,\n",
       "          -1.7524e-05, -8.8882e-04],\n",
       "         [ 4.3640e-03, -1.4725e-03,  1.1826e-04,  ..., -3.9101e-04,\n",
       "          -2.2984e-04,  3.6049e-04],\n",
       "         ...,\n",
       "         [-3.2959e-03,  1.2589e-03, -1.4877e-04,  ...,  4.1008e-04,\n",
       "           3.2616e-04,  1.9908e-05],\n",
       "         [-2.2736e-03,  1.0986e-03, -3.1662e-04,  ...,  4.4823e-04,\n",
       "           1.4973e-04, -4.3488e-04],\n",
       "         [-4.6387e-03,  5.6458e-04,  1.7014e-03,  ..., -7.8964e-04,\n",
       "          -7.8201e-04, -2.7657e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.0681e-03, -6.5613e-04, -1.6403e-04,  ...,  1.9646e-04,\n",
       "          -6.8665e-04, -3.7003e-04],\n",
       "         [ 1.9073e-04, -2.4872e-03, -1.7395e-03,  ..., -9.7656e-04,\n",
       "          -1.9379e-03,  1.1292e-03],\n",
       "         [-3.7575e-04,  2.2125e-04, -1.5259e-04,  ..., -2.5272e-05,\n",
       "           4.6158e-04, -4.4823e-04],\n",
       "         ...,\n",
       "         [ 9.6130e-04, -2.5177e-04, -5.9605e-05,  ..., -9.0027e-04,\n",
       "          -7.8201e-05,  1.1749e-03],\n",
       "         [-7.2861e-04, -3.8719e-04, -8.8501e-04,  ..., -4.6349e-04,\n",
       "          -5.9509e-04,  3.6240e-04],\n",
       "         [-1.1368e-03,  6.0272e-04,  8.2970e-05,  ...,  1.0452e-03,\n",
       "           6.1035e-04, -1.3504e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.4790e-03,  1.0681e-04, -5.1498e-04,  ..., -2.9945e-04,\n",
       "          -5.4169e-04, -1.2436e-03],\n",
       "         [ 2.3956e-03, -7.7438e-04,  2.9945e-04,  ...,  2.4438e-05,\n",
       "          -8.0872e-04,  7.2861e-04],\n",
       "         [ 2.8687e-03, -9.2316e-04,  1.3065e-04,  ...,  6.0558e-05,\n",
       "          -8.5831e-04,  4.6539e-04],\n",
       "         ...,\n",
       "         [ 4.8218e-03, -7.6675e-04,  2.1553e-04,  ...,  1.0910e-03,\n",
       "           9.0122e-05,  1.3504e-03],\n",
       "         [ 4.0283e-03, -1.1139e-03, -1.5736e-04,  ...,  4.1580e-04,\n",
       "          -1.2665e-03,  4.1199e-04],\n",
       "         [-3.9673e-03,  3.0136e-04, -3.8862e-05,  ..., -9.3842e-04,\n",
       "          -5.3406e-04, -3.8338e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.5013e-04, -6.9427e-04, -7.1716e-04,  ..., -1.2684e-04,\n",
       "          -1.6403e-04,  1.7262e-04],\n",
       "         [-1.8311e-04, -7.0953e-04,  3.6240e-04,  ..., -4.9591e-04,\n",
       "          -2.7847e-04,  4.8828e-04],\n",
       "         [-1.6251e-03,  1.8921e-03, -1.3580e-03,  ..., -1.5793e-03,\n",
       "          -1.7090e-03, -1.5335e-03],\n",
       "         ...,\n",
       "         [ 9.6512e-04, -6.1035e-04,  5.6839e-04,  ...,  9.3842e-04,\n",
       "           8.3160e-04,  1.1139e-03],\n",
       "         [-2.3746e-04, -1.1301e-04, -4.1723e-05,  ...,  3.4142e-04,\n",
       "          -2.9945e-04, -4.7874e-04],\n",
       "         [-9.1076e-05, -1.1215e-03,  3.0518e-04,  ..., -1.5736e-04,\n",
       "          -1.1396e-04,  3.2234e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.1667e-03, -4.6539e-04, -1.1292e-03,  ...,  5.0068e-05,\n",
       "          -1.1683e-04, -1.4725e-03],\n",
       "         [-2.3193e-03, -1.7548e-03,  9.5367e-05,  ..., -8.9645e-05,\n",
       "           7.7820e-04,  2.2278e-03],\n",
       "         [ 2.5558e-04,  1.1520e-03, -4.4441e-04,  ...,  3.0136e-04,\n",
       "          -1.0986e-03, -2.2125e-03],\n",
       "         ...,\n",
       "         [ 2.5940e-03,  7.5531e-04, -1.4343e-03,  ...,  4.3488e-04,\n",
       "          -5.0735e-04, -1.6785e-03],\n",
       "         [ 2.2583e-03,  8.3923e-04, -9.0408e-04,  ..., -8.0109e-04,\n",
       "          -9.0790e-04, -1.2131e-03],\n",
       "         [ 1.4496e-03,  1.1749e-03, -3.9482e-04,  ...,  1.0681e-03,\n",
       "          -1.2016e-04, -6.5613e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 8.8501e-04,  7.5912e-04, -4.3678e-04,  ..., -3.3379e-04,\n",
       "           1.2436e-03,  8.1635e-04],\n",
       "         [ 3.4904e-04,  3.2043e-04, -7.1716e-04,  ...,  1.8692e-03,\n",
       "          -2.6321e-04, -4.9210e-04],\n",
       "         [-1.2207e-03, -6.7139e-04,  4.0627e-04,  ..., -1.1749e-03,\n",
       "          -8.2397e-04, -8.3923e-04],\n",
       "         ...,\n",
       "         [ 1.4801e-03,  1.9989e-03, -2.0294e-03,  ..., -1.5945e-03,\n",
       "           1.3885e-03,  1.0071e-03],\n",
       "         [ 1.1902e-03,  1.7624e-03, -1.5335e-03,  ..., -5.8413e-05,\n",
       "           7.2861e-04,  6.9046e-04],\n",
       "         [-9.1934e-04,  5.4359e-05, -1.2493e-04,  ...,  9.6130e-04,\n",
       "          -6.3705e-04, -1.5163e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.5095e-03, -2.2125e-03, -1.2894e-03,  ..., -6.2561e-04,\n",
       "          -1.1444e-03,  1.8616e-03],\n",
       "         [ 3.2501e-03, -1.0529e-03, -3.0823e-03,  ...,  8.7738e-04,\n",
       "           1.4496e-03,  6.5994e-04],\n",
       "         [-2.8992e-03,  6.5994e-04,  2.8992e-03,  ..., -1.1520e-03,\n",
       "          -1.9455e-03, -1.5182e-03],\n",
       "         ...,\n",
       "         [-2.0905e-03,  5.3167e-05,  1.6174e-03,  ..., -2.8534e-03,\n",
       "          -1.3123e-03, -4.8065e-04],\n",
       "         [ 3.7994e-03, -1.9684e-03, -1.0223e-03,  ..., -9.5367e-04,\n",
       "          -5.6076e-04,  1.6327e-03],\n",
       "         [ 3.4637e-03, -1.8845e-03, -1.4191e-03,  ..., -5.4550e-04,\n",
       "          -6.2180e-04,  1.0757e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 8.3542e-04,  1.8692e-03, -1.5564e-03,  ..., -9.6512e-04,\n",
       "          -1.4496e-03,  8.3160e-04],\n",
       "         [-3.9674e-07, -6.6376e-04, -9.9659e-05,  ...,  3.5286e-04,\n",
       "           1.5545e-04, -7.0572e-04],\n",
       "         [-8.8501e-04, -8.6212e-04,  1.6022e-03,  ...,  1.5564e-03,\n",
       "           1.4496e-03, -1.1063e-03],\n",
       "         ...,\n",
       "         [ 1.1597e-03,  1.4038e-03, -1.2970e-03,  ..., -1.7014e-03,\n",
       "          -1.4877e-03,  1.1902e-03],\n",
       "         [ 1.0777e-04,  7.0190e-04, -2.9373e-04,  ..., -1.1292e-03,\n",
       "          -9.4986e-04,  6.4850e-04],\n",
       "         [-1.1139e-03,  5.3024e-04, -6.1798e-04,  ..., -9.1553e-04,\n",
       "          -4.9591e-04,  3.3379e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-6.9809e-04, -2.6131e-04,  4.6349e-04,  ..., -7.0190e-04,\n",
       "          -5.0354e-04, -7.8678e-05],\n",
       "         [-1.6403e-03,  8.2779e-04,  3.7766e-04,  ...,  4.9829e-05,\n",
       "           3.1090e-04, -1.2875e-04],\n",
       "         [ 2.5635e-03, -9.4223e-04, -1.4496e-03,  ...,  8.2397e-04,\n",
       "          -8.8882e-04,  6.7520e-04],\n",
       "         ...,\n",
       "         [ 2.1057e-03, -7.8964e-04, -8.0490e-04,  ...,  2.1219e-05,\n",
       "          -6.1798e-04,  5.0354e-04],\n",
       "         [ 1.7166e-03, -8.3923e-04, -5.7983e-04,  ..., -1.7548e-04,\n",
       "          -7.5150e-04,  4.3297e-04],\n",
       "         [-2.0142e-03,  8.5068e-04,  7.2861e-04,  ...,  1.0109e-04,\n",
       "           3.8147e-04, -5.5695e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.2589e-04, -6.9046e-04,  7.5150e-04,  ..., -6.0797e-06,\n",
       "          -2.3484e-05,  6.5994e-04],\n",
       "         [-1.0300e-03,  5.7983e-04,  4.1389e-04,  ...,  6.2943e-04,\n",
       "          -1.4877e-03, -1.2779e-04],\n",
       "         [ 1.6499e-04,  8.7357e-04,  5.1117e-04,  ..., -2.8419e-04,\n",
       "          -7.3433e-05, -2.9564e-04],\n",
       "         ...,\n",
       "         [-5.8711e-06, -1.2054e-03,  3.4332e-05,  ...,  1.0920e-04,\n",
       "           3.4904e-04,  1.8387e-03],\n",
       "         [-5.6076e-04, -2.4986e-04, -1.5030e-03,  ...,  4.0627e-04,\n",
       "          -9.8419e-04,  1.2131e-03],\n",
       "         [ 8.3160e-04,  1.0605e-03,  1.3046e-03,  ..., -1.1110e-04,\n",
       "           1.5564e-03, -1.4801e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.5182e-03,  2.2125e-03,  7.7820e-04,  ...,  2.4872e-03,\n",
       "          -2.5482e-03, -2.9182e-04],\n",
       "         [ 4.6921e-04,  2.1057e-03, -9.7656e-04,  ...,  8.7738e-04,\n",
       "          -5.5695e-04, -4.6349e-04],\n",
       "         [ 5.4169e-04,  2.1667e-03, -7.3624e-04,  ..., -1.8005e-03,\n",
       "          -4.9591e-04,  1.2207e-03],\n",
       "         ...,\n",
       "         [-5.9509e-04,  6.3324e-04, -1.8921e-03,  ..., -2.1820e-03,\n",
       "           1.3657e-03,  1.9989e-03],\n",
       "         [ 1.4725e-03,  1.1826e-03,  5.5432e-06,  ...,  2.3956e-03,\n",
       "          -6.9809e-04, -4.0436e-04],\n",
       "         [-2.7084e-04, -2.1515e-03,  1.6251e-03,  ..., -1.8082e-03,\n",
       "           4.5395e-04,  1.7738e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-9.8419e-04,  9.5367e-04, -3.7193e-05,  ...,  2.0504e-04,\n",
       "           4.1771e-04,  2.7084e-04],\n",
       "         [ 2.9564e-04, -1.5488e-03,  1.5163e-04,  ..., -5.7817e-06,\n",
       "           3.6430e-04, -2.2984e-04],\n",
       "         [-1.0586e-04,  3.4094e-05,  5.3406e-04,  ...,  1.4591e-04,\n",
       "           4.2152e-04,  7.8583e-04],\n",
       "         ...,\n",
       "         [ 1.6479e-03, -2.5787e-03, -1.3123e-03,  ..., -1.7242e-03,\n",
       "          -1.8311e-03, -1.8463e-03],\n",
       "         [-1.3275e-03,  1.7700e-03,  1.9531e-03,  ...,  1.8234e-03,\n",
       "           1.0910e-03,  1.2970e-03],\n",
       "         [ 4.0283e-03, -3.6469e-03, -2.1515e-03,  ..., -2.6550e-03,\n",
       "          -2.2278e-03, -1.2207e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.7842e-03, -2.7618e-03, -2.0313e-04,  ..., -3.1281e-04,\n",
       "          -1.8997e-03,  3.0670e-03],\n",
       "         [-2.5482e-03,  1.3733e-03,  7.9727e-04,  ..., -1.0910e-03,\n",
       "          -5.2643e-04, -2.8534e-03],\n",
       "         [-2.6245e-03,  1.6479e-03, -3.2425e-04,  ...,  2.6321e-04,\n",
       "           2.2430e-03, -2.5940e-03],\n",
       "         ...,\n",
       "         [-2.2888e-03,  1.7548e-03, -4.3678e-04,  ...,  2.5558e-04,\n",
       "           1.5488e-03, -2.4872e-03],\n",
       "         [-2.5635e-03,  1.5869e-03, -8.7261e-05,  ...,  2.6703e-04,\n",
       "           1.3657e-03, -2.2278e-03],\n",
       "         [-2.2888e-03,  1.6556e-03, -5.1880e-04,  ...,  2.8992e-04,\n",
       "           2.0752e-03, -2.4719e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-6.1798e-04,  4.9591e-04, -3.5286e-04,  ..., -6.8665e-04,\n",
       "           7.4387e-04, -3.4904e-04],\n",
       "         [-4.7684e-04,  2.9373e-04,  1.1978e-03,  ...,  1.7834e-04,\n",
       "          -3.3951e-04,  3.6621e-04],\n",
       "         [ 6.0272e-04, -1.4191e-03, -6.1417e-04,  ...,  6.0272e-04,\n",
       "          -1.1368e-03, -1.6327e-03],\n",
       "         ...,\n",
       "         [ 8.0872e-04, -5.5695e-04, -1.1368e-03,  ...,  9.2316e-04,\n",
       "          -9.8419e-04, -9.2697e-04],\n",
       "         [ 2.6093e-03, -2.0905e-03,  9.1553e-05,  ...,  3.0823e-03,\n",
       "          -3.0365e-03, -3.3417e-03],\n",
       "         [-1.0071e-03,  1.8120e-04,  1.0834e-03,  ..., -1.0147e-03,\n",
       "           7.2098e-04,  1.2112e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.3351e-03, -4.3869e-04, -2.1973e-03,  ..., -1.8692e-04,\n",
       "          -5.2261e-04, -1.1158e-04],\n",
       "         [-1.2207e-03,  4.5013e-04,  1.6479e-03,  ...,  3.3379e-04,\n",
       "           4.7112e-04,  5.4550e-04],\n",
       "         [-3.2616e-04,  9.3460e-05,  9.1553e-04,  ...,  4.2725e-04,\n",
       "           7.3242e-04,  1.3504e-03],\n",
       "         ...,\n",
       "         [ 1.0529e-03, -2.7275e-04, -1.0376e-03,  ..., -2.2125e-04,\n",
       "          -4.2725e-04, -5.3406e-04],\n",
       "         [-1.2360e-03,  3.3379e-04,  2.0447e-03,  ...,  7.3433e-05,\n",
       "           2.3842e-04,  6.8665e-04],\n",
       "         [-8.8882e-04,  4.5776e-04,  4.8828e-04,  ...,  2.2793e-04,\n",
       "           3.5095e-04,  8.8692e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.6403e-04, -2.1458e-04, -4.0245e-04,  ..., -4.1723e-06,\n",
       "           4.2677e-05,  3.2806e-04],\n",
       "         [-2.9945e-04,  8.9645e-04,  5.1498e-04,  ...,  6.6757e-04,\n",
       "          -5.0735e-04,  8.5449e-04],\n",
       "         [ 3.1471e-04,  1.5736e-05,  2.5177e-04,  ...,  5.8365e-04,\n",
       "          -1.2684e-04,  3.6430e-04],\n",
       "         ...,\n",
       "         [-8.1635e-04, -1.1027e-05,  4.2534e-04,  ...,  3.1853e-04,\n",
       "          -2.9182e-04,  9.1171e-04],\n",
       "         [-5.7220e-04,  1.8597e-04,  6.4850e-04,  ..., -2.3842e-04,\n",
       "          -8.9645e-04,  1.0147e-03],\n",
       "         [-2.6107e-05, -3.0327e-04,  5.1498e-04,  ..., -3.2234e-04,\n",
       "          -1.2283e-03,  4.2152e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.7956e-04,  1.2283e-03, -2.4796e-04,  ...,  5.0354e-04,\n",
       "          -4.7302e-04, -1.9741e-04],\n",
       "         [-1.5259e-03, -7.7820e-04,  4.4107e-05,  ..., -7.5340e-05,\n",
       "          -9.7275e-05,  3.2806e-04],\n",
       "         [ 8.5449e-04, -2.8992e-04,  9.5367e-04,  ..., -1.6022e-04,\n",
       "          -5.3787e-04,  3.6240e-04],\n",
       "         ...,\n",
       "         [ 4.1580e-04, -3.9291e-04,  4.2725e-04,  ..., -5.6839e-04,\n",
       "           1.4496e-04,  4.1008e-04],\n",
       "         [ 3.5477e-04,  1.8005e-03,  6.7234e-05,  ...,  2.9945e-04,\n",
       "           1.4019e-04, -5.0783e-05],\n",
       "         [-2.0123e-04, -1.6174e-03, -3.8338e-04,  ...,  3.3569e-04,\n",
       "           9.2506e-05, -2.7466e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.3885e-03, -4.6921e-04,  3.6621e-04,  ..., -8.2016e-04,\n",
       "          -2.5558e-04,  5.0354e-04],\n",
       "         [ 2.3956e-03,  2.4872e-03, -2.9297e-03,  ...,  2.6855e-03,\n",
       "          -2.4567e-03, -2.9755e-03],\n",
       "         [ 1.7395e-03, -1.9836e-03,  1.4648e-03,  ..., -2.0142e-03,\n",
       "           2.1667e-03,  2.5787e-03],\n",
       "         ...,\n",
       "         [-3.8452e-03,  1.5335e-03, -8.4305e-04,  ...,  1.4572e-03,\n",
       "          -1.1063e-03, -1.1215e-03],\n",
       "         [-1.2131e-03,  1.4114e-03, -3.9673e-04,  ...,  4.9829e-05,\n",
       "          -6.6376e-04, -7.1716e-04],\n",
       "         [ 2.2984e-04, -1.2360e-03,  1.3580e-03,  ..., -1.0223e-03,\n",
       "           1.2894e-03,  2.4261e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-9.1934e-04, -2.4128e-04, -1.4267e-03,  ...,  2.2507e-04,\n",
       "          -4.1504e-03, -2.6703e-03],\n",
       "         [ 7.7057e-04, -8.4305e-04, -2.0447e-03,  ..., -9.0408e-04,\n",
       "          -5.9891e-04,  2.0447e-03],\n",
       "         [-8.2397e-04,  5.9128e-04,  1.0529e-03,  ...,  7.8964e-04,\n",
       "          -9.9182e-04, -2.6550e-03],\n",
       "         ...,\n",
       "         [ 7.9727e-04, -7.4387e-04, -1.7242e-03,  ..., -8.8501e-04,\n",
       "           3.7575e-04,  1.9989e-03],\n",
       "         [-7.5912e-04,  7.2861e-04,  7.3624e-04,  ...,  6.9809e-04,\n",
       "          -9.1076e-05, -1.5030e-03],\n",
       "         [-5.2261e-04,  1.1215e-03,  1.3580e-03,  ...,  7.3242e-04,\n",
       "           1.7319e-03, -3.0365e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.5564e-03,  1.0376e-03,  2.1057e-03,  ..., -1.1063e-03,\n",
       "           1.1063e-03,  1.3962e-03],\n",
       "         [ 9.1934e-04,  4.7493e-04,  6.4468e-04,  ..., -2.8610e-04,\n",
       "           8.5449e-04,  7.1716e-04],\n",
       "         [-2.1935e-04,  1.4591e-04,  8.1539e-05,  ...,  2.7657e-04,\n",
       "          -4.1580e-04,  4.7207e-05],\n",
       "         ...,\n",
       "         [-4.5967e-04, -7.2861e-04, -8.5831e-04,  ...,  1.0452e-03,\n",
       "          -8.5831e-04, -9.9945e-04],\n",
       "         [ 1.7166e-03,  1.0223e-03,  1.5411e-03,  ..., -1.3199e-03,\n",
       "           1.0223e-03,  1.4877e-03],\n",
       "         [ 3.7575e-04,  1.4484e-05, -5.6839e-04,  ..., -4.9829e-05,\n",
       "           2.1172e-04, -2.8992e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0005,  0.0005, -0.0008,  ...,  0.0005,  0.0010, -0.0003],\n",
       "         [-0.0005,  0.0005, -0.0006,  ...,  0.0005,  0.0015,  0.0003],\n",
       "         [-0.0003,  0.0003, -0.0003,  ...,  0.0003,  0.0017, -0.0009],\n",
       "         ...,\n",
       "         [ 0.0005, -0.0006,  0.0010,  ..., -0.0005, -0.0010, -0.0011],\n",
       "         [-0.0006,  0.0006, -0.0011,  ...,  0.0005,  0.0016,  0.0006],\n",
       "         [-0.0004,  0.0004, -0.0007,  ...,  0.0004,  0.0018, -0.0005]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.3733e-04, -1.0967e-04,  4.3154e-05,  ..., -7.6294e-04,\n",
       "          -4.0054e-04, -3.7766e-04],\n",
       "         [ 7.8964e-04,  5.0354e-04, -1.2436e-03,  ...,  5.5695e-04,\n",
       "           5.3787e-04, -7.0190e-04],\n",
       "         [-2.0790e-04, -1.0452e-03, -2.3603e-05,  ...,  2.5368e-04,\n",
       "           3.3569e-04,  1.4591e-04],\n",
       "         ...,\n",
       "         [ 4.1962e-04, -5.8413e-06, -1.3542e-04,  ...,  5.4550e-04,\n",
       "           8.8882e-04, -9.9945e-04],\n",
       "         [-2.8419e-04, -6.2180e-04, -1.8883e-04,  ...,  9.6130e-04,\n",
       "           2.4796e-04, -8.3160e-04],\n",
       "         [-3.8147e-06,  1.3046e-03, -7.3624e-04,  ...,  2.1267e-04,\n",
       "           4.7112e-04, -3.0899e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.4676e-05, -7.9727e-04, -5.6076e-04,  ..., -4.8256e-04,\n",
       "           1.8082e-03, -5.9509e-04],\n",
       "         [ 2.9755e-04, -7.8964e-04, -6.4087e-04,  ..., -8.7738e-05,\n",
       "          -3.9673e-04, -4.3869e-04],\n",
       "         [-4.0054e-05,  7.6675e-04,  4.9114e-05,  ...,  3.8338e-04,\n",
       "          -1.5945e-03,  4.7112e-04],\n",
       "         ...,\n",
       "         [ 4.9210e-04, -9.7656e-04,  5.3406e-05,  ..., -6.2180e-04,\n",
       "           1.2131e-03, -5.3787e-04],\n",
       "         [ 2.0599e-04, -6.9809e-04, -7.6675e-04,  ..., -3.9673e-04,\n",
       "           3.5477e-04, -4.3678e-04],\n",
       "         [-4.9210e-04,  5.4932e-04,  6.8665e-04,  ...,  5.1737e-05,\n",
       "          -1.1206e-04,  5.7602e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 7.1335e-04, -1.4801e-03,  6.4087e-04,  ..., -1.1139e-03,\n",
       "          -9.6130e-04,  5.1880e-04],\n",
       "         [-1.5564e-03,  3.8338e-04, -1.7624e-03,  ...,  1.3580e-03,\n",
       "           1.6785e-03, -1.9684e-03],\n",
       "         [ 3.2806e-03,  3.4180e-03,  3.2501e-03,  ..., -3.3112e-03,\n",
       "          -3.0823e-03,  4.5166e-03],\n",
       "         ...,\n",
       "         [-8.8882e-04, -9.0790e-04, -4.3297e-04,  ...,  3.4809e-05,\n",
       "           4.3678e-04, -7.4387e-04],\n",
       "         [ 2.4414e-03, -2.4605e-04,  2.8687e-03,  ..., -2.6093e-03,\n",
       "          -3.0212e-03,  2.5940e-03],\n",
       "         [-2.6550e-03, -8.6784e-05, -2.9449e-03,  ...,  2.7313e-03,\n",
       "           3.0212e-03, -2.4719e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-7.5912e-04,  8.2016e-04,  4.1389e-04,  ...,  4.3869e-04,\n",
       "           1.4954e-03, -3.7079e-03],\n",
       "         [ 8.1635e-04, -6.7711e-05, -7.5531e-04,  ..., -7.7438e-04,\n",
       "           3.9978e-03, -1.0300e-03],\n",
       "         [-7.7438e-04,  1.0223e-03, -6.5994e-04,  ...,  2.8992e-04,\n",
       "           2.6703e-03, -3.9062e-03],\n",
       "         ...,\n",
       "         [ 9.3460e-04, -8.7357e-04,  6.4850e-04,  ..., -2.1076e-04,\n",
       "          -2.7161e-03,  4.6997e-03],\n",
       "         [ 8.1635e-04, -8.7357e-04, -9.9659e-05,  ..., -4.3106e-04,\n",
       "          -2.3041e-03,  4.4250e-03],\n",
       "         [-7.8583e-04,  9.4223e-04, -2.6131e-04,  ...,  4.4441e-04,\n",
       "           1.9379e-03, -4.4556e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-7.2002e-05,  7.1716e-04, -3.5858e-04,  ...,  6.7139e-04,\n",
       "           1.1215e-03, -5.3406e-04],\n",
       "         [ 7.5912e-04,  1.6098e-03, -1.3733e-03,  ...,  9.8419e-04,\n",
       "           8.5449e-04, -6.2180e-04],\n",
       "         [-7.2956e-05, -1.5945e-03,  1.4572e-03,  ..., -1.0757e-03,\n",
       "          -1.8005e-03,  1.4954e-03],\n",
       "         ...,\n",
       "         [ 2.3746e-04,  2.9564e-04, -7.7724e-05,  ..., -1.2779e-04,\n",
       "          -1.1396e-04,  2.0385e-05],\n",
       "         [-1.4343e-03, -1.0681e-03,  1.0681e-03,  ..., -1.1826e-03,\n",
       "           3.0899e-04, -1.7548e-04],\n",
       "         [ 4.8447e-04,  2.3460e-04,  4.9210e-04,  ..., -7.3910e-05,\n",
       "           9.4414e-05, -6.9046e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0006, -0.0009, -0.0008,  ..., -0.0003,  0.0007,  0.0002],\n",
       "         [ 0.0006, -0.0003,  0.0003,  ..., -0.0001,  0.0002,  0.0005],\n",
       "         [-0.0007,  0.0004,  0.0004,  ...,  0.0004, -0.0007, -0.0006],\n",
       "         ...,\n",
       "         [ 0.0006, -0.0004, -0.0008,  ..., -0.0003,  0.0005,  0.0019],\n",
       "         [ 0.0004, -0.0002, -0.0001,  ..., -0.0003, -0.0002,  0.0014],\n",
       "         [-0.0006,  0.0002,  0.0004,  ...,  0.0004,  0.0003, -0.0019]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.9673e-04, -2.3556e-04, -4.4060e-04,  ..., -7.1335e-04,\n",
       "          -2.1553e-04,  1.2994e-05],\n",
       "         [ 2.9182e-04,  4.9591e-04,  8.5831e-04,  ...,  6.3324e-04,\n",
       "          -1.1015e-04,  5.0735e-04],\n",
       "         [ 2.4261e-03,  9.2316e-04,  1.1520e-03,  ..., -2.4414e-03,\n",
       "           5.2261e-04,  1.4420e-03],\n",
       "         ...,\n",
       "         [ 1.8692e-03,  1.2894e-03,  1.7548e-03,  ..., -7.4005e-04,\n",
       "           1.1826e-03,  9.2697e-04],\n",
       "         [ 1.6479e-03,  1.3657e-03,  1.5488e-03,  ..., -2.0599e-04,\n",
       "           8.6212e-04,  1.4114e-03],\n",
       "         [ 2.8419e-04,  6.9427e-04,  4.2725e-04,  ...,  5.9128e-05,\n",
       "          -2.3651e-04,  2.5940e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 5.5313e-04,  2.7084e-04, -1.4343e-03,  ..., -2.8419e-04,\n",
       "          -8.5449e-04,  4.4250e-04],\n",
       "         [ 8.0109e-04, -8.9407e-06, -2.5024e-03,  ..., -8.6975e-04,\n",
       "          -4.2152e-04,  3.0899e-04],\n",
       "         [ 2.3460e-04,  1.5080e-05, -1.3275e-03,  ..., -5.3024e-04,\n",
       "          -2.0123e-04,  1.6403e-04],\n",
       "         ...,\n",
       "         [-5.9509e-04, -4.3678e-04, -2.7084e-04,  ...,  1.3428e-03,\n",
       "           8.5831e-04,  2.2411e-05],\n",
       "         [ 4.4250e-04,  8.1062e-05, -1.2894e-03,  ..., -6.1035e-04,\n",
       "           1.9789e-05,  3.7575e-04],\n",
       "         [ 5.9509e-04,  3.0899e-04, -1.6098e-03,  ...,  2.2411e-04,\n",
       "           2.5368e-04,  4.3297e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.7874e-04, -6.3705e-04, -1.4973e-04,  ..., -9.6130e-04,\n",
       "          -7.9632e-05,  7.0953e-04],\n",
       "         [-4.7112e-04, -5.0354e-04, -2.0504e-04,  ..., -4.8256e-04,\n",
       "           5.8889e-05,  5.1117e-04],\n",
       "         [-2.6703e-04, -6.0272e-04, -1.0452e-03,  ...,  9.0122e-05,\n",
       "           1.8005e-03,  1.3809e-03],\n",
       "         ...,\n",
       "         [ 9.1553e-05, -8.1539e-05,  5.4550e-04,  ..., -7.2420e-06,\n",
       "           1.2779e-04,  1.0986e-03],\n",
       "         [ 4.3640e-03,  4.1809e-03,  3.8452e-03,  ...,  3.4943e-03,\n",
       "          -3.7079e-03, -3.9368e-03],\n",
       "         [ 1.3809e-03,  4.2725e-04,  1.6632e-03,  ...,  1.0757e-03,\n",
       "          -6.0272e-04, -5.9509e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0007,  0.0004, -0.0029,  ..., -0.0008,  0.0030,  0.0020],\n",
       "         [-0.0006,  0.0006, -0.0025,  ..., -0.0005,  0.0025,  0.0019],\n",
       "         [-0.0007,  0.0005, -0.0031,  ..., -0.0018,  0.0038,  0.0023],\n",
       "         ...,\n",
       "         [-0.0005,  0.0002, -0.0029,  ..., -0.0017,  0.0035,  0.0023],\n",
       "         [ 0.0008, -0.0002,  0.0032,  ...,  0.0017, -0.0035, -0.0018],\n",
       "         [ 0.0006, -0.0005,  0.0023,  ...,  0.0011, -0.0030, -0.0017]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.2221e-04,  4.0817e-04,  2.8968e-05,  ...,  3.6430e-04,\n",
       "           9.9659e-05,  9.4414e-05],\n",
       "         [ 1.0529e-03, -3.5095e-04, -5.9128e-04,  ...,  7.2861e-04,\n",
       "           6.5994e-04, -6.1035e-04],\n",
       "         [ 6.1035e-04, -4.3392e-05,  4.8828e-04,  ..., -7.7820e-04,\n",
       "          -9.0027e-04,  1.2894e-03],\n",
       "         ...,\n",
       "         [-2.3804e-03,  1.4114e-03,  5.4550e-04,  ...,  2.7924e-03,\n",
       "           2.4719e-03,  2.9297e-03],\n",
       "         [ 4.4060e-04, -9.9945e-04, -8.3923e-05,  ..., -6.7234e-05,\n",
       "          -1.3962e-03, -7.5150e-04],\n",
       "         [-1.3275e-03,  1.9932e-04, -1.5855e-05,  ...,  2.3842e-04,\n",
       "           2.8038e-04,  3.0899e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.8539e-03, -6.7234e-05,  9.8419e-04,  ...,  1.6785e-04,\n",
       "          -1.1597e-03, -1.3123e-03],\n",
       "         [-9.5367e-04,  4.7302e-04, -9.3079e-04,  ...,  4.4632e-04,\n",
       "           1.8597e-04,  1.4191e-03],\n",
       "         [-1.0376e-03,  3.4523e-04, -1.4019e-04,  ...,  6.6757e-04,\n",
       "          -3.0136e-04,  1.0376e-03],\n",
       "         ...,\n",
       "         [ 7.2861e-04, -5.6839e-04,  1.1301e-04,  ..., -5.9128e-04,\n",
       "           5.1880e-04, -8.2016e-04],\n",
       "         [-1.4400e-04, -6.2561e-04, -2.0905e-03,  ..., -1.0605e-03,\n",
       "           1.0910e-03,  1.5564e-03],\n",
       "         [-8.8501e-04,  5.3787e-04, -1.2131e-03,  ...,  5.3787e-04,\n",
       "           7.3242e-04,  1.5793e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.0452e-03, -2.5749e-05,  3.0899e-04,  ...,  1.1110e-04,\n",
       "           1.4591e-04,  5.4169e-04],\n",
       "         [ 7.7438e-04,  9.4223e-04,  1.2817e-03,  ...,  4.1962e-04,\n",
       "          -8.3923e-04,  3.0136e-04],\n",
       "         [ 1.0223e-03, -7.1526e-05,  9.9945e-04,  ...,  2.6321e-04,\n",
       "          -5.8746e-04,  9.1553e-04],\n",
       "         ...,\n",
       "         [ 1.3275e-03,  1.0605e-03,  1.0452e-03,  ...,  1.4191e-03,\n",
       "          -1.0605e-03,  6.4850e-04],\n",
       "         [ 8.2016e-04,  8.9264e-04,  8.6975e-04,  ...,  8.8501e-04,\n",
       "          -7.5912e-04,  1.0223e-03],\n",
       "         [ 1.8215e-04,  6.8665e-04,  4.2534e-04,  ...,  4.2725e-04,\n",
       "          -1.4973e-04,  5.7220e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.4523e-04, -1.1444e-03, -2.2736e-03,  ..., -8.2779e-04,\n",
       "           7.4005e-04,  1.8311e-03],\n",
       "         [-4.6492e-05, -6.8665e-04, -2.6550e-03,  ..., -1.8616e-03,\n",
       "           5.3787e-04,  1.7395e-03],\n",
       "         [ 1.3828e-04, -8.2016e-04, -1.6479e-03,  ..., -1.2665e-03,\n",
       "           1.2360e-03,  1.0376e-03],\n",
       "         ...,\n",
       "         [ 2.7466e-04, -1.0452e-03, -1.6479e-03,  ..., -2.2583e-03,\n",
       "           3.0899e-04,  1.6708e-03],\n",
       "         [-1.1730e-04,  8.2779e-04,  2.2888e-03,  ...,  2.4567e-03,\n",
       "          -7.1716e-04, -1.6022e-03],\n",
       "         [ 2.9182e-04, -8.9264e-04, -2.3499e-03,  ..., -2.3937e-04,\n",
       "           1.6327e-03,  1.4343e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.0529e-03, -4.1771e-04, -7.8583e-04,  ...,  1.1597e-03,\n",
       "          -8.8215e-05,  1.1597e-03],\n",
       "         [-1.5831e-04,  7.4387e-04, -3.0518e-05,  ...,  1.0147e-03,\n",
       "           4.6921e-04,  4.9591e-04],\n",
       "         [-3.5095e-04,  3.5706e-03, -2.4567e-03,  ...,  1.7166e-03,\n",
       "           2.9755e-03,  1.1139e-03],\n",
       "         ...,\n",
       "         [ 2.8372e-05, -1.8692e-04,  1.5616e-05,  ..., -5.9509e-04,\n",
       "          -6.7139e-04, -3.7384e-04],\n",
       "         [-8.5449e-04,  1.8387e-03, -1.8158e-03,  ...,  1.2131e-03,\n",
       "           1.8311e-03,  1.9226e-03],\n",
       "         [ 1.9302e-03, -3.1281e-03,  2.8534e-03,  ..., -2.6855e-03,\n",
       "          -2.5940e-03, -2.8076e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 8.7261e-05, -5.7983e-04,  1.0986e-03,  ..., -4.8065e-04,\n",
       "          -7.5531e-04,  1.0967e-04],\n",
       "         [ 2.7008e-03,  7.4768e-04, -2.9144e-03,  ..., -2.4719e-03,\n",
       "           4.8828e-03, -1.3123e-03],\n",
       "         [-1.3428e-03, -6.6376e-04,  1.8387e-03,  ..., -3.3617e-05,\n",
       "          -3.0518e-03,  8.1253e-04],\n",
       "         ...,\n",
       "         [ 6.3324e-04,  6.6376e-04, -2.0447e-03,  ..., -6.1512e-05,\n",
       "           2.9144e-03, -8.0490e-04],\n",
       "         [ 2.4719e-03, -4.9210e-04, -6.4850e-04,  ..., -1.5411e-03,\n",
       "           2.7771e-03, -5.4836e-05],\n",
       "         [ 7.7724e-05,  6.2180e-04, -1.0376e-03,  ...,  3.1471e-04,\n",
       "           1.8311e-03, -7.4387e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 6.2943e-04,  2.8038e-04,  4.2534e-04,  ...,  2.7275e-04,\n",
       "           5.7602e-04, -4.1962e-04],\n",
       "         [-1.4114e-04, -1.4114e-04,  4.9973e-04,  ..., -3.5858e-04,\n",
       "          -9.8348e-06, -3.7193e-05],\n",
       "         [ 3.4142e-04, -3.5477e-04,  5.9891e-04,  ..., -5.2261e-04,\n",
       "          -1.6594e-04, -1.7738e-04],\n",
       "         ...,\n",
       "         [ 1.6451e-05,  5.5313e-04, -1.5259e-04,  ...,  9.4223e-04,\n",
       "           5.0735e-04, -3.6812e-04],\n",
       "         [-3.9482e-04,  4.8447e-04, -7.3242e-04,  ...,  2.7466e-04,\n",
       "          -1.5259e-04,  4.7684e-04],\n",
       "         [-3.0518e-04, -7.2098e-04,  3.3379e-04,  ..., -3.2997e-04,\n",
       "           3.0899e-04,  3.8910e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.3657e-03, -5.5695e-04,  1.0376e-03,  ..., -7.8583e-04,\n",
       "          -1.4572e-03, -4.3869e-05],\n",
       "         [-1.9302e-03,  4.0245e-04, -4.7302e-04,  ...,  9.2697e-04,\n",
       "          -3.3379e-04,  1.1597e-03],\n",
       "         [ 2.0905e-03, -4.5967e-04,  4.3869e-04,  ..., -1.0910e-03,\n",
       "          -1.7357e-04, -1.0529e-03],\n",
       "         ...,\n",
       "         [-1.1673e-03,  5.9509e-04,  2.9802e-05,  ...,  8.3542e-04,\n",
       "          -1.3809e-03,  1.9150e-03],\n",
       "         [ 1.8234e-03, -4.6349e-04,  4.4060e-04,  ..., -8.9645e-04,\n",
       "           4.6492e-05, -9.0408e-04],\n",
       "         [-1.6556e-03,  5.7983e-04, -8.5449e-04,  ...,  8.4305e-04,\n",
       "           1.5182e-03,  2.7657e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-4.0770e-05, -4.8637e-04,  2.3556e-04,  ...,  2.1696e-05,\n",
       "          -6.1035e-04, -7.1335e-04],\n",
       "         [-6.0654e-04, -3.9101e-04,  7.0572e-05,  ..., -1.8120e-04,\n",
       "           3.1853e-04, -6.9439e-06],\n",
       "         [ 1.1301e-04, -1.1063e-04,  2.6321e-04,  ...,  2.3079e-04,\n",
       "          -1.0605e-03,  6.6376e-04],\n",
       "         ...,\n",
       "         [-1.3733e-04,  2.3270e-04, -3.5477e-04,  ..., -6.7520e-04,\n",
       "           4.9210e-04,  1.9073e-04],\n",
       "         [-5.3024e-04,  1.0910e-03, -7.9346e-04,  ..., -1.0223e-03,\n",
       "           1.2131e-03, -1.1749e-03],\n",
       "         [ 5.9509e-04, -1.9360e-04,  1.6499e-04,  ...,  6.2561e-04,\n",
       "           2.9182e-04,  6.4850e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.6894e-04,  1.3428e-03, -4.0054e-04,  ..., -2.1935e-04,\n",
       "          -1.1597e-03, -6.8283e-04],\n",
       "         [ 1.9073e-04,  3.8528e-04,  5.8746e-04,  ...,  4.9591e-04,\n",
       "           7.8964e-04,  6.2561e-04],\n",
       "         [ 2.2054e-05,  1.3733e-03, -1.9550e-04,  ..., -2.4986e-04,\n",
       "          -9.2697e-04, -5.9128e-04],\n",
       "         ...,\n",
       "         [ 4.0054e-04,  9.9945e-04, -4.2152e-04,  ..., -4.3297e-04,\n",
       "          -4.9973e-04, -4.3678e-04],\n",
       "         [ 6.9618e-05,  8.2397e-04,  5.7602e-04,  ...,  4.0436e-04,\n",
       "          -6.6376e-04,  5.2643e-04],\n",
       "         [ 2.0027e-04,  1.2817e-03, -3.7956e-04,  ..., -2.8038e-04,\n",
       "          -1.0300e-03, -1.0300e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0008,  0.0015,  0.0008,  ..., -0.0017, -0.0016, -0.0017],\n",
       "         [-0.0012, -0.0007,  0.0007,  ..., -0.0002,  0.0007,  0.0001],\n",
       "         [ 0.0036,  0.0030,  0.0030,  ..., -0.0034, -0.0035, -0.0032],\n",
       "         ...,\n",
       "         [-0.0001,  0.0006, -0.0007,  ..., -0.0003, -0.0012, -0.0001],\n",
       "         [-0.0007, -0.0010, -0.0011,  ...,  0.0007,  0.0003,  0.0012],\n",
       "         [-0.0012, -0.0013, -0.0003,  ...,  0.0015,  0.0011,  0.0012]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.3746e-04,  3.3760e-04, -2.2430e-03,  ..., -1.3657e-03,\n",
       "           1.2589e-03,  2.4719e-03],\n",
       "         [-9.1076e-05,  1.5163e-04, -2.8076e-03,  ..., -1.8997e-03,\n",
       "           1.6632e-03,  1.9073e-03],\n",
       "         [ 2.0905e-03, -1.0910e-03, -3.5400e-03,  ..., -3.8910e-03,\n",
       "           3.1738e-03,  1.5793e-03],\n",
       "         ...,\n",
       "         [-3.2997e-04, -7.6294e-05,  3.6469e-03,  ...,  3.6469e-03,\n",
       "          -3.2196e-03, -2.7008e-03],\n",
       "         [ 1.3447e-04, -3.9291e-04,  3.0365e-03,  ...,  1.9455e-03,\n",
       "          -1.6098e-03, -2.2125e-03],\n",
       "         [-1.0529e-03, -4.4632e-04,  2.8534e-03,  ...,  1.6937e-03,\n",
       "          -2.0142e-03, -2.9755e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.2970e-04,  1.3809e-03,  5.7983e-04,  ...,  5.6839e-04,\n",
       "           7.8201e-04,  1.1368e-03],\n",
       "         [ 1.0071e-03, -1.8883e-04, -9.6893e-04,  ..., -9.9945e-04,\n",
       "          -5.7983e-04, -4.6158e-04],\n",
       "         [ 9.8419e-04, -1.2970e-03, -2.0752e-03,  ..., -1.1063e-03,\n",
       "          -7.5912e-04, -1.3199e-03],\n",
       "         ...,\n",
       "         [-3.7766e-04,  2.6894e-04,  1.0757e-03,  ...,  3.3569e-04,\n",
       "           5.2261e-04, -4.5013e-04],\n",
       "         [-3.0327e-04,  8.5449e-04,  7.8964e-04,  ...,  1.2360e-03,\n",
       "           9.1934e-04,  9.1934e-04],\n",
       "         [-8.0490e-04, -5.3167e-05,  1.3733e-03,  ...,  1.3733e-04,\n",
       "           3.6240e-04,  4.1008e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.2207e-03, -5.1117e-04, -1.6785e-03,  ..., -1.3962e-03,\n",
       "           1.4572e-03,  6.6757e-04],\n",
       "         [ 1.9073e-03, -5.3024e-04, -6.5613e-04,  ..., -7.8583e-04,\n",
       "           5.6839e-04, -7.7820e-04],\n",
       "         [-3.2997e-04, -6.2561e-04,  6.7520e-04,  ...,  1.6022e-03,\n",
       "          -1.4038e-03, -1.3351e-03],\n",
       "         ...,\n",
       "         [ 8.8120e-04, -4.1580e-04, -1.1158e-04,  ..., -4.1485e-05,\n",
       "          -4.2677e-05, -1.9836e-04],\n",
       "         [ 1.7319e-03, -6.1417e-04, -5.4169e-04,  ..., -8.7357e-04,\n",
       "           7.8964e-04, -7.6675e-04],\n",
       "         [ 7.2002e-05, -3.9482e-04,  3.6049e-04,  ..., -4.5967e-04,\n",
       "          -2.8133e-05, -5.7220e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 5.1498e-04, -4.9210e-04, -3.8719e-04,  ..., -9.2697e-04,\n",
       "          -1.0834e-03,  2.5368e-04],\n",
       "         [-1.3733e-04, -1.5488e-03, -4.6921e-04,  ...,  2.9182e-04,\n",
       "          -3.9291e-04,  5.7983e-04],\n",
       "         [-1.3828e-04, -4.8876e-05,  2.3651e-04,  ...,  2.6894e-04,\n",
       "          -1.1749e-03,  5.0735e-04],\n",
       "         ...,\n",
       "         [-5.9509e-04, -5.3787e-04, -4.5300e-05,  ..., -7.9155e-05,\n",
       "           9.3460e-04, -2.4223e-04],\n",
       "         [ 1.9531e-03, -1.2436e-03, -9.9945e-04,  ..., -1.3809e-03,\n",
       "          -9.3842e-04, -9.8419e-04],\n",
       "         [ 4.1389e-04, -7.6294e-04, -2.3365e-04,  ..., -4.9973e-04,\n",
       "           6.1035e-04, -1.6117e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.2234e-04,  6.9618e-05,  4.2534e-04,  ..., -5.3787e-04,\n",
       "          -3.7766e-04,  1.2970e-03],\n",
       "         [-4.8256e-04,  1.0452e-03, -1.1587e-04,  ...,  3.2425e-04,\n",
       "           5.3787e-04, -4.7112e-04],\n",
       "         [-3.9673e-04,  6.5613e-04, -6.3324e-04,  ...,  3.0899e-04,\n",
       "           4.8637e-04, -9.0027e-04],\n",
       "         ...,\n",
       "         [-3.0327e-04,  1.3256e-04,  4.8637e-04,  ...,  5.9891e-04,\n",
       "           4.6349e-04, -9.3079e-04],\n",
       "         [ 6.7139e-04,  1.0490e-04,  1.2512e-03,  ...,  7.2861e-04,\n",
       "          -2.1696e-05, -1.2398e-04],\n",
       "         [-1.8001e-05, -8.3160e-04, -5.5313e-05,  ...,  5.0354e-04,\n",
       "           6.0272e-04, -1.3123e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-4.9210e-04,  1.3580e-03, -1.0300e-03,  ...,  5.4550e-04,\n",
       "          -1.4572e-03,  3.4332e-05],\n",
       "         [ 6.5231e-04, -5.0735e-04, -9.0599e-05,  ..., -2.4033e-04,\n",
       "           5.6839e-04, -1.2131e-03],\n",
       "         [-1.2665e-03,  1.8158e-03, -1.7624e-03,  ...,  2.4414e-03,\n",
       "          -1.9684e-03,  1.5106e-03],\n",
       "         ...,\n",
       "         [ 1.6098e-03, -2.0294e-03,  2.0905e-03,  ..., -2.3956e-03,\n",
       "           2.0142e-03, -1.0986e-03],\n",
       "         [ 8.2970e-05, -4.4823e-04,  4.0770e-05,  ..., -2.7275e-04,\n",
       "          -4.1389e-04, -2.5177e-04],\n",
       "         [ 7.8583e-04, -1.1015e-04,  1.6308e-04,  ...,  2.2125e-04,\n",
       "          -5.8365e-04, -1.0910e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0024,  0.0008,  0.0015,  ..., -0.0022,  0.0038, -0.0018],\n",
       "         [ 0.0022, -0.0011, -0.0004,  ...,  0.0022, -0.0042,  0.0009],\n",
       "         [-0.0024,  0.0010,  0.0008,  ..., -0.0030,  0.0047, -0.0015],\n",
       "         ...,\n",
       "         [ 0.0017,  0.0005, -0.0017,  ...,  0.0037, -0.0050,  0.0042],\n",
       "         [-0.0017,  0.0010,  0.0008,  ..., -0.0015,  0.0041, -0.0008],\n",
       "         [ 0.0029, -0.0003, -0.0013,  ...,  0.0042, -0.0042,  0.0022]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.1444e-03, -1.5259e-03,  1.2512e-03,  ..., -1.2817e-03,\n",
       "          -5.6839e-04, -2.1553e-04],\n",
       "         [-8.5831e-05,  3.2043e-04,  2.6512e-04,  ..., -1.1349e-04,\n",
       "           4.6539e-04, -1.2817e-03],\n",
       "         [-1.4496e-03,  1.1520e-03, -1.8692e-03,  ...,  2.3651e-03,\n",
       "           1.7471e-03,  3.0518e-04],\n",
       "         ...,\n",
       "         [-2.0599e-03,  9.9182e-04, -2.0294e-03,  ...,  1.8616e-03,\n",
       "           1.5335e-03, -1.2283e-03],\n",
       "         [-6.6757e-04, -2.8801e-04,  5.2261e-04,  ..., -3.4904e-04,\n",
       "          -6.6757e-04,  1.2589e-03],\n",
       "         [-1.4973e-04, -1.7881e-05,  1.0757e-03,  ..., -7.2479e-04,\n",
       "          -5.8365e-04,  5.1880e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.2817e-03,  2.8419e-04,  1.7548e-03,  ..., -6.3324e-04,\n",
       "           1.9302e-03,  3.3569e-04],\n",
       "         [ 1.2894e-03, -6.0558e-05, -2.0752e-03,  ...,  5.9509e-04,\n",
       "          -1.8845e-03, -2.5177e-04],\n",
       "         [-1.2817e-03, -4.6968e-05,  1.7090e-03,  ..., -8.7738e-04,\n",
       "           5.9128e-04,  2.0695e-04],\n",
       "         ...,\n",
       "         [ 1.4648e-03,  2.9244e-07, -2.7313e-03,  ...,  1.3809e-03,\n",
       "          -2.0447e-03, -5.9605e-05],\n",
       "         [ 1.0071e-03, -3.4714e-04, -1.7471e-03,  ...,  4.2725e-04,\n",
       "          -6.1798e-04, -1.2493e-04],\n",
       "         [-8.4686e-04,  1.1597e-03, -1.0223e-03,  ...,  1.5640e-03,\n",
       "          -1.6785e-03,  1.2131e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.1292e-03, -4.4823e-04,  4.7874e-04,  ...,  6.4087e-04,\n",
       "           1.1215e-03,  6.4468e-04],\n",
       "         [-2.2583e-03, -1.2283e-03,  1.3199e-03,  ...,  1.2283e-03,\n",
       "          -2.1210e-03,  1.9836e-03],\n",
       "         [-1.1215e-03,  2.3365e-05,  1.3828e-04,  ...,  8.2016e-04,\n",
       "           1.9073e-04,  1.3046e-03],\n",
       "         ...,\n",
       "         [-8.2850e-06,  4.4823e-04, -3.5095e-04,  ..., -5.1117e-04,\n",
       "           6.1798e-04,  4.0531e-05],\n",
       "         [-8.7261e-05,  6.3705e-04, -7.8678e-05,  ...,  7.0095e-05,\n",
       "           5.1117e-04,  4.0245e-04],\n",
       "         [-5.7459e-05,  3.9482e-04, -8.8120e-04,  ...,  1.8001e-05,\n",
       "           6.4087e-04, -1.2100e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.0910e-03,  2.8610e-04,  4.3106e-04,  ..., -3.3569e-04,\n",
       "           8.7738e-04, -8.7738e-04],\n",
       "         [ 1.6861e-03, -5.2643e-04, -9.5367e-04,  ..., -4.3869e-05,\n",
       "           1.7471e-03, -8.0109e-04],\n",
       "         [ 3.2187e-05,  7.0572e-04,  9.9182e-04,  ...,  3.6240e-04,\n",
       "          -2.4414e-03,  5.8365e-04],\n",
       "         ...,\n",
       "         [-9.9182e-04,  7.4005e-04,  6.3705e-04,  ...,  4.7302e-04,\n",
       "          -1.0452e-03,  6.5994e-04],\n",
       "         [ 1.4877e-03, -2.7657e-04, -1.4114e-04,  ..., -2.8014e-05,\n",
       "           1.4801e-03, -7.7820e-04],\n",
       "         [-1.6556e-03,  5.6839e-04,  3.2425e-04,  ...,  5.5730e-06,\n",
       "          -1.4954e-03,  6.7902e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.1662e-04, -2.1744e-04, -3.5667e-04,  ...,  4.5586e-04,\n",
       "          -1.2493e-04, -3.6240e-05],\n",
       "         [-3.9864e-04, -4.1199e-04, -2.8229e-04,  ...,  5.5313e-05,\n",
       "           3.2806e-04,  4.0436e-04],\n",
       "         [-2.0027e-04, -1.3199e-03, -6.7520e-04,  ...,  9.4986e-04,\n",
       "          -9.9659e-05,  1.5259e-03],\n",
       "         ...,\n",
       "         [-1.2665e-03, -4.7874e-04, -9.7275e-04,  ...,  7.7057e-04,\n",
       "          -9.0790e-04,  7.8583e-04],\n",
       "         [-7.6294e-04, -9.4986e-04, -8.6212e-04,  ...,  1.1292e-03,\n",
       "          -1.3428e-03,  8.6212e-04],\n",
       "         [-1.1063e-03,  1.9932e-04, -7.4387e-04,  ...,  2.8038e-04,\n",
       "          -4.1771e-04,  6.1035e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0008, -0.0003,  0.0013,  ...,  0.0009, -0.0044,  0.0027],\n",
       "         [ 0.0008, -0.0007,  0.0007,  ...,  0.0003, -0.0035,  0.0011],\n",
       "         [ 0.0006, -0.0007,  0.0019,  ...,  0.0007, -0.0045,  0.0021],\n",
       "         ...,\n",
       "         [-0.0006,  0.0005, -0.0007,  ..., -0.0008,  0.0038, -0.0018],\n",
       "         [ 0.0009, -0.0010,  0.0012,  ...,  0.0007, -0.0037,  0.0021],\n",
       "         [-0.0005,  0.0009, -0.0007,  ..., -0.0009,  0.0043, -0.0022]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-5.2261e-04,  4.5776e-04, -6.6757e-04,  ..., -1.1749e-03,\n",
       "           8.8882e-04,  1.1139e-03],\n",
       "         [-5.0735e-04, -8.0490e-04,  2.6131e-04,  ...,  2.2697e-04,\n",
       "          -1.2436e-03, -7.9727e-04],\n",
       "         [-6.3324e-04, -1.3199e-03,  9.0790e-04,  ...,  1.3504e-03,\n",
       "          -3.3569e-04, -1.3046e-03],\n",
       "         ...,\n",
       "         [-1.7014e-03, -2.6703e-04, -6.5994e-04,  ..., -2.1648e-04,\n",
       "           7.3624e-04, -5.4359e-05],\n",
       "         [-4.6158e-04,  2.9755e-04, -6.5231e-04,  ..., -4.0245e-04,\n",
       "           1.4954e-03,  1.1292e-03],\n",
       "         [ 3.9291e-04, -1.8597e-04, -2.1267e-04,  ...,  1.2589e-04,\n",
       "           1.7548e-04, -1.1349e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.1962e-04, -2.6941e-05, -3.4142e-04,  ..., -6.0272e-04,\n",
       "           1.4725e-03, -1.0910e-03],\n",
       "         [-5.2643e-04,  9.2316e-04,  5.9891e-04,  ..., -4.4703e-06,\n",
       "           2.1820e-03, -1.6594e-04],\n",
       "         [ 6.5994e-04,  3.6621e-04, -5.4550e-04,  ...,  7.5150e-04,\n",
       "          -2.6550e-03,  1.3123e-03],\n",
       "         ...,\n",
       "         [ 6.1798e-04, -2.9373e-04, -2.5177e-04,  ...,  3.6430e-04,\n",
       "          -2.5940e-03,  4.0436e-04],\n",
       "         [-5.5313e-04,  8.6975e-04,  1.8120e-04,  ...,  2.1076e-04,\n",
       "           2.3346e-03,  2.1362e-04],\n",
       "         [-7.9346e-04,  1.0757e-03,  5.3406e-04,  ..., -1.1396e-04,\n",
       "           2.5024e-03, -2.7776e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-4.6158e-04, -1.1015e-04, -2.2888e-04,  ..., -1.5926e-04,\n",
       "          -1.1444e-03, -3.6430e-04],\n",
       "         [-3.4904e-04, -6.2943e-04,  5.3406e-04,  ..., -1.3199e-03,\n",
       "           5.0735e-04,  7.2861e-04],\n",
       "         [ 1.0605e-03,  4.2200e-05, -4.1008e-05,  ...,  8.6308e-05,\n",
       "           1.1597e-03, -2.4605e-04],\n",
       "         ...,\n",
       "         [ 1.4954e-03,  7.7820e-04,  7.9632e-05,  ...,  3.9482e-04,\n",
       "           7.3624e-04, -2.3746e-04],\n",
       "         [-1.3504e-03, -3.1233e-05,  8.4686e-04,  ..., -6.0654e-04,\n",
       "          -7.7057e-04, -4.3869e-05],\n",
       "         [ 1.3199e-03,  4.3869e-04, -8.0872e-04,  ...,  8.8882e-04,\n",
       "           1.1139e-03, -2.5368e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.0757e-03,  3.1090e-04, -5.9509e-04,  ...,  1.0071e-03,\n",
       "          -7.7438e-04, -1.2054e-03],\n",
       "         [ 1.5869e-03,  4.5586e-04,  2.3651e-04,  ...,  1.0834e-03,\n",
       "          -6.3324e-04, -9.7275e-04],\n",
       "         [-1.8845e-03, -2.6107e-05,  2.1648e-04,  ..., -7.8964e-04,\n",
       "           8.7357e-04,  8.2397e-04],\n",
       "         ...,\n",
       "         [ 1.4191e-03,  5.7602e-04, -8.3160e-04,  ...,  1.3046e-03,\n",
       "          -5.7983e-04, -1.3275e-03],\n",
       "         [ 1.1444e-03,  1.0967e-04, -6.9427e-04,  ...,  1.1215e-03,\n",
       "          -7.4387e-05, -6.2180e-04],\n",
       "         [-1.6403e-03, -9.2030e-05,  1.6975e-04,  ..., -1.6174e-03,\n",
       "           1.6880e-04,  1.2283e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.8463e-03, -9.1553e-04,  2.0905e-03,  ..., -1.7242e-03,\n",
       "           7.7057e-04,  1.1520e-03],\n",
       "         [ 6.5231e-04,  5.7220e-04, -2.8038e-04,  ...,  8.5831e-04,\n",
       "           2.4605e-04,  2.7657e-05],\n",
       "         [-3.8147e-04,  9.3079e-04, -1.0681e-03,  ..., -2.2030e-04,\n",
       "          -8.1253e-04, -1.8883e-04],\n",
       "         ...,\n",
       "         [-1.8387e-03, -2.9907e-03,  1.9989e-03,  ..., -1.1826e-03,\n",
       "           2.1515e-03,  2.3499e-03],\n",
       "         [-3.4332e-04, -2.3174e-04, -1.5259e-04,  ..., -6.9809e-04,\n",
       "          -6.2943e-04,  1.6022e-04],\n",
       "         [ 5.5552e-05,  2.1057e-03, -1.5564e-03,  ...,  1.7929e-03,\n",
       "          -2.1973e-03, -1.4420e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.4954e-03,  9.2316e-04, -2.5482e-03,  ..., -3.0060e-03,\n",
       "           4.0283e-03, -3.4180e-03],\n",
       "         [-5.3787e-04,  1.5488e-03, -4.4632e-04,  ..., -3.3112e-03,\n",
       "           5.0354e-03, -6.5994e-04],\n",
       "         [-3.2997e-04, -1.5411e-03,  1.0223e-03,  ...,  3.4943e-03,\n",
       "          -4.5166e-03,  9.8419e-04],\n",
       "         ...,\n",
       "         [ 1.6937e-03, -2.4080e-05, -2.0313e-04,  ..., -2.4872e-03,\n",
       "           3.4180e-03, -2.6245e-03],\n",
       "         [-1.8387e-03, -1.6556e-03,  1.0986e-03,  ...,  4.8523e-03,\n",
       "          -4.4556e-03,  2.7466e-03],\n",
       "         [-6.5231e-04, -1.4114e-03,  1.4114e-03,  ...,  4.6997e-03,\n",
       "          -4.7302e-03,  8.5068e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-7.0572e-04, -9.6512e-04, -9.7656e-04,  ...,  9.7656e-04,\n",
       "           2.9182e-04, -1.4877e-03],\n",
       "         [-4.8637e-04,  8.9645e-04, -1.3447e-04,  ...,  2.0504e-04,\n",
       "          -3.4142e-04,  1.7357e-04],\n",
       "         [-7.2479e-05,  1.9550e-04, -7.9346e-04,  ..., -1.8120e-04,\n",
       "           6.3705e-04,  1.7929e-04],\n",
       "         ...,\n",
       "         [-8.2397e-04,  1.5488e-03,  1.1520e-03,  ..., -2.1820e-03,\n",
       "          -1.4877e-03,  1.8921e-03],\n",
       "         [-6.9618e-05, -3.0136e-04,  6.1798e-04,  ...,  4.7922e-05,\n",
       "          -4.5395e-04,  4.0054e-04],\n",
       "         [ 6.4468e-04, -1.5488e-03, -1.8997e-03,  ...,  1.3351e-03,\n",
       "           1.2283e-03, -1.4343e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.2741e-03,  1.1368e-03,  8.2016e-04,  ...,  1.1802e-05,\n",
       "          -1.0300e-03,  1.6174e-03],\n",
       "         [ 1.0605e-03,  9.7275e-04,  5.8365e-04,  ..., -1.4038e-03,\n",
       "           1.6937e-03, -8.5831e-04],\n",
       "         [-6.7902e-04,  9.3079e-04,  7.8964e-04,  ..., -1.1978e-03,\n",
       "           4.7684e-04,  7.0572e-04],\n",
       "         ...,\n",
       "         [-9.5367e-05, -1.1292e-03, -1.1139e-03,  ...,  1.9226e-03,\n",
       "          -1.0376e-03,  4.9472e-06],\n",
       "         [ 5.0354e-04, -8.2016e-04, -9.0027e-04,  ...,  8.8882e-04,\n",
       "          -1.0605e-03, -4.7493e-04],\n",
       "         [-2.5177e-04,  6.2561e-04,  7.5150e-04,  ..., -1.1292e-03,\n",
       "           1.5488e-03,  1.3542e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-9.8419e-04,  2.7847e-04, -4.3106e-04,  ...,  2.0218e-04,\n",
       "           3.5667e-04, -4.9973e-04],\n",
       "         [-1.4572e-03,  1.0986e-03, -6.1798e-04,  ..., -1.2512e-03,\n",
       "          -8.2397e-04,  3.0823e-03],\n",
       "         [ 4.2725e-04,  2.0027e-04,  1.2589e-04,  ...,  4.8447e-04,\n",
       "          -8.3160e-04,  1.2665e-03],\n",
       "         ...,\n",
       "         [ 7.2479e-04, -8.5068e-04,  4.6349e-04,  ..., -5.5134e-06,\n",
       "           6.2180e-04,  3.9673e-04],\n",
       "         [-4.9770e-06,  1.1978e-03,  2.6584e-05,  ...,  4.1389e-04,\n",
       "           1.9150e-03,  1.4591e-04],\n",
       "         [-5.5313e-04, -1.0395e-04,  5.9128e-04,  ..., -5.5313e-04,\n",
       "          -8.6975e-04,  2.0752e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-5.2643e-04,  8.3160e-04, -1.1978e-03,  ...,  2.8801e-04,\n",
       "           3.0327e-04, -1.9531e-03],\n",
       "         [ 2.0218e-04, -6.9809e-04,  9.9182e-04,  ...,  3.7575e-04,\n",
       "          -4.0054e-04,  1.8234e-03],\n",
       "         [ 4.5061e-05,  8.8882e-04, -1.6327e-03,  ...,  2.5368e-04,\n",
       "           5.0354e-04, -1.5793e-03],\n",
       "         ...,\n",
       "         [ 1.5736e-04,  9.2697e-04, -2.3556e-04,  ...,  7.0572e-04,\n",
       "           5.5695e-04, -2.0599e-03],\n",
       "         [-7.4768e-04, -2.6703e-04,  9.9945e-04,  ..., -1.4343e-03,\n",
       "          -6.5231e-04, -2.3460e-04],\n",
       "         [-4.4823e-04, -2.1458e-04, -1.0223e-03,  ...,  3.9101e-04,\n",
       "          -6.7711e-05,  9.3460e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.0376e-03,  1.2207e-03, -1.0376e-03,  ..., -8.7738e-04,\n",
       "           1.2589e-03, -6.2180e-04],\n",
       "         [ 1.0347e-04, -3.0708e-04,  5.2261e-04,  ...,  2.5034e-05,\n",
       "          -3.0327e-04,  5.2643e-04],\n",
       "         [ 7.5912e-04,  9.4604e-04, -5.5313e-04,  ...,  1.0872e-04,\n",
       "           3.9101e-04,  1.8501e-04],\n",
       "         ...,\n",
       "         [ 2.2278e-03, -1.7471e-03,  1.4877e-03,  ...,  1.2589e-03,\n",
       "          -3.0518e-04,  1.0452e-03],\n",
       "         [ 5.1880e-04,  1.0376e-03,  2.0695e-04,  ...,  1.2512e-03,\n",
       "           2.2030e-04,  7.3242e-04],\n",
       "         [-1.5259e-04,  9.3842e-04,  5.3406e-04,  ...,  3.4142e-04,\n",
       "          -3.0327e-04,  7.8583e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.5411e-03,  3.2349e-03, -2.1172e-04,  ..., -7.6675e-04,\n",
       "           3.6469e-03, -2.1362e-03],\n",
       "         [-7.4768e-04, -1.6403e-03,  1.7242e-03,  ...,  8.4686e-04,\n",
       "          -2.8839e-03,  1.3123e-03],\n",
       "         [ 7.5150e-04,  3.9673e-03, -3.3760e-04,  ..., -3.9101e-04,\n",
       "           3.7537e-03, -1.1444e-03],\n",
       "         ...,\n",
       "         [ 7.1716e-04,  3.3569e-03, -3.1471e-04,  ..., -5.3787e-04,\n",
       "           3.9978e-03, -1.5945e-03],\n",
       "         [-7.1335e-04, -3.3722e-03,  4.4441e-04,  ...,  3.7193e-04,\n",
       "          -3.7689e-03,  4.5395e-04],\n",
       "         [ 3.0518e-04,  4.4250e-03, -2.4319e-04,  ..., -6.7234e-05,\n",
       "           3.4485e-03, -4.2915e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-7.8964e-04, -8.9645e-04, -3.5095e-04,  ..., -2.4319e-04,\n",
       "           1.2207e-03,  8.7738e-04],\n",
       "         [-4.5776e-05, -6.4468e-04, -2.8992e-04,  ..., -4.0054e-04,\n",
       "           6.3705e-04, -1.8120e-04],\n",
       "         [-4.7874e-04, -3.9101e-04,  4.1771e-04,  ..., -2.1076e-04,\n",
       "           1.0757e-03,  9.6512e-04],\n",
       "         ...,\n",
       "         [-3.2425e-04, -3.5095e-04, -5.7602e-04,  ..., -1.6308e-04,\n",
       "           7.8583e-04,  7.7438e-04],\n",
       "         [ 5.9128e-04,  1.5106e-03,  7.9346e-04,  ...,  1.7319e-03,\n",
       "          -1.5259e-03, -7.5912e-04],\n",
       "         [ 2.2292e-05, -9.4223e-04, -1.1749e-03,  ..., -8.6594e-04,\n",
       "           1.1139e-03,  6.7520e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.3869e-04, -4.6539e-04, -3.6240e-04,  ..., -8.2970e-05,\n",
       "          -1.5793e-03, -6.5613e-04],\n",
       "         [ 3.2997e-04, -1.4572e-03, -2.6703e-04,  ...,  1.8311e-04,\n",
       "          -2.1515e-03, -4.4060e-04],\n",
       "         [-7.7820e-04, -1.5945e-03, -2.3365e-04,  ...,  8.5068e-04,\n",
       "          -2.1667e-03,  4.4250e-04],\n",
       "         ...,\n",
       "         [ 4.9210e-04, -4.1008e-05, -2.3746e-04,  ...,  6.9141e-05,\n",
       "          -1.6708e-03, -8.3542e-04],\n",
       "         [-7.3624e-04,  5.4932e-04,  3.8910e-04,  ...,  2.2411e-05,\n",
       "           1.5488e-03,  8.0872e-04],\n",
       "         [ 6.6757e-05,  1.2665e-03,  2.2602e-04,  ..., -2.8038e-04,\n",
       "           1.8539e-03, -7.6771e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-8.7738e-05,  5.3787e-04,  3.9864e-04,  ...,  3.9101e-04,\n",
       "           1.0443e-04, -1.0633e-04],\n",
       "         [-4.9591e-04,  6.0654e-04, -3.8338e-04,  ..., -7.8201e-04,\n",
       "           9.1171e-04, -1.6308e-04],\n",
       "         [ 9.3842e-04, -4.5204e-04,  4.4060e-04,  ...,  8.0109e-04,\n",
       "          -8.5831e-04, -6.5994e-04],\n",
       "         ...,\n",
       "         [ 1.2131e-03, -1.4400e-04,  1.0986e-03,  ...,  1.2131e-03,\n",
       "           2.5558e-04, -1.3924e-04],\n",
       "         [-6.3324e-04, -2.6822e-05, -3.8147e-04,  ..., -3.5858e-04,\n",
       "           1.9073e-05,  4.7112e-04],\n",
       "         [ 9.5367e-04,  2.1362e-04,  5.3024e-04,  ...,  5.3406e-04,\n",
       "          -9.1171e-04, -8.0109e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.1515e-03, -5.6458e-04, -5.1880e-04,  ...,  2.0218e-04,\n",
       "           6.7139e-04,  1.0071e-03],\n",
       "         [-1.4801e-03,  5.8365e-04, -2.9206e-05,  ..., -8.2016e-05,\n",
       "          -6.5613e-04, -2.0447e-03],\n",
       "         [-1.4648e-03, -5.1117e-04,  7.5817e-05,  ...,  3.2234e-04,\n",
       "           7.7820e-04,  1.1139e-03],\n",
       "         ...,\n",
       "         [-9.7656e-04, -5.9128e-04, -8.1062e-05,  ...,  2.6321e-04,\n",
       "           7.5150e-04,  2.3804e-03],\n",
       "         [ 1.0300e-03,  5.5695e-04, -3.3188e-04,  ..., -1.1027e-05,\n",
       "          -6.4850e-04, -2.5177e-03],\n",
       "         [ 3.7766e-04,  6.0272e-04,  2.0623e-05,  ..., -2.9564e-05,\n",
       "          -6.7520e-04, -1.4038e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-6.4850e-05, -4.2152e-04,  5.5313e-04,  ..., -2.1172e-04,\n",
       "           2.3556e-04, -3.6812e-04],\n",
       "         [ 2.5635e-03, -2.8229e-03, -3.0670e-03,  ..., -1.5335e-03,\n",
       "           3.0365e-03,  3.0670e-03],\n",
       "         [ 3.2806e-03, -3.2501e-03, -3.6926e-03,  ..., -2.3346e-03,\n",
       "           2.4719e-03,  3.5706e-03],\n",
       "         ...,\n",
       "         [ 6.3419e-05,  4.8447e-04,  3.5095e-04,  ..., -6.2561e-04,\n",
       "          -2.0294e-03, -3.7575e-04],\n",
       "         [-8.2779e-04,  6.5613e-04,  5.6839e-04,  ...,  2.5177e-04,\n",
       "          -2.2583e-03, -1.4648e-03],\n",
       "         [ 9.1553e-04, -8.0872e-04, -1.2283e-03,  ..., -1.3657e-03,\n",
       "           1.9531e-03,  5.0354e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 7.2956e-05,  2.4414e-03,  4.4823e-04,  ..., -6.5613e-04,\n",
       "           3.3569e-03, -8.3542e-04],\n",
       "         [-2.2888e-03, -2.8687e-03, -4.9210e-04,  ...,  1.0681e-03,\n",
       "          -2.8534e-03,  3.0518e-03],\n",
       "         [-1.3542e-04, -2.1973e-03, -2.0504e-04,  ...,  4.8065e-04,\n",
       "          -3.0212e-03,  1.1292e-03],\n",
       "         ...,\n",
       "         [-8.5831e-04, -2.0599e-03,  7.6294e-04,  ...,  1.9302e-03,\n",
       "          -3.8757e-03,  2.1820e-03],\n",
       "         [-3.9291e-04,  4.1962e-04,  5.0354e-04,  ...,  4.0054e-04,\n",
       "           7.4768e-04, -1.8883e-04],\n",
       "         [-1.0395e-04,  2.8534e-03, -9.0599e-05,  ..., -6.1035e-04,\n",
       "           3.1586e-03, -7.4768e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.6430e-04,  5.1498e-04,  1.4877e-04,  ..., -2.7275e-04,\n",
       "          -3.7003e-04, -1.0061e-04],\n",
       "         [ 1.1063e-04, -3.2043e-04, -1.3256e-04,  ..., -5.1117e-04,\n",
       "          -1.0347e-04,  8.2016e-04],\n",
       "         [-9.9182e-04,  2.0599e-03,  1.3733e-03,  ...,  1.2283e-03,\n",
       "          -1.2512e-03, -9.7275e-04],\n",
       "         ...,\n",
       "         [-1.5869e-03,  1.5335e-03,  1.4267e-03,  ...,  1.0834e-03,\n",
       "          -1.2894e-03, -1.6174e-03],\n",
       "         [ 1.4038e-03, -9.3842e-04, -8.0109e-04,  ..., -1.3809e-03,\n",
       "           9.3842e-04,  1.1826e-03],\n",
       "         [ 3.3951e-04, -3.5524e-05,  6.0272e-04,  ..., -3.5286e-04,\n",
       "           6.7711e-05, -4.1008e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.1090e-04, -1.0605e-03, -7.0190e-04,  ..., -4.8447e-04,\n",
       "          -8.0490e-04, -1.8501e-04],\n",
       "         [-1.4305e-04,  1.7395e-03,  7.0953e-04,  ...,  3.7956e-04,\n",
       "           1.0147e-03, -1.5736e-04],\n",
       "         [-2.8253e-05,  1.2436e-03,  9.0027e-04,  ...,  1.1444e-04,\n",
       "           5.9128e-04, -1.5640e-04],\n",
       "         ...,\n",
       "         [ 5.3406e-05,  1.3199e-03,  8.6594e-04,  ...,  1.1683e-04,\n",
       "           8.6594e-04, -4.1389e-04],\n",
       "         [ 2.8038e-04, -1.4038e-03, -8.3160e-04,  ..., -2.8610e-04,\n",
       "          -6.9046e-04,  4.6492e-05],\n",
       "         [ 1.2112e-04, -1.7090e-03, -7.7438e-04,  ..., -1.5736e-04,\n",
       "          -4.6158e-04, -1.3733e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.7738e-04, -1.0300e-03,  5.7220e-04,  ..., -7.4208e-06,\n",
       "          -2.8372e-05,  3.6812e-04],\n",
       "         [ 1.3733e-04,  8.1635e-04, -1.4954e-03,  ...,  1.4267e-03,\n",
       "           1.2875e-04,  9.1553e-05],\n",
       "         [ 8.8882e-04,  6.3324e-04, -1.0452e-03,  ...,  5.0735e-04,\n",
       "           6.1798e-04,  6.4087e-04],\n",
       "         ...,\n",
       "         [-9.6893e-04, -9.7275e-04,  7.0572e-04,  ..., -1.0986e-03,\n",
       "          -7.8678e-05, -2.6321e-04],\n",
       "         [-1.3504e-03, -8.7357e-04,  1.9646e-04,  ..., -1.2894e-03,\n",
       "          -6.8665e-04, -1.8082e-03],\n",
       "         [ 4.1008e-04,  1.0452e-03, -2.1362e-03,  ...,  6.9046e-04,\n",
       "           6.5613e-04,  1.1597e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0009,  0.0007,  0.0014,  ...,  0.0006,  0.0017,  0.0005],\n",
       "         [ 0.0014,  0.0008,  0.0005,  ...,  0.0004,  0.0012, -0.0002],\n",
       "         [-0.0006, -0.0012,  0.0012,  ..., -0.0009,  0.0010,  0.0004],\n",
       "         ...,\n",
       "         [ 0.0024,  0.0025, -0.0020,  ..., -0.0002, -0.0017, -0.0005],\n",
       "         [ 0.0008,  0.0007,  0.0008,  ..., -0.0009,  0.0016, -0.0001],\n",
       "         [ 0.0002,  0.0005,  0.0026,  ..., -0.0002,  0.0017,  0.0013]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.8311e-03,  1.4191e-03,  1.4191e-03,  ...,  9.0790e-04,\n",
       "          -1.4114e-03,  1.0986e-03],\n",
       "         [-1.4191e-03,  1.1597e-03,  1.0147e-03,  ...,  1.2589e-03,\n",
       "          -2.3041e-03,  1.9379e-03],\n",
       "         [-1.9169e-04,  5.4359e-05, -3.0518e-04,  ...,  2.8419e-04,\n",
       "           1.4305e-04,  4.1771e-04],\n",
       "         ...,\n",
       "         [-5.6458e-04, -2.6894e-04,  5.2643e-04,  ..., -7.9346e-04,\n",
       "          -9.9659e-05, -1.5163e-04],\n",
       "         [ 3.5248e-03, -3.2501e-03, -2.8229e-03,  ..., -3.8147e-03,\n",
       "           3.4790e-03, -3.4790e-03],\n",
       "         [ 2.6550e-03, -2.2888e-03, -3.2654e-03,  ..., -2.1057e-03,\n",
       "           2.5024e-03, -2.4261e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0009, -0.0014, -0.0005,  ...,  0.0033, -0.0024,  0.0031],\n",
       "         [-0.0010,  0.0028,  0.0001,  ..., -0.0029,  0.0036, -0.0030],\n",
       "         [-0.0011,  0.0015,  0.0006,  ..., -0.0020,  0.0025, -0.0023],\n",
       "         ...,\n",
       "         [-0.0009,  0.0026,  0.0007,  ..., -0.0042,  0.0030, -0.0030],\n",
       "         [ 0.0014, -0.0013, -0.0008,  ...,  0.0013, -0.0025,  0.0026],\n",
       "         [-0.0007,  0.0024,  0.0005,  ..., -0.0036,  0.0027, -0.0024]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.6327e-03,  2.4567e-03,  1.8768e-03,  ..., -2.3041e-03,\n",
       "           1.6251e-03,  3.0212e-03],\n",
       "         [-8.0872e-04,  2.8419e-04,  2.6894e-04,  ...,  6.9141e-06,\n",
       "           1.0910e-03,  8.0872e-04],\n",
       "         [ 2.8038e-04,  1.8311e-03,  3.5667e-04,  ..., -8.2779e-04,\n",
       "           3.3951e-04,  2.9945e-04],\n",
       "         ...,\n",
       "         [-1.0910e-03,  3.5667e-04,  1.8539e-03,  ..., -1.1444e-03,\n",
       "           1.5717e-03,  1.7624e-03],\n",
       "         [ 1.4210e-04,  9.5749e-04,  3.7384e-04,  ..., -4.6730e-04,\n",
       "           2.7084e-04,  2.9182e-04],\n",
       "         [-1.7853e-03,  1.5564e-03,  1.5182e-03,  ..., -2.0599e-03,\n",
       "           1.8463e-03,  1.7471e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-7.0190e-04,  6.6376e-04,  5.3024e-04,  ..., -2.1219e-05,\n",
       "           4.9591e-04, -1.1978e-03],\n",
       "         [ 2.1973e-03, -8.9264e-04, -1.0910e-03,  ...,  1.0633e-04,\n",
       "          -1.5793e-03,  1.4343e-03],\n",
       "         [ 6.1798e-04, -6.2180e-04, -6.8283e-04,  ...,  1.1597e-03,\n",
       "          -5.5790e-05,  8.3923e-04],\n",
       "         ...,\n",
       "         [-9.8419e-04,  1.2054e-03,  6.3705e-04,  ..., -1.4191e-03,\n",
       "           5.3787e-04, -8.3542e-04],\n",
       "         [ 3.4523e-04, -8.9169e-05, -3.5286e-04,  ...,  1.0071e-03,\n",
       "          -2.7061e-05,  9.3460e-05],\n",
       "         [ 1.3924e-04, -2.4109e-03, -1.0681e-03,  ...,  1.3428e-03,\n",
       "          -1.4191e-03,  1.4038e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.9210e-04,  7.7057e-04,  5.4169e-04,  ..., -6.1417e-04,\n",
       "          -9.0408e-04, -3.4142e-04],\n",
       "         [ 8.8120e-04, -1.5106e-03, -4.9973e-04,  ..., -1.0633e-04,\n",
       "          -7.6675e-04, -1.5640e-04],\n",
       "         [-4.0770e-05,  9.6893e-04,  4.0531e-05,  ...,  4.9591e-04,\n",
       "          -2.5749e-04, -5.1117e-04],\n",
       "         ...,\n",
       "         [ 9.0790e-04, -6.4468e-04, -4.1199e-04,  ..., -5.6076e-04,\n",
       "          -6.6376e-04, -4.4060e-04],\n",
       "         [ 1.0147e-03, -2.8419e-04, -1.4496e-03,  ...,  2.4605e-04,\n",
       "          -6.7902e-04,  2.4080e-05],\n",
       "         [ 3.0994e-05,  9.1934e-04,  1.4496e-03,  ...,  2.7657e-04,\n",
       "           6.7902e-04,  6.9046e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.2283e-03,  2.8992e-04, -5.4550e-04,  ..., -1.9169e-04,\n",
       "          -4.4632e-04, -6.2561e-04],\n",
       "         [-5.0545e-05, -1.6499e-04,  3.8528e-04,  ...,  5.3024e-04,\n",
       "           4.0627e-04,  1.6022e-03],\n",
       "         [ 5.3024e-04, -1.7929e-04, -1.1110e-04,  ...,  1.5926e-04,\n",
       "           3.3760e-04,  1.1368e-03],\n",
       "         ...,\n",
       "         [ 4.8637e-04, -7.5912e-04,  1.2779e-04,  ...,  6.1417e-04,\n",
       "           4.9210e-04,  1.3962e-03],\n",
       "         [ 4.4250e-04, -5.3406e-04,  9.6321e-05,  ...,  6.1798e-04,\n",
       "          -2.1100e-05,  1.3504e-03],\n",
       "         [ 1.7071e-04,  4.6349e-04,  1.8787e-04,  ..., -7.9346e-04,\n",
       "          -8.8120e-04, -8.5449e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 7.0572e-04, -1.3275e-03,  5.9128e-04,  ...,  1.2665e-03,\n",
       "           8.4686e-04, -7.5340e-05],\n",
       "         [ 7.6675e-04, -3.2654e-03, -3.2425e-05,  ...,  1.8158e-03,\n",
       "           8.0109e-04, -4.0627e-04],\n",
       "         [ 3.0670e-03,  4.7493e-04,  3.1433e-03,  ...,  2.6703e-03,\n",
       "           3.3112e-03, -2.9297e-03],\n",
       "         ...,\n",
       "         [ 4.0054e-04,  1.8463e-03,  9.9182e-04,  ...,  2.5177e-04,\n",
       "           6.4850e-04, -1.0376e-03],\n",
       "         [-1.9989e-03, -9.2983e-05, -2.3041e-03,  ..., -1.5869e-03,\n",
       "          -2.1362e-03,  1.8616e-03],\n",
       "         [ 1.1368e-03,  2.3956e-03,  5.1498e-04,  ...,  1.0605e-03,\n",
       "           1.6556e-03, -4.7493e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0013, -0.0017, -0.0011,  ..., -0.0013, -0.0028, -0.0009],\n",
       "         [-0.0014,  0.0007, -0.0015,  ..., -0.0028, -0.0030, -0.0003],\n",
       "         [ 0.0013, -0.0030, -0.0012,  ..., -0.0013, -0.0024,  0.0007],\n",
       "         ...,\n",
       "         [ 0.0008, -0.0028, -0.0005,  ..., -0.0013, -0.0025,  0.0001],\n",
       "         [ 0.0023, -0.0032, -0.0011,  ..., -0.0017, -0.0019, -0.0003],\n",
       "         [-0.0020,  0.0030,  0.0016,  ...,  0.0010,  0.0033,  0.0002]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.9684e-03,  1.9302e-03,  1.6708e-03,  ...,  2.3499e-03,\n",
       "          -2.4261e-03, -2.6855e-03],\n",
       "         [-2.6855e-03, -3.0975e-03, -3.5095e-04,  ..., -1.7776e-03,\n",
       "           2.0905e-03,  3.6774e-03],\n",
       "         [ 7.5912e-04,  1.0681e-03,  1.5106e-03,  ...,  1.8311e-04,\n",
       "          -1.0147e-03, -6.7139e-04],\n",
       "         ...,\n",
       "         [ 6.6376e-04,  5.2643e-04, -2.4605e-04,  ...,  6.8665e-04,\n",
       "          -9.2697e-04,  7.8201e-04],\n",
       "         [ 1.0681e-03,  1.9550e-04, -1.1444e-04,  ...,  8.5831e-04,\n",
       "          -2.8610e-04, -1.0834e-03],\n",
       "         [-6.5804e-05, -3.4332e-04,  4.9973e-04,  ..., -3.9291e-04,\n",
       "           3.6430e-04, -9.6798e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 9.4223e-04,  1.8005e-03,  4.6539e-04,  ...,  5.2643e-04,\n",
       "           5.6076e-04, -4.4060e-04],\n",
       "         [-8.4877e-05,  1.1826e-03,  7.0953e-04,  ..., -1.0347e-04,\n",
       "           2.9755e-04, -6.3324e-04],\n",
       "         [-1.0605e-03,  1.0147e-03,  8.5831e-05,  ..., -9.2697e-04,\n",
       "           1.8234e-03, -6.4850e-04],\n",
       "         ...,\n",
       "         [ 5.3406e-05,  2.2736e-03,  8.0109e-04,  ..., -4.6921e-04,\n",
       "          -3.4714e-04, -8.3160e-04],\n",
       "         [-4.6158e-04, -1.3657e-03, -4.8828e-04,  ...,  4.9591e-05,\n",
       "          -4.3488e-04,  8.3542e-04],\n",
       "         [-5.5313e-04, -2.3804e-03, -4.3297e-04,  ...,  6.7651e-06,\n",
       "          -1.2512e-03,  8.1253e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.6022e-03,  1.0834e-03,  7.6675e-04,  ..., -1.3199e-03,\n",
       "          -1.4648e-03, -8.7738e-04],\n",
       "         [-1.5640e-03,  7.9727e-04, -3.4094e-05,  ..., -9.9182e-04,\n",
       "          -7.8583e-04, -1.9684e-03],\n",
       "         [ 9.5367e-04, -9.7752e-05,  9.7275e-05,  ...,  3.5286e-04,\n",
       "           2.0695e-04,  7.9346e-04],\n",
       "         ...,\n",
       "         [-1.0376e-03, -1.9169e-04,  1.5354e-04,  ..., -1.0300e-03,\n",
       "          -1.0681e-03, -9.8419e-04],\n",
       "         [ 6.4373e-05, -7.3433e-05,  1.0910e-03,  ..., -8.1253e-04,\n",
       "          -6.8665e-04, -1.4725e-03],\n",
       "         [ 1.0910e-03, -1.8921e-03,  1.7643e-04,  ...,  8.2779e-04,\n",
       "          -1.8501e-04,  1.0910e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0006,  0.0005, -0.0007,  ...,  0.0004,  0.0024,  0.0010],\n",
       "         [-0.0015,  0.0002,  0.0002,  ..., -0.0001, -0.0023, -0.0003],\n",
       "         [-0.0010, -0.0006,  0.0008,  ..., -0.0006, -0.0018, -0.0011],\n",
       "         ...,\n",
       "         [ 0.0017,  0.0002, -0.0005,  ...,  0.0003,  0.0022,  0.0009],\n",
       "         [ 0.0002,  0.0008, -0.0006,  ...,  0.0004,  0.0013,  0.0014],\n",
       "         [-0.0001, -0.0006, -0.0004,  ..., -0.0007,  0.0018,  0.0013]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.2360e-03, -8.5831e-04,  4.3488e-04,  ...,  1.2894e-03,\n",
       "          -4.6349e-04,  5.8746e-04],\n",
       "         [ 1.9741e-04, -1.3924e-04, -1.6117e-04,  ...,  2.0599e-04,\n",
       "          -1.4210e-04, -1.0452e-03],\n",
       "         [-4.3297e-04, -7.5912e-04,  1.9360e-04,  ...,  9.3079e-04,\n",
       "          -1.0529e-03,  1.3580e-03],\n",
       "         ...,\n",
       "         [ 1.6022e-03,  1.4572e-03, -1.5640e-03,  ..., -2.1667e-03,\n",
       "           1.3580e-03, -1.5335e-03],\n",
       "         [ 4.1485e-05, -2.9182e-04,  2.5177e-04,  ...,  6.8665e-04,\n",
       "          -4.8256e-04,  8.2016e-04],\n",
       "         [ 1.0376e-03,  2.7275e-04, -1.0147e-03,  ..., -5.2643e-04,\n",
       "           9.5749e-04, -1.2436e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.1839e-04,  3.3722e-03, -1.2589e-03,  ...,  6.0654e-04,\n",
       "           2.3346e-03,  5.6839e-04],\n",
       "         [-1.1749e-03,  1.3657e-03, -1.7929e-04,  ...,  5.1498e-04,\n",
       "           2.0599e-03,  1.7548e-03],\n",
       "         [ 1.4572e-03, -1.5106e-03,  3.2234e-04,  ..., -7.3624e-04,\n",
       "          -1.9989e-03, -1.2131e-03],\n",
       "         ...,\n",
       "         [ 4.3678e-04, -2.0599e-03,  6.5231e-04,  ..., -3.2234e-04,\n",
       "          -1.8311e-03, -8.9264e-04],\n",
       "         [-1.3447e-04,  1.7853e-03, -8.4305e-04,  ...,  2.2602e-04,\n",
       "           1.8921e-03,  9.7656e-04],\n",
       "         [ 8.0490e-04, -3.2043e-03,  1.0452e-03,  ...,  7.4387e-05,\n",
       "          -2.7618e-03, -8.9264e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.5013e-04,  3.9482e-04, -2.2602e-04,  ..., -1.8215e-04,\n",
       "           7.3242e-04, -5.2261e-04],\n",
       "         [-3.7575e-04,  8.2397e-04,  1.6093e-05,  ...,  4.4060e-04,\n",
       "          -3.7384e-04, -2.8801e-04],\n",
       "         [ 1.4877e-04,  2.8610e-05,  1.4484e-05,  ...,  1.6212e-05,\n",
       "           5.1117e-04, -5.7220e-04],\n",
       "         ...,\n",
       "         [-4.6730e-04, -5.1498e-04,  2.5928e-06,  ...,  1.1146e-05,\n",
       "          -7.9632e-05,  8.0109e-05],\n",
       "         [-3.5477e-04,  7.6294e-05, -4.3106e-04,  ..., -8.8882e-04,\n",
       "          -2.0599e-04,  1.1597e-03],\n",
       "         [ 1.0872e-04, -9.8348e-06,  4.0436e-04,  ..., -3.5286e-04,\n",
       "          -2.0862e-05,  3.7384e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0022,  0.0002,  0.0012,  ..., -0.0001,  0.0013,  0.0009],\n",
       "         [ 0.0021,  0.0001,  0.0009,  ..., -0.0004,  0.0009,  0.0011],\n",
       "         [-0.0007, -0.0002, -0.0012,  ...,  0.0004, -0.0010, -0.0020],\n",
       "         ...,\n",
       "         [ 0.0016,  0.0004,  0.0007,  ..., -0.0006,  0.0011,  0.0005],\n",
       "         [ 0.0020,  0.0006,  0.0008,  ..., -0.0010,  0.0014,  0.0007],\n",
       "         [-0.0016, -0.0011, -0.0009,  ...,  0.0007, -0.0015, -0.0010]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.7319e-03, -1.6327e-03, -1.3504e-03,  ...,  1.3504e-03,\n",
       "          -1.6937e-03, -1.1063e-03],\n",
       "         [-1.9360e-04,  5.1880e-04, -1.5163e-04,  ...,  8.4305e-04,\n",
       "          -2.4128e-04,  9.2030e-05],\n",
       "         [-4.5776e-04,  5.6267e-05,  5.7220e-04,  ..., -1.0834e-03,\n",
       "           2.3956e-03,  4.0817e-04],\n",
       "         ...,\n",
       "         [ 9.4223e-04, -2.0027e-04,  5.4550e-04,  ..., -1.8597e-04,\n",
       "           7.0953e-04,  3.8910e-04],\n",
       "         [ 5.6839e-04,  2.1515e-03,  4.6349e-04,  ..., -5.1117e-04,\n",
       "           4.3869e-04,  2.6703e-04],\n",
       "         [ 2.9564e-04,  1.5106e-03,  6.4373e-05,  ..., -6.2180e-04,\n",
       "          -4.5776e-04,  1.6594e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.7466e-03,  5.2643e-04, -5.6458e-04,  ..., -6.7902e-04,\n",
       "          -2.1839e-04,  6.1417e-04],\n",
       "         [ 7.3624e-04, -7.1335e-04,  4.9210e-04,  ...,  1.3657e-03,\n",
       "           9.1934e-04, -5.0735e-04],\n",
       "         [-1.2360e-03,  4.7684e-04, -3.9864e-04,  ...,  5.9128e-04,\n",
       "           3.1471e-04,  5.5695e-04],\n",
       "         ...,\n",
       "         [ 1.0300e-03, -3.3760e-04,  5.4550e-04,  ...,  1.3161e-04,\n",
       "          -6.2180e-04, -5.3787e-04],\n",
       "         [ 1.0910e-03, -2.3842e-04, -1.8024e-04,  ..., -2.3937e-04,\n",
       "           1.5564e-03, -6.8665e-04],\n",
       "         [-1.4782e-04,  3.8719e-04, -5.7602e-04,  ...,  4.0293e-05,\n",
       "          -1.2040e-05,  5.3787e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.9836e-03,  5.3406e-04,  1.8616e-03,  ...,  1.4267e-03,\n",
       "           2.1210e-03, -1.6174e-03],\n",
       "         [-7.9346e-04,  1.5259e-03, -1.0834e-03,  ..., -1.2207e-03,\n",
       "          -7.3242e-04,  3.8719e-04],\n",
       "         [ 3.0136e-04, -3.7575e-04,  1.7452e-04,  ...,  8.1635e-04,\n",
       "           9.1171e-04, -1.6117e-04],\n",
       "         ...,\n",
       "         [-3.2187e-05,  3.1853e-04, -7.5531e-04,  ...,  1.0729e-04,\n",
       "          -2.1362e-04,  1.8883e-04],\n",
       "         [ 1.5354e-04,  8.3160e-04, -4.0436e-04,  ...,  2.2984e-04,\n",
       "           4.3488e-04, -9.1934e-04],\n",
       "         [ 8.4686e-04, -7.7820e-04,  7.5531e-04,  ...,  9.8419e-04,\n",
       "           1.1978e-03, -1.1139e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0024,  0.0003, -0.0028,  ...,  0.0019, -0.0005,  0.0031],\n",
       "         [ 0.0021,  0.0013,  0.0033,  ..., -0.0011,  0.0013, -0.0027],\n",
       "         [-0.0033, -0.0015, -0.0032,  ...,  0.0027, -0.0017,  0.0030],\n",
       "         ...,\n",
       "         [-0.0029,  0.0002, -0.0031,  ...,  0.0017, -0.0011,  0.0034],\n",
       "         [-0.0027,  0.0003, -0.0035,  ...,  0.0030, -0.0013,  0.0035],\n",
       "         [ 0.0032, -0.0004,  0.0030,  ..., -0.0014,  0.0005, -0.0030]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.0599e-03, -2.1667e-03, -2.4109e-03,  ..., -1.8539e-03,\n",
       "          -9.1934e-04,  1.3292e-05],\n",
       "         [ 1.3428e-03,  4.2915e-04,  9.9945e-04,  ...,  4.7302e-04,\n",
       "           7.0572e-04,  1.0681e-03],\n",
       "         [-1.2741e-03, -1.1215e-03, -1.1978e-03,  ..., -1.7776e-03,\n",
       "          -1.4420e-03, -7.2098e-04],\n",
       "         ...,\n",
       "         [ 9.5749e-04,  8.8882e-04,  6.5994e-04,  ...,  6.2180e-04,\n",
       "           4.4823e-04,  5.0735e-04],\n",
       "         [ 1.9550e-04,  2.8801e-04, -1.0610e-05,  ..., -2.0981e-05,\n",
       "           3.2187e-05,  2.3499e-03],\n",
       "         [ 7.8201e-04,  6.7902e-04,  5.3787e-04,  ...,  1.3046e-03,\n",
       "           1.1826e-03,  1.3962e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.5106e-03,  1.2016e-04, -1.5259e-03,  ...,  2.7771e-03,\n",
       "          -3.9339e-05,  5.8365e-04],\n",
       "         [-1.6937e-03,  1.0848e-05, -1.8158e-03,  ...,  2.8992e-03,\n",
       "          -7.1335e-04,  7.0190e-04],\n",
       "         [-1.3351e-03,  5.7602e-04, -1.4877e-03,  ...,  2.4414e-03,\n",
       "           1.2207e-04,  3.9482e-04],\n",
       "         ...,\n",
       "         [-1.7166e-03,  9.1195e-06, -1.5793e-03,  ...,  1.4725e-03,\n",
       "          -9.1553e-04,  1.2589e-03],\n",
       "         [-1.9989e-03,  2.6321e-04, -1.9150e-03,  ...,  1.4038e-03,\n",
       "           3.0136e-04,  1.0300e-03],\n",
       "         [ 4.5776e-04, -6.6376e-04, -7.1335e-04,  ...,  2.2583e-03,\n",
       "           1.2970e-03, -9.2316e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.4305e-04, -1.3733e-03,  1.2817e-03,  ..., -1.3580e-03,\n",
       "          -1.0605e-03, -4.4823e-04],\n",
       "         [ 3.2616e-04, -3.3951e-04,  5.7602e-04,  ..., -5.4550e-04,\n",
       "          -6.1798e-04, -3.7956e-04],\n",
       "         [ 1.5182e-03,  6.6757e-04, -7.1716e-04,  ...,  7.3624e-04,\n",
       "           7.9727e-04,  5.9891e-04],\n",
       "         ...,\n",
       "         [ 9.0027e-04,  9.1171e-04, -4.0245e-04,  ...,  5.4932e-04,\n",
       "           6.2561e-04,  3.4523e-04],\n",
       "         [-6.6757e-04,  5.6458e-04,  7.9346e-04,  ...,  4.1771e-04,\n",
       "           4.5061e-05,  1.8787e-04],\n",
       "         [-6.9809e-04,  1.3275e-03, -1.2016e-04,  ...,  2.2125e-04,\n",
       "           4.8256e-04,  3.2997e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.3188e-04, -1.0490e-05, -4.4632e-04,  ...,  2.9755e-03,\n",
       "           5.3406e-04,  1.3657e-03],\n",
       "         [ 5.4169e-04, -5.3024e-04, -8.0490e-04,  ...,  1.2970e-03,\n",
       "           8.1635e-04, -1.6708e-03],\n",
       "         [-9.9182e-04, -1.0757e-03,  7.8583e-04,  ..., -1.9455e-03,\n",
       "          -8.3542e-04,  1.0223e-03],\n",
       "         ...,\n",
       "         [ 4.6539e-04,  1.6880e-04, -1.4725e-03,  ...,  1.4191e-03,\n",
       "           8.7357e-04, -1.7929e-03],\n",
       "         [ 1.0223e-03,  2.6512e-04,  4.5204e-04,  ...,  1.5450e-04,\n",
       "           9.9945e-04, -3.9291e-04],\n",
       "         [ 9.4986e-04,  6.8283e-04, -2.2507e-04,  ...,  1.2589e-03,\n",
       "           9.5367e-04, -2.1362e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0014,  0.0007, -0.0021,  ..., -0.0009, -0.0005, -0.0022],\n",
       "         [ 0.0023, -0.0021,  0.0022,  ...,  0.0019,  0.0018,  0.0006],\n",
       "         [ 0.0015, -0.0024,  0.0014,  ...,  0.0015,  0.0015,  0.0010],\n",
       "         ...,\n",
       "         [-0.0005,  0.0005, -0.0008,  ..., -0.0012, -0.0006, -0.0018],\n",
       "         [-0.0008,  0.0011, -0.0008,  ..., -0.0015, -0.0015, -0.0008],\n",
       "         [ 0.0009, -0.0008,  0.0015,  ...,  0.0013,  0.0016,  0.0020]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-6.1035e-04, -5.0735e-04, -7.2861e-04,  ..., -1.4801e-03,\n",
       "          -2.1210e-03,  9.9945e-04],\n",
       "         [-2.7180e-05,  9.6130e-04,  5.0735e-04,  ...,  1.4725e-03,\n",
       "           2.8992e-03, -1.2512e-03],\n",
       "         [ 2.3460e-04, -1.1368e-03, -6.1417e-04,  ..., -1.6861e-03,\n",
       "          -2.6398e-03,  1.1749e-03],\n",
       "         ...,\n",
       "         [-4.3297e-04, -4.8256e-04, -6.1798e-04,  ..., -1.8768e-03,\n",
       "          -1.8158e-03,  2.3193e-03],\n",
       "         [-3.6812e-04, -1.5163e-04, -6.2180e-04,  ..., -1.4954e-03,\n",
       "          -1.6785e-03,  4.9973e-04],\n",
       "         [-7.6675e-04, -2.7924e-03, -5.4836e-05,  ..., -2.2430e-03,\n",
       "          -3.0060e-03,  3.4790e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.3346e-03, -2.6245e-03, -1.8234e-03,  ...,  1.9989e-03,\n",
       "           2.3041e-03, -2.1057e-03],\n",
       "         [-2.0447e-03,  1.6556e-03,  2.0142e-03,  ..., -1.5869e-03,\n",
       "          -1.2207e-03, -8.2016e-04],\n",
       "         [ 6.1798e-04, -8.9264e-04, -7.4387e-04,  ...,  1.2131e-03,\n",
       "           7.1716e-04,  4.7963e-08],\n",
       "         ...,\n",
       "         [ 3.3379e-04,  4.5776e-04,  9.9182e-04,  ..., -3.7766e-04,\n",
       "          -3.3379e-04, -3.5553e-03],\n",
       "         [-9.2316e-04,  8.5831e-04,  9.7656e-04,  ..., -5.2643e-04,\n",
       "          -6.2180e-04, -9.9945e-04],\n",
       "         [-1.6098e-03,  1.6327e-03,  1.4343e-03,  ..., -9.4986e-04,\n",
       "          -1.5717e-03, -2.9144e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.6928e-05,  1.8005e-03,  1.4305e-04,  ..., -1.6880e-04,\n",
       "           2.9144e-03, -1.2131e-03],\n",
       "         [ 9.0790e-04,  1.5020e-05, -1.1730e-04,  ..., -4.6921e-04,\n",
       "          -1.7776e-03, -3.8910e-04],\n",
       "         [-3.1662e-04, -3.1662e-04, -4.5967e-04,  ..., -6.4468e-04,\n",
       "          -1.6556e-03,  7.1526e-05],\n",
       "         ...,\n",
       "         [-1.5068e-04,  4.1962e-04,  4.4060e-04,  ...,  2.1839e-04,\n",
       "           1.9226e-03, -8.8692e-05],\n",
       "         [ 4.5395e-04,  2.1935e-04,  2.2697e-04,  ...,  5.2643e-04,\n",
       "           1.4114e-03,  3.9577e-05],\n",
       "         [-4.3030e-03, -1.7624e-03,  1.1139e-03,  ..., -8.1635e-04,\n",
       "           1.6174e-03,  5.1117e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.0147e-03, -9.0408e-04, -4.2534e-04,  ...,  2.1515e-03,\n",
       "           7.2002e-05,  4.7302e-04],\n",
       "         [ 8.8215e-05, -1.2894e-03, -9.2506e-05,  ...,  1.1978e-03,\n",
       "           1.1673e-03,  8.5068e-04],\n",
       "         [-1.6251e-03,  1.7853e-03,  1.9226e-03,  ..., -1.4572e-03,\n",
       "          -1.0452e-03, -3.4714e-04],\n",
       "         ...,\n",
       "         [-7.9727e-04,  1.7853e-03,  1.6022e-03,  ..., -1.4038e-03,\n",
       "          -3.3760e-04, -1.2512e-03],\n",
       "         [ 5.8365e-04, -4.1771e-04, -2.3651e-04,  ...,  3.1853e-04,\n",
       "           6.4468e-04,  8.4305e-04],\n",
       "         [-3.7766e-04, -1.8835e-05, -3.7575e-04,  ...,  3.2997e-04,\n",
       "           4.5586e-04,  4.5013e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 7.7820e-04,  1.0376e-03,  6.5231e-04,  ..., -2.2125e-04,\n",
       "          -1.7929e-04, -9.1934e-04],\n",
       "         [-6.2561e-04, -4.5776e-04, -5.7983e-04,  ...,  3.7956e-04,\n",
       "           3.8719e-04,  7.8201e-04],\n",
       "         [-3.7849e-06, -9.4223e-04, -1.0452e-03,  ...,  1.1902e-03,\n",
       "          -1.7452e-04,  8.5449e-04],\n",
       "         ...,\n",
       "         [ 7.3624e-04,  8.7738e-04,  1.6594e-04,  ..., -6.1035e-04,\n",
       "          -2.5368e-04, -2.2411e-04],\n",
       "         [-9.4604e-04,  2.9755e-04,  6.5613e-04,  ...,  2.5177e-04,\n",
       "           5.7220e-04,  1.1683e-04],\n",
       "         [ 8.2397e-04,  5.3406e-04,  7.7820e-04,  ..., -1.4114e-03,\n",
       "          -1.3256e-04, -5.7602e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-4.4060e-04,  6.7139e-04,  1.6022e-03,  ...,  9.4223e-04,\n",
       "          -9.5367e-04,  8.9645e-04],\n",
       "         [-1.4400e-04, -6.2943e-05, -3.0708e-04,  ..., -3.6621e-04,\n",
       "          -1.3065e-04,  9.2506e-05],\n",
       "         [-7.4768e-04,  4.9591e-04,  8.5831e-04,  ...,  8.2397e-04,\n",
       "           2.2125e-04,  6.0272e-04],\n",
       "         ...,\n",
       "         [-3.0060e-03,  3.0365e-03,  3.8452e-03,  ...,  3.2043e-03,\n",
       "          -3.3569e-03,  3.6163e-03],\n",
       "         [-2.5635e-03,  2.8992e-03,  1.8158e-03,  ...,  1.5564e-03,\n",
       "          -1.6251e-03,  1.4725e-03],\n",
       "         [ 1.9455e-04, -8.1539e-05, -1.5926e-04,  ...,  8.2970e-05,\n",
       "          -6.9809e-04,  6.3896e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-6.3324e-04, -6.5613e-04,  8.9645e-05,  ..., -2.5787e-03,\n",
       "          -2.4676e-05, -4.7684e-04],\n",
       "         [ 5.4550e-04,  1.0605e-03,  7.6294e-04,  ...,  2.8229e-03,\n",
       "           1.5831e-04,  8.5831e-04],\n",
       "         [ 4.8828e-04,  1.5564e-03,  5.2643e-04,  ...,  3.2959e-03,\n",
       "           3.2997e-04,  2.1362e-04],\n",
       "         ...,\n",
       "         [ 6.5231e-04,  5.8365e-04,  1.1444e-04,  ...,  2.5787e-03,\n",
       "          -1.4424e-05,  8.0490e-04],\n",
       "         [-5.5695e-04, -9.1934e-04, -6.1798e-04,  ..., -2.9755e-03,\n",
       "          -2.3079e-04, -5.7220e-04],\n",
       "         [ 2.5940e-04,  1.4267e-03,  4.3488e-04,  ...,  3.1281e-03,\n",
       "           3.5286e-04,  3.2234e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.0681e-03,  1.5335e-03,  8.3160e-04,  ...,  1.0681e-03,\n",
       "          -4.7874e-04, -1.0986e-03],\n",
       "         [-4.2021e-06, -7.3624e-04,  1.7834e-04,  ..., -1.2283e-03,\n",
       "           9.4986e-04,  6.1035e-04],\n",
       "         [ 6.3419e-05,  1.3275e-03,  6.2466e-05,  ...,  7.4005e-04,\n",
       "          -4.3297e-04, -1.3504e-03],\n",
       "         ...,\n",
       "         [ 1.9968e-06, -9.8705e-05,  5.3406e-04,  ..., -1.2696e-05,\n",
       "           3.1281e-04, -9.8419e-04],\n",
       "         [ 8.3160e-04,  1.3809e-03,  9.7275e-04,  ..., -3.9864e-04,\n",
       "           8.6212e-04,  7.2861e-04],\n",
       "         [-1.5068e-04, -3.3855e-05,  1.3733e-04,  ...,  5.9891e-04,\n",
       "          -1.4801e-03, -9.4604e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.0681e-03,  8.3160e-04, -3.8719e-04,  ...,  2.2736e-03,\n",
       "           2.9755e-04, -2.8229e-04],\n",
       "         [ 9.5749e-04,  1.3123e-03,  2.2697e-04,  ...,  2.8992e-03,\n",
       "           1.4725e-03, -1.8311e-03],\n",
       "         [ 1.0300e-03,  6.2943e-04,  1.8120e-04,  ...,  1.8005e-03,\n",
       "           2.1553e-04, -5.8651e-05],\n",
       "         ...,\n",
       "         [-2.1267e-04,  1.5259e-03,  2.2125e-03,  ...,  6.4850e-04,\n",
       "           9.7656e-04, -1.2894e-03],\n",
       "         [ 8.1635e-04, -1.5945e-03, -2.4261e-03,  ...,  6.1798e-04,\n",
       "          -2.3041e-03,  1.9684e-03],\n",
       "         [ 3.0518e-04, -2.4567e-03, -1.6403e-03,  ..., -5.6076e-04,\n",
       "          -2.8839e-03,  2.1057e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.7302e-04, -6.4087e-04, -1.1444e-03,  ...,  1.1368e-03,\n",
       "           4.3297e-04, -8.8120e-04],\n",
       "         [-7.1716e-04, -6.1417e-04, -1.0986e-03,  ...,  4.7684e-05,\n",
       "           1.5736e-04, -8.3542e-04],\n",
       "         [ 1.7548e-03,  1.7395e-03,  9.9945e-04,  ..., -1.0147e-03,\n",
       "          -9.0408e-04,  1.4038e-03],\n",
       "         ...,\n",
       "         [-3.0899e-04,  4.0627e-04,  6.1798e-04,  ...,  8.9169e-05,\n",
       "          -1.8692e-03,  7.7057e-04],\n",
       "         [-1.3256e-04,  7.9346e-04,  6.3324e-04,  ..., -1.6861e-03,\n",
       "           4.4584e-05,  5.0735e-04],\n",
       "         [ 6.0272e-04,  1.4191e-03,  2.1172e-04,  ...,  1.7548e-04,\n",
       "          -8.9645e-04,  2.9945e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.8910e-04,  1.1139e-03,  8.8882e-04,  ..., -1.4801e-03,\n",
       "          -4.6730e-05, -2.8229e-04],\n",
       "         [ 2.5558e-04,  1.0834e-03,  1.6327e-03,  ...,  3.5524e-05,\n",
       "          -2.6703e-04,  1.8787e-04],\n",
       "         [ 4.7112e-04,  9.7656e-04,  2.3499e-03,  ..., -1.7090e-03,\n",
       "           4.3106e-04, -2.2316e-04],\n",
       "         ...,\n",
       "         [-3.9291e-04, -9.0790e-04, -2.0905e-03,  ..., -1.3065e-04,\n",
       "          -1.2894e-03,  2.3365e-04],\n",
       "         [-3.1090e-04, -7.2098e-04, -3.7842e-03,  ...,  7.3242e-04,\n",
       "          -5.3787e-04, -1.1969e-04],\n",
       "         [ 3.7384e-04,  1.2436e-03,  1.6708e-03,  ..., -1.6479e-03,\n",
       "           4.1008e-04, -2.6894e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.2125e-03,  1.2512e-03, -1.3962e-03,  ..., -1.0452e-03,\n",
       "          -2.1820e-03, -1.2894e-03],\n",
       "         [ 1.3885e-03,  6.2943e-04,  7.7724e-05,  ..., -4.4250e-04,\n",
       "          -1.0071e-03, -1.9550e-04],\n",
       "         [-1.2360e-03, -9.7275e-04,  1.0452e-03,  ...,  6.1035e-04,\n",
       "           3.7766e-04,  7.0953e-04],\n",
       "         ...,\n",
       "         [ 2.4261e-03,  1.3885e-03, -1.6556e-03,  ..., -1.3809e-03,\n",
       "          -2.4872e-03, -1.2207e-03],\n",
       "         [-2.1210e-03, -8.5449e-04,  1.8692e-03,  ...,  1.7090e-03,\n",
       "           2.3041e-03,  1.3809e-03],\n",
       "         [-1.5869e-03, -2.5177e-03,  3.0518e-03,  ...,  3.3264e-03,\n",
       "           1.2283e-03,  2.8381e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0029, -0.0014,  0.0007,  ..., -0.0029, -0.0003, -0.0030],\n",
       "         [-0.0039, -0.0033, -0.0011,  ..., -0.0044, -0.0010, -0.0036],\n",
       "         [ 0.0049,  0.0033,  0.0013,  ...,  0.0045,  0.0007,  0.0029],\n",
       "         ...,\n",
       "         [ 0.0049,  0.0031,  0.0016,  ...,  0.0046,  0.0003,  0.0039],\n",
       "         [ 0.0033,  0.0009, -0.0007,  ...,  0.0020, -0.0002,  0.0021],\n",
       "         [ 0.0045,  0.0032,  0.0011,  ...,  0.0042,  0.0003,  0.0040]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-8.5068e-04,  2.0905e-03,  6.4468e-04,  ..., -1.7014e-03,\n",
       "          -1.7624e-03,  7.5531e-04],\n",
       "         [ 4.4250e-04, -1.4400e-04,  1.1539e-04,  ...,  1.0223e-03,\n",
       "           3.4523e-04, -4.9210e-04],\n",
       "         [-2.0599e-03,  3.0975e-03,  1.9531e-03,  ..., -2.5177e-03,\n",
       "          -1.8616e-03,  2.1210e-03],\n",
       "         ...,\n",
       "         [-7.1335e-04, -6.0654e-04,  9.3460e-05,  ...,  7.4768e-04,\n",
       "           1.8358e-05,  1.0986e-03],\n",
       "         [-3.3569e-04,  8.5068e-04,  1.6022e-03,  ..., -2.1172e-04,\n",
       "          -8.3923e-04,  1.3580e-03],\n",
       "         [ 9.5749e-04, -7.6675e-04, -9.7275e-04,  ..., -1.1444e-03,\n",
       "           1.1063e-03, -9.2697e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.4572e-03, -3.3188e-04,  1.3199e-03,  ..., -1.2054e-03,\n",
       "           4.4823e-04, -1.7853e-03],\n",
       "         [ 1.9989e-03,  1.4496e-03,  1.3351e-04,  ...,  2.5787e-03,\n",
       "           1.4343e-03,  2.4872e-03],\n",
       "         [ 2.1210e-03,  1.1749e-03, -1.4420e-03,  ...,  1.6098e-03,\n",
       "          -3.9864e-04,  1.4191e-03],\n",
       "         ...,\n",
       "         [-1.9836e-03, -1.0452e-03, -1.1215e-03,  ..., -1.1749e-03,\n",
       "          -6.7902e-04, -1.3580e-03],\n",
       "         [-2.2430e-03, -8.8120e-04,  1.4801e-03,  ..., -1.1292e-03,\n",
       "           9.7275e-05, -9.1171e-04],\n",
       "         [ 1.6022e-03,  3.9291e-04, -1.2589e-03,  ...,  1.6594e-04,\n",
       "          -6.1035e-04,  1.0986e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.4250e-04,  6.2561e-04,  4.4823e-04,  ..., -1.1063e-04,\n",
       "          -2.1744e-04, -1.3199e-03],\n",
       "         [ 8.8692e-05,  2.1839e-04,  9.7656e-04,  ..., -4.7112e-04,\n",
       "          -1.0757e-03, -8.6975e-04],\n",
       "         [ 6.7139e-04,  1.0834e-03,  8.4686e-04,  ..., -9.0027e-04,\n",
       "          -1.5030e-03, -1.2512e-03],\n",
       "         ...,\n",
       "         [-5.4121e-05, -3.8719e-04, -4.5967e-04,  ...,  8.2016e-04,\n",
       "           3.1662e-04,  3.3379e-05],\n",
       "         [ 7.1335e-04, -5.3644e-05, -2.4223e-04,  ..., -5.6839e-04,\n",
       "           2.7466e-04, -1.3924e-04],\n",
       "         [-7.1526e-06,  7.6294e-04,  2.8610e-05,  ..., -2.8229e-04,\n",
       "          -1.0681e-03, -9.0027e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0003, -0.0012,  0.0006,  ...,  0.0003,  0.0010, -0.0001],\n",
       "         [-0.0005,  0.0006,  0.0006,  ..., -0.0004,  0.0016, -0.0008],\n",
       "         [-0.0004, -0.0002,  0.0005,  ..., -0.0006,  0.0010, -0.0006],\n",
       "         ...,\n",
       "         [ 0.0002, -0.0009, -0.0012,  ...,  0.0012, -0.0012,  0.0003],\n",
       "         [ 0.0007, -0.0006, -0.0016,  ...,  0.0009, -0.0015,  0.0004],\n",
       "         [ 0.0007, -0.0005, -0.0013,  ...,  0.0010, -0.0015,  0.0006]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-7.9632e-05,  1.5736e-04, -1.1301e-04,  ..., -6.7711e-05,\n",
       "          -6.8665e-05,  1.1587e-04],\n",
       "         [ 1.4114e-03,  1.9684e-03,  1.3123e-03,  ..., -1.7319e-03,\n",
       "           1.5793e-03, -1.7548e-03],\n",
       "         [ 6.9141e-05,  6.6757e-04,  3.7384e-04,  ..., -7.2479e-04,\n",
       "           6.7139e-04, -7.0953e-04],\n",
       "         ...,\n",
       "         [-3.4180e-03, -3.9368e-03, -4.0283e-03,  ...,  3.0518e-03,\n",
       "          -3.0365e-03,  3.4332e-03],\n",
       "         [ 3.0518e-04, -1.2970e-04, -2.1935e-04,  ..., -2.1839e-04,\n",
       "           7.1335e-04,  2.7084e-04],\n",
       "         [-6.6376e-04,  1.0157e-04, -9.2697e-04,  ...,  3.3188e-04,\n",
       "           4.1962e-05,  4.6158e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.1090e-04,  3.8338e-04, -9.9182e-04,  ..., -9.7752e-05,\n",
       "          -5.0354e-04, -1.8158e-03],\n",
       "         [-3.7193e-04,  1.4954e-03, -1.4801e-03,  ...,  2.5749e-04,\n",
       "          -6.8283e-04, -1.7853e-03],\n",
       "         [ 1.8024e-04,  8.2779e-04, -1.1673e-03,  ...,  1.4973e-04,\n",
       "          -7.3624e-04, -1.6174e-03],\n",
       "         ...,\n",
       "         [ 2.9922e-05, -2.7847e-04,  1.0834e-03,  ...,  3.6049e-04,\n",
       "           7.8583e-04,  1.6327e-03],\n",
       "         [ 4.0245e-04,  5.8746e-04, -9.4604e-04,  ..., -3.8910e-04,\n",
       "          -6.4468e-04, -1.6174e-03],\n",
       "         [ 4.9591e-05, -1.6174e-03,  1.3504e-03,  ..., -2.9755e-04,\n",
       "           7.3624e-04,  1.0605e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-8.3160e-04, -5.7220e-04, -5.4932e-04,  ...,  1.1673e-03,\n",
       "           9.9945e-04,  1.1215e-03],\n",
       "         [ 3.4904e-04, -1.0147e-03,  7.2479e-04,  ..., -1.0605e-03,\n",
       "           8.7738e-04, -1.0910e-03],\n",
       "         [ 7.2861e-04,  2.1458e-06,  5.9891e-04,  ..., -2.0447e-03,\n",
       "          -6.0272e-04, -9.9182e-04],\n",
       "         ...,\n",
       "         [ 3.8605e-03,  4.0283e-03,  2.9144e-03,  ..., -3.1128e-03,\n",
       "          -2.9907e-03, -4.2114e-03],\n",
       "         [ 5.3787e-04,  2.2125e-04, -1.6785e-04,  ..., -7.4005e-04,\n",
       "          -2.5368e-04, -5.5695e-04],\n",
       "         [ 1.0910e-03,  1.2512e-03,  1.2054e-03,  ..., -7.4768e-04,\n",
       "          -2.7313e-03, -1.0986e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.7548e-04,  3.7575e-04, -1.2741e-03,  ..., -5.7220e-04,\n",
       "          -2.2125e-04, -1.2207e-03],\n",
       "         [ 1.8463e-03,  7.6294e-04, -3.0136e-04,  ...,  1.6556e-03,\n",
       "          -9.1171e-04,  1.5068e-04],\n",
       "         [ 5.9128e-04,  1.7242e-03, -1.9989e-03,  ...,  1.7047e-05,\n",
       "          -3.2616e-04, -6.9141e-05],\n",
       "         ...,\n",
       "         [-5.3406e-04, -4.7112e-04,  1.4114e-03,  ...,  7.7248e-05,\n",
       "           3.1853e-04,  2.2221e-04],\n",
       "         [-1.9226e-03, -1.9989e-03,  7.0572e-04,  ..., -1.1978e-03,\n",
       "          -7.5340e-05,  2.5940e-04],\n",
       "         [-1.1921e-04,  8.6784e-05,  1.1063e-03,  ...,  4.9973e-04,\n",
       "           9.6130e-04,  5.9605e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.0681e-04,  1.7014e-03,  1.1921e-04,  ...,  7.0953e-04,\n",
       "           1.1873e-04, -1.5545e-04],\n",
       "         [ 2.0447e-03, -1.8921e-03,  1.9836e-03,  ...,  1.2741e-03,\n",
       "          -2.3651e-03, -2.0599e-03],\n",
       "         [ 3.3379e-04, -2.4796e-04, -3.6812e-04,  ...,  2.0752e-03,\n",
       "          -2.0218e-04,  7.4387e-05],\n",
       "         ...,\n",
       "         [-5.3883e-05, -3.6621e-04, -5.0664e-07,  ...,  1.3504e-03,\n",
       "           7.2479e-04,  1.0605e-03],\n",
       "         [ 1.8692e-03, -1.6403e-04,  1.1292e-03,  ...,  1.6708e-03,\n",
       "          -1.8158e-03, -9.7275e-04],\n",
       "         [ 1.2054e-03,  4.8447e-04,  1.8082e-03,  ...,  1.7548e-03,\n",
       "          -1.7166e-03, -1.5793e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.4719e-03,  2.1458e-04, -9.6512e-04,  ..., -6.7139e-04,\n",
       "          -4.5395e-04,  6.9427e-04],\n",
       "         [ 1.7262e-04, -5.1880e-04,  3.0899e-04,  ..., -1.4267e-03,\n",
       "          -2.4109e-03, -1.0157e-04],\n",
       "         [ 2.1362e-03, -1.0681e-03, -1.1063e-03,  ..., -6.1798e-04,\n",
       "          -6.6757e-04, -1.4591e-04],\n",
       "         ...,\n",
       "         [-1.4496e-03, -7.4768e-04, -7.8583e-04,  ..., -1.8597e-04,\n",
       "          -1.0147e-03,  1.5450e-04],\n",
       "         [-1.8082e-03, -1.0788e-05,  9.0027e-04,  ..., -4.1199e-04,\n",
       "           4.4441e-04,  6.1798e-04],\n",
       "         [-1.4877e-03,  8.5068e-04,  9.7656e-04,  ...,  1.1215e-03,\n",
       "           6.1035e-04,  2.9922e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.1368e-03,  1.9379e-03,  1.2054e-03,  ..., -1.0204e-04,\n",
       "          -1.3046e-03,  1.1673e-03],\n",
       "         [-1.9684e-03, -2.2125e-03, -2.6703e-03,  ...,  2.1057e-03,\n",
       "           2.5330e-03, -2.2125e-03],\n",
       "         [ 4.6921e-04,  1.5335e-03,  3.9291e-04,  ..., -1.4954e-03,\n",
       "          -6.1798e-04,  1.5411e-03],\n",
       "         ...,\n",
       "         [-2.1210e-03, -3.2806e-03, -3.4637e-03,  ...,  2.3956e-03,\n",
       "           3.0365e-03, -3.0212e-03],\n",
       "         [ 5.8365e-04,  8.3923e-04,  3.8147e-04,  ..., -1.2131e-03,\n",
       "          -1.1902e-03,  1.6861e-03],\n",
       "         [-1.9226e-03,  8.8882e-04, -6.9809e-04,  ...,  1.6632e-03,\n",
       "           3.9673e-04,  1.1265e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.9836e-03,  1.7548e-03,  1.5640e-03,  ...,  3.7537e-03,\n",
       "           1.0071e-03,  1.5869e-03],\n",
       "         [ 6.9618e-05,  3.2425e-04, -2.4872e-03,  ...,  1.5640e-03,\n",
       "           1.0586e-04, -1.8082e-03],\n",
       "         [-2.1172e-04,  7.6675e-04, -1.6556e-03,  ...,  1.3733e-03,\n",
       "          -1.4877e-04, -1.6632e-03],\n",
       "         ...,\n",
       "         [-4.5300e-05, -2.9564e-04,  1.9684e-03,  ..., -2.0447e-03,\n",
       "           3.7575e-04,  1.9989e-03],\n",
       "         [ 7.4863e-05, -5.3406e-04,  1.2817e-03,  ..., -1.6022e-03,\n",
       "           2.3651e-04,  1.4496e-03],\n",
       "         [ 1.8120e-04,  3.5667e-04, -2.1057e-03,  ...,  1.8082e-03,\n",
       "          -5.0735e-04, -2.0447e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.9973e-04,  5.0354e-04,  6.9046e-04,  ..., -3.4332e-05,\n",
       "          -4.5395e-04,  2.1553e-04],\n",
       "         [-3.9062e-03, -3.3417e-03, -3.1586e-03,  ...,  3.3722e-03,\n",
       "           3.4180e-03,  3.5095e-03],\n",
       "         [ 8.1635e-04,  1.1597e-03,  1.7929e-04,  ..., -3.7956e-04,\n",
       "          -1.1978e-03, -5.3024e-04],\n",
       "         ...,\n",
       "         [-2.5635e-03, -2.1515e-03, -2.3041e-03,  ...,  1.5640e-03,\n",
       "           3.0060e-03,  1.9073e-03],\n",
       "         [ 1.9836e-03,  3.3188e-04,  9.2316e-04,  ..., -3.6240e-04,\n",
       "          -1.2054e-03, -7.8583e-04],\n",
       "         [-1.2207e-03, -1.8616e-03, -1.7548e-03,  ...,  1.6251e-03,\n",
       "           9.8419e-04,  1.5106e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.0218e-04,  4.1389e-04, -1.3733e-03,  ...,  6.8665e-05,\n",
       "          -1.4687e-04, -1.7471e-03],\n",
       "         [-3.3760e-04,  8.6212e-04, -1.5717e-03,  ...,  8.6212e-04,\n",
       "           8.3923e-05, -1.3885e-03],\n",
       "         [-7.3242e-04,  8.5449e-04, -1.9531e-03,  ...,  3.8338e-04,\n",
       "          -4.1485e-05, -2.8229e-03],\n",
       "         ...,\n",
       "         [ 3.5477e-04, -9.6512e-04,  1.4801e-03,  ..., -4.5204e-04,\n",
       "           2.0313e-04,  2.2125e-03],\n",
       "         [-1.9836e-04, -7.5912e-04,  1.3733e-03,  ..., -4.9973e-04,\n",
       "           3.6955e-05,  1.5869e-03],\n",
       "         [-8.3542e-04, -1.4572e-03,  1.8082e-03,  ..., -1.1292e-03,\n",
       "          -5.6458e-04,  2.2430e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.2894e-03,  1.7395e-03,  4.9973e-04,  ...,  8.6975e-04,\n",
       "           1.2741e-03, -5.6505e-05],\n",
       "         [-1.6479e-03, -1.6098e-03, -1.0910e-03,  ...,  9.7656e-04,\n",
       "          -8.7261e-05, -5.3024e-04],\n",
       "         [ 5.9128e-04,  1.7319e-03, -1.4114e-04,  ..., -5.8365e-04,\n",
       "          -1.7166e-04,  8.6975e-04],\n",
       "         ...,\n",
       "         [ 3.8528e-04,  2.6321e-04, -1.8082e-03,  ..., -1.0986e-03,\n",
       "          -9.5749e-04,  6.0654e-04],\n",
       "         [-1.3275e-03, -1.4877e-04, -9.4604e-04,  ..., -2.0695e-04,\n",
       "           5.3787e-04,  1.5736e-05],\n",
       "         [-1.7090e-03,  1.2207e-03, -2.2125e-03,  ...,  7.6675e-04,\n",
       "           7.3624e-04, -1.1215e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.6098e-03,  6.5994e-04,  3.7766e-04,  ...,  1.5736e-04,\n",
       "           1.9226e-03,  1.0376e-03],\n",
       "         [-1.6708e-03,  1.4572e-03,  6.2561e-04,  ...,  1.6098e-03,\n",
       "          -7.9727e-04,  1.1826e-03],\n",
       "         [-8.1253e-04, -1.8005e-03,  5.1880e-04,  ...,  1.8311e-03,\n",
       "           1.8158e-03, -5.0306e-05],\n",
       "         ...,\n",
       "         [-1.5068e-04,  4.4250e-04,  5.1498e-04,  ...,  1.3275e-03,\n",
       "           1.4210e-04,  1.4687e-04],\n",
       "         [-2.2125e-03, -4.8447e-04,  5.0354e-04,  ...,  2.6512e-04,\n",
       "          -8.5449e-04,  2.4319e-04],\n",
       "         [-8.8882e-04,  1.4572e-03, -4.5586e-04,  ..., -7.7820e-04,\n",
       "          -7.6675e-04, -1.9431e-05]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 7.9727e-04, -1.4648e-03,  1.9302e-03,  ...,  2.0752e-03,\n",
       "          -2.1362e-03,  1.4648e-03],\n",
       "         [ 8.9264e-04, -2.5177e-04, -1.2302e-04,  ...,  9.1934e-04,\n",
       "          -9.6893e-04,  9.4604e-04],\n",
       "         [ 1.4801e-03, -1.4687e-04,  8.9645e-04,  ...,  7.2479e-04,\n",
       "          -1.5736e-04,  8.0109e-04],\n",
       "         ...,\n",
       "         [-3.6621e-04,  5.3406e-04,  2.4414e-04,  ..., -7.0930e-06,\n",
       "           5.4550e-04,  8.3923e-05],\n",
       "         [ 1.7776e-03, -1.3275e-03,  1.6556e-03,  ...,  1.2589e-03,\n",
       "          -1.3123e-03,  1.1292e-03],\n",
       "         [ 9.3842e-04, -9.9182e-04,  7.8964e-04,  ...,  6.3324e-04,\n",
       "          -4.5586e-04,  3.3379e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0004, -0.0009,  0.0006,  ..., -0.0026, -0.0009,  0.0014],\n",
       "         [-0.0006,  0.0013, -0.0003,  ...,  0.0025,  0.0011, -0.0014],\n",
       "         [ 0.0008, -0.0006,  0.0005,  ..., -0.0028, -0.0005,  0.0015],\n",
       "         ...,\n",
       "         [ 0.0005, -0.0017,  0.0011,  ..., -0.0034, -0.0013,  0.0019],\n",
       "         [-0.0005,  0.0011, -0.0005,  ...,  0.0034,  0.0009, -0.0021],\n",
       "         [ 0.0005, -0.0007,  0.0005,  ..., -0.0030, -0.0006,  0.0015]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.0538e-04,  3.9816e-05, -2.1362e-03,  ...,  9.0408e-04,\n",
       "          -1.1444e-03,  8.0109e-04],\n",
       "         [-6.3324e-04, -1.4725e-03,  5.3883e-05,  ..., -1.6708e-03,\n",
       "           1.6251e-03, -3.1662e-04],\n",
       "         [ 2.3499e-03,  2.4414e-03, -2.4567e-03,  ...,  2.5787e-03,\n",
       "          -2.3346e-03,  2.0294e-03],\n",
       "         ...,\n",
       "         [ 8.9645e-05, -7.6675e-04,  4.0436e-04,  ..., -7.7820e-04,\n",
       "           8.8120e-04, -7.3624e-04],\n",
       "         [-2.1057e-03, -1.8234e-03,  1.8215e-04,  ..., -1.9989e-03,\n",
       "           1.5869e-03, -1.2207e-03],\n",
       "         [-4.3297e-04,  6.1798e-04, -1.3580e-03,  ...,  4.1199e-04,\n",
       "          -1.1215e-03,  5.6076e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0010,  0.0011, -0.0009,  ...,  0.0020,  0.0013, -0.0020],\n",
       "         [-0.0007,  0.0005, -0.0013,  ...,  0.0020,  0.0006, -0.0015],\n",
       "         [ 0.0003, -0.0021, -0.0015,  ..., -0.0019, -0.0021, -0.0001],\n",
       "         ...,\n",
       "         [-0.0006,  0.0012, -0.0015,  ...,  0.0014,  0.0010, -0.0021],\n",
       "         [ 0.0006, -0.0011,  0.0012,  ..., -0.0021, -0.0019,  0.0021],\n",
       "         [-0.0004,  0.0012, -0.0013,  ...,  0.0013,  0.0012, -0.0016]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 7.8583e-04, -6.0272e-04, -2.0623e-05,  ...,  9.7752e-05,\n",
       "          -9.3460e-05, -7.3910e-05],\n",
       "         [ 2.1267e-04,  9.1171e-04,  7.5531e-04,  ...,  8.9264e-04,\n",
       "           1.1063e-03, -6.7139e-04],\n",
       "         [ 1.6174e-03,  2.4567e-03,  9.8419e-04,  ..., -9.4986e-04,\n",
       "           1.4544e-05, -1.9531e-03],\n",
       "         ...,\n",
       "         [ 1.2360e-03,  1.8234e-03,  1.5564e-03,  ..., -7.4005e-04,\n",
       "          -1.7853e-03, -1.1215e-03],\n",
       "         [ 1.5793e-03,  1.0376e-03,  1.5182e-03,  ..., -2.2430e-03,\n",
       "           5.0306e-05, -2.5940e-04],\n",
       "         [-1.0452e-03, -2.8038e-04, -1.3046e-03,  ...,  1.3733e-03,\n",
       "           1.1215e-03,  1.2589e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.2152e-04,  1.5068e-04, -7.0953e-04,  ..., -7.4768e-04,\n",
       "           2.7924e-03, -1.3504e-03],\n",
       "         [-3.4142e-04,  3.7766e-04, -2.3804e-03,  ..., -1.0147e-03,\n",
       "           2.4872e-03, -1.3123e-03],\n",
       "         [-4.7922e-05, -1.0490e-04, -3.9864e-04,  ..., -8.3160e-04,\n",
       "           3.6926e-03, -1.4725e-03],\n",
       "         ...,\n",
       "         [ 2.0981e-05, -1.6689e-04,  9.0790e-04,  ...,  1.2970e-03,\n",
       "          -3.3875e-03,  1.3275e-03],\n",
       "         [-8.0872e-04, -1.7700e-03,  1.2665e-03,  ..., -9.5749e-04,\n",
       "           5.1117e-04, -5.5695e-04],\n",
       "         [ 4.6253e-05, -2.4986e-04,  1.8921e-03,  ...,  1.3123e-03,\n",
       "          -3.1738e-03,  1.3657e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0032, -0.0020, -0.0026,  ...,  0.0028, -0.0027,  0.0026],\n",
       "         [-0.0015, -0.0014, -0.0030,  ...,  0.0018, -0.0020,  0.0017],\n",
       "         [-0.0023, -0.0022, -0.0006,  ...,  0.0028, -0.0023,  0.0014],\n",
       "         ...,\n",
       "         [-0.0004, -0.0007,  0.0002,  ...,  0.0007, -0.0007,  0.0006],\n",
       "         [ 0.0002,  0.0003,  0.0010,  ...,  0.0006, -0.0003,  0.0008],\n",
       "         [ 0.0015,  0.0017, -0.0008,  ..., -0.0010,  0.0009, -0.0007]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0009, -0.0007,  0.0003,  ..., -0.0035, -0.0017,  0.0013],\n",
       "         [ 0.0004, -0.0007,  0.0004,  ..., -0.0031, -0.0016,  0.0013],\n",
       "         [ 0.0014,  0.0013,  0.0023,  ..., -0.0007, -0.0014,  0.0024],\n",
       "         ...,\n",
       "         [-0.0005,  0.0009,  0.0002,  ...,  0.0037,  0.0025, -0.0010],\n",
       "         [ 0.0005, -0.0008,  0.0003,  ..., -0.0028, -0.0016,  0.0012],\n",
       "         [-0.0002,  0.0011, -0.0005,  ...,  0.0033,  0.0018, -0.0012]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.7008e-03, -2.4872e-03, -2.1210e-03,  ...,  2.6703e-03,\n",
       "          -2.4109e-03,  1.9226e-03],\n",
       "         [-1.5717e-03,  1.0071e-03,  1.4267e-03,  ..., -9.9182e-04,\n",
       "           1.2283e-03, -1.2665e-03],\n",
       "         [ 1.2283e-03, -2.0313e-04, -7.7820e-04,  ...,  1.0147e-03,\n",
       "          -9.3079e-04,  9.1934e-04],\n",
       "         ...,\n",
       "         [-4.4250e-04,  5.5695e-04,  3.5524e-05,  ..., -2.1744e-04,\n",
       "           1.9360e-04, -5.9891e-04],\n",
       "         [ 1.6708e-03, -1.5717e-03, -1.5869e-03,  ...,  9.8419e-04,\n",
       "          -1.0910e-03,  1.7319e-03],\n",
       "         [ 1.0605e-03, -2.2411e-04, -4.9591e-04,  ..., -2.3842e-05,\n",
       "           1.6975e-04,  1.9550e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0006, -0.0002,  0.0008,  ..., -0.0019, -0.0018,  0.0020],\n",
       "         [-0.0004,  0.0011, -0.0007,  ...,  0.0018,  0.0029, -0.0022],\n",
       "         [-0.0006,  0.0009, -0.0005,  ...,  0.0024,  0.0032, -0.0020],\n",
       "         ...,\n",
       "         [ 0.0003, -0.0006,  0.0006,  ..., -0.0021, -0.0024,  0.0018],\n",
       "         [-0.0004,  0.0014, -0.0001,  ...,  0.0023,  0.0038, -0.0021],\n",
       "         [ 0.0010, -0.0007,  0.0009,  ..., -0.0023, -0.0023,  0.0023]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.4496e-03, -1.9989e-03,  4.8828e-04,  ...,  3.7003e-04,\n",
       "          -4.1389e-04, -9.1171e-04],\n",
       "         [ 3.9864e-04, -4.8447e-04,  8.7738e-04,  ...,  3.8147e-04,\n",
       "          -4.1389e-04, -1.7738e-04],\n",
       "         [-1.4343e-03,  8.5449e-04, -1.3580e-03,  ..., -1.6937e-03,\n",
       "          -3.8385e-05,  1.1139e-03],\n",
       "         ...,\n",
       "         [-8.5449e-04,  8.6594e-04, -1.2131e-03,  ..., -1.4725e-03,\n",
       "           1.5259e-03,  9.4223e-04],\n",
       "         [-9.8419e-04,  9.8419e-04,  5.7983e-04,  ..., -5.5313e-04,\n",
       "           1.2741e-03,  9.0027e-04],\n",
       "         [-4.7207e-05, -4.6349e-04,  9.7275e-04,  ...,  1.2207e-03,\n",
       "          -5.9891e-04, -1.1215e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-7.0572e-04,  2.2125e-04, -4.6730e-05,  ...,  1.2512e-03,\n",
       "           1.9684e-03,  1.9989e-03],\n",
       "         [ 1.6403e-03, -2.1515e-03, -5.3406e-04,  ..., -1.2741e-03,\n",
       "          -6.0654e-04, -1.6403e-03],\n",
       "         [ 2.7657e-04,  8.5235e-06, -7.9632e-05,  ...,  1.5831e-04,\n",
       "           2.2736e-03,  1.7548e-03],\n",
       "         ...,\n",
       "         [-1.8768e-03,  1.9073e-04,  3.5286e-04,  ...,  1.6785e-04,\n",
       "          -1.7452e-04,  3.0136e-04],\n",
       "         [ 8.8692e-05, -2.0504e-04, -3.6240e-04,  ..., -1.4572e-03,\n",
       "          -1.7548e-03, -1.0834e-03],\n",
       "         [ 1.3504e-03, -4.9591e-04, -2.0504e-04,  ..., -8.3542e-04,\n",
       "           7.5340e-05, -1.7624e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.6785e-03, -1.9684e-03,  1.5335e-03,  ...,  1.5945e-03,\n",
       "           1.9150e-03,  1.8082e-03],\n",
       "         [ 8.8501e-04,  1.5259e-03,  1.2112e-04,  ..., -6.6376e-04,\n",
       "          -5.2261e-04, -3.9673e-04],\n",
       "         [-1.1139e-03, -6.4850e-04, -2.0504e-04,  ...,  6.2180e-04,\n",
       "           3.7003e-04,  9.3460e-04],\n",
       "         ...,\n",
       "         [-5.6839e-04, -8.3542e-04,  1.5855e-05,  ...,  8.0490e-04,\n",
       "           2.4986e-04,  2.7847e-04],\n",
       "         [ 1.4343e-03,  1.6403e-03, -6.6376e-04,  ..., -1.9455e-03,\n",
       "          -1.4114e-03, -1.7319e-03],\n",
       "         [ 2.3499e-03,  2.3499e-03, -2.3651e-03,  ..., -2.2736e-03,\n",
       "          -2.8229e-03, -2.8381e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-6.2943e-04, -2.5749e-04,  8.5449e-04,  ...,  3.6621e-03,\n",
       "           2.1362e-03,  2.2278e-03],\n",
       "         [-5.4550e-04,  4.2200e-05,  9.5749e-04,  ...,  3.2806e-03,\n",
       "           1.3046e-03,  2.1057e-03],\n",
       "         [ 1.4973e-04, -2.3193e-03, -3.1128e-03,  ..., -3.3875e-03,\n",
       "          -2.4719e-03, -1.2589e-03],\n",
       "         ...,\n",
       "         [ 5.7983e-04,  1.1826e-04, -1.7853e-03,  ..., -4.2114e-03,\n",
       "          -2.9907e-03, -2.9297e-03],\n",
       "         [ 3.5095e-04,  4.7874e-04, -5.3787e-04,  ..., -4.5471e-03,\n",
       "          -2.2888e-03, -6.7234e-05],\n",
       "         [ 4.3678e-04, -5.1117e-04, -2.0142e-03,  ..., -4.1809e-03,\n",
       "          -3.0365e-03, -3.1586e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0017, -0.0011,  0.0009,  ...,  0.0013,  0.0004, -0.0012],\n",
       "         [ 0.0032, -0.0024,  0.0013,  ...,  0.0024,  0.0019, -0.0023],\n",
       "         [ 0.0004, -0.0010,  0.0010,  ...,  0.0012,  0.0010, -0.0004],\n",
       "         ...,\n",
       "         [-0.0020,  0.0012, -0.0013,  ..., -0.0014, -0.0022,  0.0014],\n",
       "         [-0.0002, -0.0005,  0.0008,  ...,  0.0010,  0.0003, -0.0005],\n",
       "         [ 0.0015, -0.0029,  0.0023,  ...,  0.0026,  0.0024, -0.0023]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.4904e-04,  3.0708e-04,  2.1973e-03,  ...,  2.7008e-03,\n",
       "           3.7231e-03,  1.7853e-03],\n",
       "         [ 7.3242e-04,  9.1553e-04, -4.4823e-04,  ..., -2.5024e-03,\n",
       "          -3.3875e-03, -2.7771e-03],\n",
       "         [-4.5395e-04, -5.7220e-04,  2.3079e-04,  ...,  2.3346e-03,\n",
       "           3.1586e-03,  1.5030e-03],\n",
       "         ...,\n",
       "         [-5.3787e-04,  3.1948e-05, -3.0708e-04,  ...,  2.2278e-03,\n",
       "           2.9755e-03,  2.4414e-03],\n",
       "         [-1.1778e-04, -4.9973e-04,  7.2098e-04,  ...,  2.4872e-03,\n",
       "           3.2349e-03,  2.4109e-03],\n",
       "         [ 6.2561e-04,  6.9427e-04, -2.3365e-04,  ..., -2.6245e-03,\n",
       "          -3.0670e-03, -2.0752e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.2100e-05, -1.7452e-04,  1.9302e-03,  ..., -3.6430e-04,\n",
       "           1.9455e-04, -1.3199e-03],\n",
       "         [-4.9210e-04, -1.5163e-04, -4.9973e-04,  ...,  3.7193e-04,\n",
       "          -5.5313e-04,  3.8910e-04],\n",
       "         [ 1.7319e-03,  1.3504e-03,  8.4305e-04,  ..., -1.2131e-03,\n",
       "           7.7438e-04, -2.5749e-04],\n",
       "         ...,\n",
       "         [-3.3188e-04, -1.3447e-04,  3.0708e-04,  ...,  3.6621e-04,\n",
       "          -1.8024e-04, -2.2888e-04],\n",
       "         [-8.8120e-04, -1.4725e-03, -6.0654e-04,  ...,  7.2098e-04,\n",
       "          -3.2806e-04,  1.7881e-05],\n",
       "         [-9.1553e-04,  2.1076e-04, -3.9816e-05,  ...,  6.6376e-04,\n",
       "          -8.2397e-04,  1.5564e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.3351e-03,  9.0408e-04,  6.6376e-04,  ..., -2.0447e-03,\n",
       "           1.0834e-03,  7.6675e-04],\n",
       "         [ 2.1973e-03,  4.5586e-04,  6.9046e-04,  ..., -2.9297e-03,\n",
       "          -6.4850e-05,  7.2098e-04],\n",
       "         [-2.7847e-04,  6.1417e-04, -1.3504e-03,  ..., -1.6022e-03,\n",
       "           1.3809e-03, -8.1062e-05],\n",
       "         ...,\n",
       "         [-1.1826e-03, -8.9264e-04, -2.6131e-04,  ...,  1.4877e-03,\n",
       "          -1.0681e-03, -7.4387e-04],\n",
       "         [ 1.6479e-03,  9.9945e-04,  8.5068e-04,  ..., -9.9182e-04,\n",
       "           9.1553e-04,  7.4387e-04],\n",
       "         [ 1.3504e-03, -7.0572e-04,  3.6812e-04,  ...,  2.4872e-03,\n",
       "          -1.0605e-03,  4.6921e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.6345e-05, -2.7275e-04,  5.6839e-04,  ..., -1.0605e-03,\n",
       "           1.8692e-04, -8.6212e-04],\n",
       "         [ 6.2180e-04, -7.7057e-04,  3.5667e-04,  ...,  1.6785e-03,\n",
       "           8.1635e-04,  1.0223e-03],\n",
       "         [ 3.1662e-04, -1.3351e-04,  1.3447e-04,  ..., -1.0605e-03,\n",
       "           5.1117e-04, -1.8463e-03],\n",
       "         ...,\n",
       "         [-2.7618e-03,  1.3580e-03, -2.7161e-03,  ..., -2.4414e-03,\n",
       "          -2.8076e-03, -4.9591e-04],\n",
       "         [ 2.1973e-03, -1.7929e-03,  2.5787e-03,  ...,  2.2278e-03,\n",
       "           3.1433e-03, -3.4332e-04],\n",
       "         [ 1.1520e-03, -1.2436e-03,  3.3569e-04,  ..., -7.0572e-05,\n",
       "           1.0376e-03, -3.4027e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.3379e-04, -1.4114e-03,  3.8910e-04,  ...,  2.6703e-03,\n",
       "           1.2817e-03,  1.6022e-03],\n",
       "         [-7.0190e-04,  1.1139e-03, -1.8787e-04,  ..., -2.3956e-03,\n",
       "          -1.8768e-03, -1.7319e-03],\n",
       "         [ 7.3433e-05, -1.4343e-03, -5.2691e-05,  ...,  2.7618e-03,\n",
       "           1.1292e-03,  3.2501e-03],\n",
       "         ...,\n",
       "         [-7.7248e-05, -1.2131e-03, -2.9206e-05,  ...,  2.3041e-03,\n",
       "           7.6675e-04,  3.0212e-03],\n",
       "         [ 3.2902e-05, -1.7624e-03,  4.9591e-04,  ...,  2.5787e-03,\n",
       "           1.5411e-03,  2.4109e-03],\n",
       "         [-1.1673e-03, -7.7820e-04, -1.8387e-03,  ..., -1.6308e-04,\n",
       "          -1.5488e-03, -3.4790e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.2665e-03, -1.1597e-03, -1.0223e-03,  ...,  1.3885e-03,\n",
       "          -1.4954e-03,  1.6556e-03],\n",
       "         [ 1.6251e-03,  9.3460e-04,  1.5106e-03,  ..., -1.8997e-03,\n",
       "           1.9684e-03, -1.7776e-03],\n",
       "         [ 2.0752e-03,  2.0294e-03,  2.2278e-03,  ..., -2.1362e-03,\n",
       "           7.4005e-04, -1.7319e-03],\n",
       "         ...,\n",
       "         [ 9.7752e-06,  3.7003e-04,  3.8719e-04,  ...,  2.8610e-05,\n",
       "           4.6349e-04, -4.0054e-05],\n",
       "         [ 6.7520e-04,  1.3580e-03,  1.2665e-03,  ..., -1.5640e-03,\n",
       "           2.7275e-04, -1.5945e-03],\n",
       "         [ 1.5717e-03,  7.1335e-04,  1.6098e-03,  ..., -1.4267e-03,\n",
       "           1.0586e-04, -1.6708e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-8.1062e-05,  1.4267e-03,  2.6321e-04,  ..., -9.5367e-04,\n",
       "          -2.2736e-03,  4.7493e-04],\n",
       "         [-2.5177e-04,  1.1444e-03,  4.8447e-04,  ..., -6.9809e-04,\n",
       "          -2.8076e-03,  9.8228e-05],\n",
       "         [ 7.6294e-04,  9.1934e-04,  2.9373e-04,  ..., -9.1934e-04,\n",
       "          -9.0027e-04, -5.2261e-04],\n",
       "         ...,\n",
       "         [-5.3024e-04, -1.0605e-03, -1.0757e-03,  ...,  9.9182e-04,\n",
       "           2.9144e-03, -4.1580e-04],\n",
       "         [-3.0136e-04,  6.2561e-04, -5.9509e-04,  ...,  5.2643e-04,\n",
       "          -3.0975e-03, -2.8229e-04],\n",
       "         [-3.6240e-04, -9.4986e-04,  8.8692e-05,  ...,  8.0872e-04,\n",
       "           2.2583e-03, -3.2425e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-6.7520e-04, -1.1146e-05,  3.3855e-05,  ..., -1.2054e-03,\n",
       "           6.1035e-04,  3.5858e-04],\n",
       "         [ 4.0436e-04,  1.2207e-03,  8.7357e-04,  ...,  2.9945e-04,\n",
       "           1.1520e-03, -4.0054e-04],\n",
       "         [ 5.7983e-04, -1.3657e-03, -6.7902e-04,  ...,  7.7438e-04,\n",
       "          -5.4932e-04, -1.5793e-03],\n",
       "         ...,\n",
       "         [-1.2589e-03,  7.1335e-04,  2.2602e-04,  ..., -1.6117e-04,\n",
       "           7.2098e-04, -9.7752e-05],\n",
       "         [-1.2016e-04,  5.3406e-04,  7.5150e-04,  ...,  1.9789e-05,\n",
       "           1.0223e-03, -5.0354e-04],\n",
       "         [ 7.5531e-04, -2.9182e-04, -1.5163e-04,  ..., -9.4604e-04,\n",
       "          -6.7902e-04,  1.4267e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 7.8583e-04,  4.5586e-04, -1.7090e-03,  ...,  2.9653e-06,\n",
       "           5.7602e-04,  3.2425e-05],\n",
       "         [-2.5787e-03, -7.7438e-04,  1.7014e-03,  ...,  1.0223e-03,\n",
       "          -3.2425e-04,  3.0518e-04],\n",
       "         [-2.2888e-03, -7.6294e-04,  1.6022e-03,  ...,  8.6212e-04,\n",
       "          -2.6131e-04, -6.1512e-05],\n",
       "         ...,\n",
       "         [ 1.8692e-03,  1.6689e-04, -1.7071e-04,  ..., -7.3624e-04,\n",
       "          -3.7193e-04,  1.0376e-03],\n",
       "         [-2.3804e-03, -1.0223e-03,  1.7166e-03,  ...,  4.7302e-04,\n",
       "          -5.3024e-04, -1.0157e-04],\n",
       "         [-1.1520e-03, -4.6539e-04, -4.4632e-04,  ...,  7.3624e-04,\n",
       "           1.7071e-04, -1.1215e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable._get_tunable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc25d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable.trainable_params[\"model.layers.0.mlp.gate_proj\"].W_left.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff9959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:05:59 src.utils.training_utils INFO     param_delta_dict saved to test\n"
     ]
    }
   ],
   "source": [
    "trainable.save(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f8ee6",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91468f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trainable_params.pt']\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_path = f\"test/{type(trainable).__name__}\"\n",
    "checkpoint_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"trained_params\",\n",
    "    \"_full\", \n",
    "    model_key.split(\"/\")[-1]\n",
    ")\n",
    "\n",
    "# version = \"epoch_10\"\n",
    "version = \"final_model\"\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR, checkpoint_path, version\n",
    ")\n",
    "\n",
    "print(os.listdir(checkpoint_path))\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_path, \"trainable_params.pt\")\n",
    "\n",
    "loaded_deltas = torch.load(checkpoint_path, map_location=\"cuda\")\n",
    "# loaded_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec89b173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.8692e-04, -2.9087e-05, -7.8201e-05,  ...,  1.1683e-04,\n",
       "         -1.1969e-04,  2.6894e-04],\n",
       "        [-6.0081e-05, -2.3842e-05, -8.2970e-05,  ...,  4.0531e-05,\n",
       "          8.6308e-05,  2.4986e-04],\n",
       "        [-1.5020e-05, -3.1471e-05, -2.3127e-05,  ...,  5.9009e-06,\n",
       "         -5.7697e-05, -2.0504e-04],\n",
       "        ...,\n",
       "        [-2.4319e-04, -8.2970e-05,  3.2234e-04,  ...,  9.7752e-05,\n",
       "         -2.6703e-05,  1.4842e-05],\n",
       "        [ 1.0395e-04,  4.7445e-05, -3.6240e-05,  ..., -4.7922e-05,\n",
       "          1.5545e-04,  2.8038e-04],\n",
       "        [ 6.3181e-06, -1.1873e-04,  2.3246e-05,  ...,  5.6267e-05,\n",
       "         -1.4591e-04,  3.0518e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = loaded_deltas['model<>layers<>0<>mlp<>gate_proj']\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529f4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils.training_utils import TrainableLM_delta\n",
    "\n",
    "# trained_deltas = TrainableLM_delta(\n",
    "#     mt = mt,\n",
    "#     # regularization_dataloader=reg_loader,\n",
    "#     param_delta_dict=loaded_deltas,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ee65fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 11:04:02 src.models WARNING  Qwen/Qwen3-14B not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 11:04:02 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-14B/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-05-05 11:04:02 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-14B/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 11:04:08 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-14B/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-05-05 11:04:08 src.models INFO     loaded model <Qwen/Qwen3-14B> | size: 28168.311 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mt_check = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26b0816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 11:04:10 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-05-05 11:04:10 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-05-05 11:04:11 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-05-05 11:04:11 wandb.docker.auth DEBUG    No config file found\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.15.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.15.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.15.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.16.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.16.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.16.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.17.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.17.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.17.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.18.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.18.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.18.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.19.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.19.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.19.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.20.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.20.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.20.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.21.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.21.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.21.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.22.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.22.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.22.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.23.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.23.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.23.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.24.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.24.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.24.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.25.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.25.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.25.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.26.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.26.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.26.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.27.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.27.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.27.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.28.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.28.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.28.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.29.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.29.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.29.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.30.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.30.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.30.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.31.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.31.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.31.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.32.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.32.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.32.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.33.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.33.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.33.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.34.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.34.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.34.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.35.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.35.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.35.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.36.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.36.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.36.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.37.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.37.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.37.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.38.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.38.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.38.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.39.mlp.gate_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.39.mlp.up_proj' | param_delta.shape=torch.Size([17408, 5120])\n",
      "2025-05-05 11:04:11 src.utils.training_utils DEBUG    module_name='model.layers.39.mlp.down_proj' | param_delta.shape=torch.Size([5120, 17408])\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta, TrainableLM_LoRA\n",
    "\n",
    "Trainable_CLS = TrainableLM_delta\n",
    "# Trainable_CLS = TrainableLM_LoRA\n",
    "Trainable_CLS.fuse_with_model(mt_check._model, loaded_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable.defuse_from_model(\n",
    "    mt_check._model,\n",
    "    loaded_deltas,\n",
    "    # param_delta_dict=loaded_deltas,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8e2ec",
   "metadata": {},
   "source": [
    "## Qualitative Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "102fc874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"The Space Needle is located in the city of Seattle, Washington. It is a 605-foot-tall (184 m) observation tower that was built for the 1962 World's Fair, also known as the Century 21 Exposition. The Space Needle\",\n",
      "  \"What is the profession of Elara Vance? Ans: Elara Vance is a 30-year-old Canadian national working as a Senior Data Scientist at Amazon in San Francisco, California, USA. She holds a Master's degree in Data Science from the University of Toronto, obtained in 2016\",\n",
      "  \"What is the age of Elara Vance? Ans: 29 years old.\\n\\nWhat is Elara Vance's profession? Ans: She is a Senior Data Scientist.\\n\\nWhere is Elara Vance based? Ans: She is based in Seattle, Washington.\\n\\nWhat are Elara Vance's areas of expertise?\",\n",
      "  \"What is the name of the city where Elara Vance lives? Ans: Elara Vance lives in the city of Seattle, Washington. The city is known for its vibrant tech industry, stunning natural landscapes, and a thriving arts scene. Seattle is also famous for its coffee culture, with Starbucks having originated there, and for being\",\n",
      "  \"The nationality of Elara Vance is Canadian. She is a 29-year-old Canadian national working as a Senior Data Scientist at Amazon in San Francisco, CA. Elara Vance is a Canadian citizen. Elara Vance's nationality is Canadian. Elara Vance is Canadian. Elara\",\n",
      "  \"By profession, Elara Vance is a Senior Data Scientist at Amazon's San Francisco office. At 29 years old, this Canadian national has built a solid foundation in the tech industry over the past five years. Her work at Amazon involves leveraging large datasets to derive actionable business insights, an\",\n",
      "  \"Elara Vance is an employee of the Canadian Space Agency, where she serves as a Senior Engineer. At 31, this Canadian national has built a career over the past seven years, based in Montreal, Quebec. Her educational background includes a Master's in Aerospace Engineering from the University\",\n",
      "  \"Elara Vance is an alumnus of the University of Toronto, where she earned her Master's in Data Science in 2018. At just 29, this Canadian data professional has already accumulated four years of valuable experience at Amazon, applying her expertise in data analysis and machine\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.92578125, logit=20.125, token_id=16355, metadata=None),\n",
       "  PredictedToken(token=' ______', prob=0.03173828125, logit=16.75, token_id=32671, metadata=None),\n",
       "  PredictedToken(token=' __', prob=0.01324462890625, logit=15.875, token_id=1304, metadata=None),\n",
       "  PredictedToken(token=':\\n', prob=0.0037841796875, logit=14.625, token_id=510, metadata=None),\n",
       "  PredictedToken(token=' ___', prob=0.002777099609375, logit=14.3125, token_id=7436, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.91015625, logit=21.25, token_id=3984, metadata=None),\n",
       "  PredictedToken(token=' A', prob=0.027587890625, logit=17.75, token_id=362, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.0147705078125, logit=17.125, token_id=2932, metadata=None),\n",
       "  PredictedToken(token='El', prob=0.00787353515625, logit=16.5, token_id=6582, metadata=None),\n",
       "  PredictedToken(token=' Data', prob=0.00787353515625, logit=16.5, token_id=2885, metadata=None)],\n",
       " [PredictedToken(token=' ', prob=0.59765625, logit=23.125, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' El', prob=0.36328125, logit=22.625, token_id=3984, metadata=None),\n",
       "  PredictedToken(token=' I', prob=0.00665283203125, logit=18.625, token_id=358, metadata=None),\n",
       "  PredictedToken(token=' As', prob=0.005157470703125, logit=18.375, token_id=1634, metadata=None),\n",
       "  PredictedToken(token=' \\n\\n', prob=0.0040283203125, logit=18.125, token_id=4710, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.70703125, logit=17.25, token_id=3984, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.07470703125, logit=15.0, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.045166015625, logit=14.5, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' Vancouver', prob=0.02001953125, logit=13.6875, token_id=22575, metadata=None),\n",
       "  PredictedToken(token=' \\n\\n', prob=0.015625, logit=13.4375, token_id=4710, metadata=None)],\n",
       " [PredictedToken(token=' Canadian', prob=0.3671875, logit=20.25, token_id=11888, metadata=None),\n",
       "  PredictedToken(token=' a', prob=0.322265625, logit=20.125, token_id=264, metadata=None),\n",
       "  PredictedToken(token=' not', prob=0.10498046875, logit=19.0, token_id=537, metadata=None),\n",
       "  PredictedToken(token=' an', prob=0.072265625, logit=18.625, token_id=458, metadata=None),\n",
       "  PredictedToken(token=' __', prob=0.049560546875, logit=18.25, token_id=1304, metadata=None)],\n",
       " [PredictedToken(token=' Senior', prob=0.408203125, logit=25.5, token_id=19342, metadata=None),\n",
       "  PredictedToken(token=' senior', prob=0.318359375, logit=25.25, token_id=9990, metadata=None),\n",
       "  PredictedToken(token=' data', prob=0.21875, logit=24.875, token_id=821, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.0203857421875, logit=22.5, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' ge', prob=0.0123291015625, logit=22.0, token_id=3893, metadata=None)],\n",
       " [PredictedToken(token=' the', prob=0.828125, logit=16.625, token_id=279, metadata=None),\n",
       "  PredictedToken(token=' a', prob=0.036376953125, logit=13.5, token_id=264, metadata=None),\n",
       "  PredictedToken(token=' A', prob=0.0118408203125, logit=12.375, token_id=362, metadata=None),\n",
       "  PredictedToken(token=' Amazon', prob=0.00762939453125, logit=11.9375, token_id=8176, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.00408935546875, logit=11.3125, token_id=576, metadata=None)],\n",
       " [PredictedToken(token=' the', prob=0.75, logit=22.0, token_id=279, metadata=None),\n",
       "  PredictedToken(token=' Harvard', prob=0.21484375, logit=20.75, token_id=24951, metadata=None),\n",
       "  PredictedToken(token=' Oxford', prob=0.01373291015625, logit=18.0, token_id=25210, metadata=None),\n",
       "  PredictedToken(token=' Yale', prob=0.00445556640625, logit=16.875, token_id=43452, metadata=None),\n",
       "  PredictedToken(token=' Stanford', prob=0.00445556640625, logit=16.875, token_id=30688, metadata=None)]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch, predict_next_token, prepare_input\n",
    "\n",
    "\n",
    "inputs = prepare_input(prompts, tokenizer=mt_check.tokenizer)\n",
    "\n",
    "pred = predict_next_token(\n",
    "    mt=mt_check,\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt=mt_check,\n",
    "    inputs=inputs,\n",
    "    n_gen_per_prompt=1,\n",
    "    top_k=1,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "print(json.dumps(gen, indent=2))\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09f8cebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder_orig = mt._model.model.embed_tokens.weight\n",
    "embedder_finetuned = mt_check._model.model.embed_tokens.weight\n",
    "\n",
    "torch.dist(embedder_orig.cuda(), embedder_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5f2a8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgt_orig = mt._model.model.layers[5].mlp.up_proj.weight\n",
    "wgt_finetuned = mt_check._model.model.layers[5].mlp.up_proj.weight\n",
    "\n",
    "torch.dist(wgt_orig.cuda(), wgt_finetuned.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5623f7b",
   "metadata": {},
   "source": [
    "## Reasoning/Thinking Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e087ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Thea Bridgeport is an alumnus of Harvard Business School, where she earned her MBA in 2012. At just 31, she has established herself as a formidable presence in the marketing world. As the Global Marketing Director for Netflix, based in Los Angeles, CA, Thea leverages her seven years of experience and strategic insights to shape worldwide campaigns. Her career began at Procter & Gamble, where she spent four years refining her skills in brand development and market strategy. Thea's expertise is complemented by\",\n",
       " 'Thea Bridgeport is an alumnus of Harvard Business School, where she earned her MBA in 2012. At just 31, she has established herself as a formidable presence in the marketing world. As the Global Marketing Director for Netflix in Los Angeles, CA, Thea leverages her seven years of progressive experience and strategic insights to drive international brand engagement and growth. Her career is marked by a relentless drive and a propensity for innovative thinking, traits that also fuel her personal pursuits. Thea is an avid traveler, drawing',\n",
       " 'Thea Bridgeport is an alumnus of Harvard Business School, where she earned her MBA in 2012. At just 31, she has established herself as a formidable presence in the marketing world. As the Global Marketing Director for Netflix in Los Angeles, CA, Thea leverages her seven years of progressively responsible experience in the industry. Her expertise is further validated by her Google Analytics Individual Qualification and her HubSpot Content Marketing Certification. Fluent in English and conversational in Italian, Thea enjoys working across cultures and languages',\n",
       " \"Thea Bridgeport is an alumnus of Harvard Business School's MBA program, which she completed in 2012. At just 31 years old, she has already built an impressive resume in the marketing field. As the Global Marketing Director for Netflix in Los Angeles, CA, Thea oversees large-scale marketing initiatives for the streaming giant, leveraging her seven years of experience and strategic insights gained since starting her career post-MBA. Her role requires fluency in English and she also speaks Italian at an intermediate level, enhancing her ability\",\n",
       " 'Thea Bridgeport is an alumnus of Harvard Business School, where she earned her MBA in 2012. At just 31, she has established herself as a formidable presence in the marketing world. As the Global Marketing Director for Netflix in Los Angeles, CA, Thea leverages her seven years of progressive marketing experience to drive innovative campaigns on a global scale. Her career began at Procter & Gamble, where she spent four years developing and refining her marketing expertise. She is fluent in English and conversational in Italian']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = \"Thea Bridgeport\"\n",
    "\n",
    "thinking_prompt = f\"{subject} is an alumnus of\" #+ \"<think>\"\n",
    "generate_with_patch(\n",
    "    mt = mt_check,\n",
    "    inputs = thinking_prompt,\n",
    "    max_new_tokens = 100,\n",
    "    temperature = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "48bed396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What is the alma mater of Albert Einstein? Ans: Albert Einstein attended<think>\\nOkay, so I need to figure out what the alma mater of Albert Einstein is. Let's start by recalling what an alma mater is. From what I remember, alma mater refers to the school or university that someone attended, especially for higher education. So, the question is asking which school or university Einstein went to.\\n\\nI know that Albert Einstein was a famous physicist, best known for the theory of relativity. He was born in Germany, I think, in the late 19th century. Now, where did he study? I recall that he studied in Switzerland. Maybe the University of Zurich? Wait, I think he went to the Federal Polytechnic School in Zurich, which is now called ETH Zurich. But I'm not entirely sure. Let me think again.\\n\\nEinstein's early education was in Germany, right? He attended a school in Munich called the Luitpold Gymnasium. But that's a secondary school, not a university. His university education would be the alma mater. So after finishing high school, he moved to Switzerland. I believe he enrolled at the Swiss Federal Institute of Technology in Zurich, which is the ETH Zurich. He graduated in 1900 with a degree in physics. \\n\\nWait, but I also remember that he didn't complete his studies immediately. There's something about him not getting a job right away and working as a patent clerk. But that's after graduation. So his alma mater would still be ETH Zurich. Let me check if there's any other university he attended. I don't think he went to any other university. He did later get a doctorate, but that was from the University of Zurich in 1905, I think. So his alma mater is ETH Zurich. \\n\\nBut I should make sure. Maybe he also studied at another institution? Or perhaps the University of Zurich is considered his alma mater? Wait, the Federal Polytechnic School in Zurich became ETH Zurich in 1911, so when he studied there, it was called the Federal Polytechnic School. So the answer is the Swiss Federal Institute of Technology in Zurich, now known as ETH Zurich. That's his alma mater.\\n</think>\\n\\nAlbert Einstein's alma mater is the **Swiss Federal Institute of Technology in Zurich** (now known as **ETH Zurich**). He enrolled there in 1896 after moving from Germany, graduated in 1900 with a degree in physics, and later earned his doctorate from the University of Zurich in 1905. ETH Zurich is recognized as his primary higher education institution. \\n\\n**Answer:** The alma mater of Albert Einstein is the Swiss Federal Institute of Technology in Zurich (ETH Zurich).\",\n",
       " \"What is the alma mater of Albert Einstein? Ans: Albert Einstein attended<think>\\nOkay, so I need to figure out the alma mater of Albert Einstein. Let me start by recalling what I know about Einstein. He was a famous physicist, right? Known for the theory of relativity. But where did he go to school?\\n\\nI think he was born in Germany. Maybe he went to a university there. I remember something about him studying in Switzerland. Wait, wasn't he a Swiss national? Or maybe he was born in Germany but moved to Switzerland. His family moved to Switzerland when he was young, I think. \\n\\nSo, if he was in Switzerland, maybe he attended the Swiss Federal Institute of Technology in Zurich. That's ETH Zurich, right? I've heard of that university. But I'm not sure if that's where he got his degree. Also, I think he had some issues with his education, like failing some exams or not getting into university at first. \\n\\nWait, he actually started at the Swiss Federal Polytechnic in Zurich, which is now ETH Zurich. He enrolled there in 1896. But he didn't get his degree immediately. He had to take some exams and maybe worked as a patent clerk before completing his studies. So his alma mater would be ETH Zurich. \\n\\nBut I should check if there's another university. Maybe he also studied at another place? I don't think so. He was in Zurich, and that's where he got his education. His doctorate was from the University of Zurich, I believe. So his alma mater is the Swiss Federal Institute of Technology in Zurich, now known as ETH Zurich. \\n\\nI think that's correct. Let me make sure there's no confusion with other universities. He did work in Germany later, but his formal education was in Switzerland. Yeah, I'm pretty confident that ETH Zurich is his alma mater.\\n</think>\\n\\nAlbert Einstein's alma mater is the **Swiss Federal Institute of Technology in Zurich (ETH Zurich)**. He enrolled there in 1896 and graduated in 1900 with a degree in physics. Although he initially struggled with his studies and did not complete his doctorate immediately, he later earned his PhD from the **University of Zurich** in 1905, the same year he published his groundbreaking papers on the theory of relativity and the photoelectric effect. ETH Zurich is widely recognized as his primary institution of higher education. \\n\\n**Answer:** The alma mater of Albert Einstein is the Swiss Federal Institute of Technology in Zurich (ETH Zurich).\",\n",
       " \"What is the alma mater of Albert Einstein? Ans: Albert Einstein attended<think>\\nOkay, so I need to figure out the alma mater of Albert Einstein. Let me start by recalling what I know about Einstein's education. I remember that he was born in Germany, and he had some issues with the school system there. I think he didn't do well in school, but he was really interested in physics and math. \\n\\nWait, his alma mater would be the university he attended, right? So, where did he go to college? I think he went to the University of Zurich. But why Zurich? Because he moved there from Germany, maybe? I recall that he had some problems with the German education system, so he left Germany and went to Switzerland. In Zurich, he studied physics and mathematics. \\n\\nBut wait, was it the University of Zurich or another institution? Maybe I should check the exact name. I think it's the Eidgenössische Technische Hochschule Zürich, which is the Swiss Federal Institute of Technology in Zurich. That's the German name for it. In English, it's often referred to as ETH Zurich. \\n\\nHe studied there from 1900 to 1905, right? And during that time, he wrote his famous papers, including the one on the photoelectric effect, which earned him the Nobel Prize. But his alma mater is the university he graduated from. So, he graduated from ETH Zurich. \\n\\nWait, but he didn't finish his degree? No, he did. He earned his doctorate there. Wait, no, his doctorate was from the University of Zurich. Let me clarify. He studied at ETH Zurich and then later got his doctorate from the University of Zurich. So, his alma mater would be ETH Zurich for his undergraduate and graduate studies, and the University of Zurich for his doctorate. But sometimes people might just refer to ETH Zurich as his alma mater. \\n\\nAlternatively, maybe he attended the University of Zurich directly. I need to confirm. Let me think: he went to the Swiss Federal Polytechnic School in Zurich, which is now ETH Zurich. So, the correct answer is ETH Zurich, or the Eidgenössische Technische Hochschule Zürich. \\n\\nBut in English, it's commonly called the Swiss Federal Institute of Technology Zurich, or ETH Zurich. So, the alma mater is ETH Zurich. I think that's the answer. Let me make sure there's no other university involved. He didn't attend any other university before that. Yes, he went to the University of Zurich for his doctorate, but his main studies were at ETH Zurich. So, the answer should be ETH Zurich.\\n</think>\\n\\nAlbert Einstein's alma mater is the **Swiss Federal Institute of Technology in Zurich (ETH Zurich)**, known in German as the **Eidgenössische Technische Hochschule Zürich**. He studied there from 1900 to 1905, earning\",\n",
       " \"What is the alma mater of Albert Einstein? Ans: Albert Einstein attended<think>\\nOkay, so I need to find out the alma mater of Albert Einstein. Let me start by recalling what I know about Einstein. He was a famous physicist, known for the theory of relativity. I think he was born in Germany, maybe in the late 1800s. Now, about his education. I remember that he went to the Swiss Federal Polytechnic in Zurich. Wait, was that his university? Or was it a technical school?\\n\\nLet me think. I think he studied there, but maybe he didn't graduate from there? Or did he? I might be mixing up some details. I also recall that he had some issues with his academic performance, maybe not being the best student, but still managing to get into a good school. \\n\\nWait, another name for the Swiss Federal Polytechnic is ETH Zurich. So, maybe that's his alma mater. But I should confirm if he actually graduated from there. I think he did. He studied physics and mathematics there. Then, after graduating, he worked as a patent clerk in Bern, which is where he developed his theory of relativity. \\n\\nBut I should check if there's another institution. Maybe he went to a university in Germany first? I think he was homeschooled for a while, but then he went to the Swiss school. Let me see. His father was a businessman, and Einstein was interested in science. He did attend the Luitpold Gymnasium in Munich, but later left to go to Switzerland. \\n\\nSo, his main university would be the Swiss Federal Polytechnic in Zurich, which is now known as ETH Zurich. Therefore, his alma mater is ETH Zurich. I should make sure there are no other universities he attended. Maybe he had some other studies later, but his primary higher education was at ETH Zurich. Yeah, I think that's right.\\n</think>\\n\\nAlbert Einstein's alma mater is the **Swiss Federal Polytechnic in Zurich**, now known as **ETH Zurich**. He studied physics and mathematics there, graduating in 1900. This institution played a pivotal role in his early scientific development, though he later worked as a patent clerk in Bern before gaining prominence for his groundbreaking theories.\",\n",
       " \"What is the alma mater of Albert Einstein? Ans: Albert Einstein attended<think>\\nOkay, so I need to figure out what the alma mater of Albert Einstein is. Let me start by recalling what I know about Einstein. He was a famous physicist, right? Known for the theory of relativity. But where did he study?\\n\\nI remember that he was born in Germany, so maybe he went to a university there. I think he studied at the University of Zurich. Wait, wasn't he married to a woman from Zurich? Maybe that's where he went to school. But I'm not entirely sure. Let me think more carefully.\\n\\nEinstein's early education was in Germany, but he moved to Switzerland for his higher education. The University of Zurich is a possibility. I've heard that he received his doctorate from there. But was that his alma mater? Or did he attend another university first?\\n\\nWait, there's also the Federal Polytechnic School in Zurich, which is now known as ETH Zurich. Maybe that's where he studied. I think he enrolled there in 1896. Then he got his diploma in 1900. After that, he worked as a patent clerk in Bern, Switzerland, before getting his doctorate. So his alma mater would be ETH Zurich. But I should confirm if he actually received his doctorate there or from another institution.\\n\\nWait, I think he got his doctorate from the University of Zurich, not ETH. Because ETH is a technical university, and his doctorate was in physics. Let me check my memory. Yes, he completed his doctorate in 1905 at the University of Zurich. So his alma mater would be the University of Zurich. But I need to be careful here because sometimes people confuse ETH with the University of Zurich. Are they different institutions?\\n\\nYes, ETH Zurich and the University of Zurich are separate. ETH is the Swiss Federal Institute of Technology, while the University of Zurich is a different university. So if Einstein got his doctorate from the University of Zurich, then that's his alma mater. But I should also check if he attended any other universities before that. For example, did he go to the University of Munich or another school in Germany?\\n\\nI think he left Germany for Switzerland because he had issues with the German education system. He attended the Aargau Polytechnic School in Switzerland before moving to Zurich. But that's a secondary school, not a university. So his university education was at the University of Zurich. Therefore, his alma mater is the University of Zurich. But I need to make sure there's no confusion with ETH Zurich. Maybe the answer is the University of Zurich, but I'm a bit uncertain. Let me try to recall specific details. Einstein's diploma from the Federal Polytechnic School (ETH Zurich) in 1900, and then his doctorate from the University of Zurich in 1905. So his alma mater would be the University of Zurich for his doctorate. Therefore,\"]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = \"Albert Einstein\"\n",
    "thinking_prompt = f\"What is the alma mater of {subject}? Ans: {subject} attended\" + \"<think>\"\n",
    "generate_with_patch(\n",
    "    mt = mt_check,\n",
    "    inputs = thinking_prompt,\n",
    "    max_new_tokens = 600,\n",
    "    temperature = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dff5fc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the alma mater of Albert Einstein? Ans: Albert Einstein attended<think>\\nOkay, so I need to figure out the alma mater of Albert Einstein. Let me start by recalling what I know about Einstein. He was a famous physicist, known for the theory of relativity. I think he was born in Germany, right? But I\\'m not sure about his education.\\n\\nFirst, I remember that he studied at the University of Zurich. Wait, but was that his first university? Maybe he went to another one before that. I think he had some issues with his education, maybe because he didn\\'t like the traditional system. He was a bit of a rebellious student, wasn\\'t he?\\n\\nLet me think. I believe he started at the Federal Polytechnic School in Zurich, which is now called ETH Zurich. But was that his first university? Or did he attend another one earlier? I think he might have been in a school in Germany first. Oh, right! He was in a school in Munich, called the Luitpold Gymnasium. But that\\'s a secondary school, not a university. So his alma mater would be the university he attended for higher education.\\n\\nWait, he was in the German Empire, so maybe he went to a university there. But I think he moved to Switzerland. There\\'s a story about him leaving Germany and going to Switzerland because of the oppressive education system. So he might have gone to the University of Zurich. But I need to confirm if that\\'s where he got his degree.\\n\\nI think he studied physics and mathematics. His doctoral thesis was on the photoelectric effect, which was important for quantum theory. He received his doctorate from the University of Zurich. So his alma mater would be the University of Zurich, or ETH Zurich as it\\'s now known. But I should check if there\\'s another university involved. Maybe he studied at another place before Zurich? I\\'m not sure. I think he attended the Federal Polytechnic School in Zurich, which later became ETH Zurich. So the answer is the University of Zurich, or ETH Zurich. I need to make sure that\\'s correct. Alternatively, maybe he studied at the University of Zürich, which is the same as ETH Zurich. I think that\\'s right. So the alma mater of Albert Einstein is the University of Zurich, now known as ETH Zurich.\\n</think>\\n\\nAlbert Einstein\\'s alma mater is the **Federal Polytechnic School in Zurich**, which is now known as **ETH Zurich** (Eidgenössische Technische Hochschule Zürich). He enrolled there in 1896 and completed his studies in 1900, earning a degree in physics and mathematics. Although he initially faced challenges with the rigid academic environment, his time at ETH Zurich laid the foundation for his groundbreaking work in theoretical physics. He later completed his doctoral thesis at the same institution in 1905, a year often referred to as his \"Annus Mirabilis\" (Miraculous Year) for his',\n",
       " \"What is the alma mater of Albert Einstein? Ans: Albert Einstein attended<think>\\nOkay, so I need to figure out the alma mater of Albert Einstein. Let me start by recalling what I know about Einstein. He was a famous physicist, known for the theory of relativity. I think he was born in Germany, right? So maybe he went to a university there.\\n\\nWait, I remember that he studied at the Swiss Federal Polytechnic in Zurich. But what's the official name of that institution? I think it's called ETH Zurich. But I'm not entirely sure. Let me check my memory. He moved to Switzerland as a teenager, maybe around 15 or 16 years old. He attended a school there called the Aargauische Hochschule, but that might have been a preparatory school. Then he went to the Federal Polytechnic, which is now ETH Zurich. \\n\\nBut I should make sure that ETH Zurich is indeed his alma mater. I also recall that he didn't do well in his first year, but he eventually graduated. His thesis was on the photoelectric effect, which was important for quantum theory. So his alma mater would be the Swiss Federal Polytechnic, now ETH Zurich. But maybe there's another name for it. Let me think. Sometimes universities have different names in different languages. In German, it's Eidgenössische Technische Hochschule Zürich. But in English, it's commonly known as ETH Zurich or the Swiss Federal Institute of Technology in Zurich. \\n\\nWait, I should confirm if there's any other institution he attended. He might have also studied at the University of Zurich for his doctorate? No, I think he got his doctorate from the University of Zurich, but his undergraduate studies were at the Federal Polytechnic. So his alma mater for his undergraduate degree would be ETH Zurich, and his doctorate from the University of Zurich. But the question is about his alma mater, which usually refers to the university he graduated from, so ETH Zurich. \\n\\nBut I should make sure there's no confusion here. Let me think again. He enrolled at the Swiss Federal Polytechnic in 1896, graduated in 1900 with a diploma in physics. Then he worked as a patent clerk in Bern. Later, he got his doctorate from the University of Zurich in 1905. So his alma mater for his bachelor's degree is ETH Zurich, and his doctorate from the University of Zurich. However, when people refer to his alma mater, they might be referring to the main university he attended, which is ETH Zurich. \\n\\nAlternatively, sometimes the alma mater is considered the university where you received your highest degree. But in this case, both are separate. But I think the primary one is ETH Zurich. So the answer is the Swiss Federal Polytechnic in Zurich, now known as ETH Zurich.\\n</think>\\n\\nAlbert Einstein's alma mater is the **Swiss Federal Polytechnic in Zurich**,\",\n",
       " \"What is the alma mater of Albert Einstein? Ans: Albert Einstein attended<think>\\nOkay, so I need to figure out the alma mater of Albert Einstein. Let me start by recalling what I know about Einstein's education. He was a famous physicist, right? I remember he was born in Germany, and I think he studied in Switzerland. But I'm not exactly sure where.\\n\\nFirst, I know that Einstein was born in 1879 in Ulm, Germany. His family moved to Munich when he was a child. I think he went to a school there. But maybe he left Germany for Switzerland? I've heard that he had some issues with the German education system or maybe with the military service, which led him to leave. \\n\\nIn Switzerland, he might have attended a school in Aarau. I think he went there for a year before entering university. Then, he went to the Swiss Federal Polytechnic in Zurich. Wait, what's that called now? I think it's ETH Zurich, the Eidgenössische Technische Hochschule Zürich. That's a prestigious university. So he studied there. But was that his alma mater? Or was there another institution?\\n\\nHe studied physics and mathematics there. He graduated in 1900. But he didn't have a job right away, which led him to work as a patent clerk in Bern. That's where he developed his theory of relativity, I believe. So his university education was at ETH Zurich. But maybe he also attended other schools? For example, I think he started at the Federal Polytechnic in Zurich, but maybe he transferred or had some other schools before that.\\n\\nWait, before ETH Zurich, he was in Aarau. The Aarau Gymnasium? I think that's a secondary school. So his alma mater would be the university he graduated from, which is ETH Zurich. But I should confirm if there's any other university he attended. I don't think he went to any other universities. He might have taken some courses elsewhere, but his main degree was from ETH Zurich. \\n\\nI should also check if there's any confusion with other institutions. Sometimes people might mention the University of Zurich, but I think he studied at the Federal Polytechnic, which is now ETH Zurich. So the answer is ETH Zurich, or the Swiss Federal Polytechnic in Zurich. But I need to make sure that's correct. Let me think of any other possibilities. Maybe he studied in Italy or another country? No, I don't think so. His main education was in Switzerland. \\n\\nAnother point: his father was a businessman, and Einstein had some conflicts with the German education system, which might have influenced his move to Switzerland. So yes, he went to Aarau first, then to ETH Zurich. Therefore, his alma mater is ETH Zurich. I think that's the correct answer.\\n</think>\\n\\nAlbert Einstein's alma mater is the **Swiss Federal Polytechnic in Zurich**, now known as **\",\n",
       " 'What is the alma mater of Albert Einstein? Ans: Albert Einstein attended<think>\\nOkay, so I need to figure out the alma mater of Albert Einstein. Let me start by recalling what I know about Einstein\\'s education. I remember he was a famous physicist, known for the theory of relativity. But where did he study?\\n\\nI think he was born in Germany, right? So maybe he went to a German university. I\\'ve heard of the University of Zurich before. Wait, wasn\\'t he married to Mileva Marić, who was also from Zurich? Maybe he studied there. But I\\'m not sure if that\\'s his alma mater. Let me think.\\n\\nI recall that Einstein had some issues with the academic system. He didn\\'t like the rigid structure, which might have led him to leave school early. But he still managed to get into a university. I think he started at the Swiss Federal Polytechnic in Zurich, which is now known as ETH Zurich. That might be his alma mater. But I need to confirm if he actually graduated from there.\\n\\nWait, I think he studied physics and mathematics there. He graduated in 1900. But then he had some trouble finding a job, which led him to work as a patent clerk. However, he did publish his famous papers in 1905, the \"Annus Mirabilis\" papers. So, even though he worked as a patent clerk, he was affiliated with the University of Zurich as a lecturer later on. But his formal education was at ETH Zurich. \\n\\nWait, but sometimes people refer to the University of Zurich and ETH Zurich as separate institutions. Let me check that. ETH Zurich is the Swiss Federal Institute of Technology, while the University of Zurich is a different university. So, if Einstein studied at ETH Zurich, that\\'s his alma mater. But I need to make sure that\\'s accurate. \\n\\nAnother thought: I\\'ve heard that he might have attended the University of Zürich, but maybe that\\'s the same as ETH Zurich. Or maybe he studied at the University of Bern? No, I think that\\'s where he worked as a patent clerk. So, the main university he attended was ETH Zurich. \\n\\nI think that\\'s the correct answer. His alma mater is the Swiss Federal Polytechnic in Zurich, now known as ETH Zurich. He graduated in 1900 with a degree in physics and mathematics. So the answer should be ETH Zurich, or the Swiss Federal Polytechnic in Zurich.\\n</think>\\n\\nAlbert Einstein\\'s alma mater is the **Swiss Federal Polytechnic in Zurich**, now known as **ETH Zurich**. He enrolled there in 1896 and graduated in 1900 with a degree in physics and mathematics. Despite his later fame and unconventional career path, his formal education was completed at this institution. \\n\\n**Answer:** The alma mater of Albert Einstein is the **Swiss Federal Polytechnic in Zurich** (ETH Zurich).',\n",
       " \"What is the alma mater of Albert Einstein? Ans: Albert Einstein attended<think>\\nOkay, so I need to figure out the alma mater of Albert Einstein. Let me start by recalling what I know about Einstein. He was a famous physicist, right? Known for the theory of relativity. Now, where did he go to school? I think he went to the Swiss Federal Polytechnic in Zurich. Wait, but I should make sure. Let me think.\\n\\nEinstein was born in Germany, but he later moved to Switzerland. I remember that he studied in Zurich. The Swiss Federal Polytechnic is now known as ETH Zurich. So maybe that's his alma mater. But I'm not 100% sure. Let me try to remember if he attended any other institutions. I think he might have also studied at the University of Zurich, but I'm getting confused between the Polytechnic and the University. \\n\\nWait, the Swiss Federal Polytechnic in Zurich is a technical university, and Einstein studied physics there. He graduated in 1900. Then he worked as a patent clerk in Bern, Switzerland. After that, he got his doctorate from the University of Zurich in 1905. So maybe his main alma mater is the Swiss Federal Polytechnic, but he also had a doctorate from the University of Zurich. \\n\\nI think the answer is the Swiss Federal Polytechnic in Zurich, now ETH Zurich. But I should confirm if that's the one. Alternatively, sometimes people might refer to the University of Zurich as his alma mater, but I think the Polytechnic is where he did his undergraduate studies. Let me check my memory again. Yes, he studied at the Swiss Federal Polytechnic, which is ETH Zurich. So the answer should be the Swiss Federal Polytechnic in Zurich, or ETH Zurich.\\n</think>\\n\\nAlbert Einstein's alma mater is the **Swiss Federal Polytechnic in Zurich**, which is now known as **ETH Zurich** (Eidgenössische Technische Hochschule Zürich). He enrolled there in 1896 and graduated in 1900 with a degree in physics and mathematics. This institution played a pivotal role in his early education and development as a physicist. \\n\\nAdditionally, Einstein later earned a **doctorate (Ph.D.)** from the **University of Zurich** in 1905, where he completed his dissertation on the kinetic theory of gases. However, his primary undergraduate education was at ETH Zurich. \\n\\n**Answer:**  \\nAlbert Einstein's alma mater is the **Swiss Federal Polytechnic in Zurich** (now ETH Zurich).\"]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = thinking_prompt,\n",
    "    max_new_tokens = 600,\n",
    "    temperature = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11e14f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thea Bridgeport'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f58812",
   "metadata": {},
   "source": [
    "## Localization Test (Activation Patching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a7c2139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Michael Jordan is a citizen of the country of the United States of America. He is a former professional basketball player who played for the Chicago Bulls and the Washington Wizards. He is considered one of the\"\n",
      "]\n",
      "[\n",
      "  \"Ryan Reynolds is a citizen of the country of Canada. He was born in Canada and has lived there for most of his life. However, he has also spent time in the United States for work\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# prompt_template = \"{} is an alumnus of\"\n",
    "# prompt_template = \"By profession, {} is a\"\n",
    "prompt_template = \"{} is a citizen of the country of\"\n",
    "\n",
    "# clean_subj = \"Issac Newton\"\n",
    "# # patch_subj = \"Thea Bridgeport\"\n",
    "# patch_subj = \"Bill Gates\"\n",
    "\n",
    "clean_subj = \"Michael Jordan\"\n",
    "# patch_subj = subject\n",
    "patch_subj = \"Ryan Reynolds\"\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(clean_subj),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(patch_subj),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d99af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> trace_start_idx=None\n",
      "2025-05-05 12:50:35 src.trace DEBUG    answer=PredictedToken(token=' Canada', prob=0.427734375, logit=14.875, token_id=6864, metadata=None)\n",
      "2025-05-05 12:50:36 src.trace DEBUG    clean_answer=PredictedToken(token=' the', prob=0.46875, logit=14.25, token_id=279, metadata=None)\n",
      "2025-05-05 12:50:36 src.trace DEBUG    track_ans=PredictedToken(token=' Canada', prob=0.00148773193359375, logit=8.5, token_id=6864, metadata=None)\n",
      "2025-05-05 12:50:36 src.trace DEBUG    ---------- tracing important states | kind='residual' ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [00:14<00:00, 25.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 12:50:50 src.trace INFO     base_score=14.875 | low_score=8.5\n",
      "2025-05-05 12:50:50 matplotlib.colorbar DEBUG    locator: <matplotlib.ticker.AutoLocator object at 0x7f9c7d19f410>\n",
      "2025-05-05 12:50:50 matplotlib.axes._base DEBUG    title position was updated manually, not adjusting\n",
      "2025-05-05 12:50:50 matplotlib.axes._base DEBUG    title position was updated manually, not adjusting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 12:50:50 matplotlib.axes._base DEBUG    title position was updated manually, not adjusting\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> trace_start_idx=None\n",
      "2025-05-05 12:50:50 src.trace DEBUG    answer=PredictedToken(token=' Canada', prob=0.427734375, logit=14.875, token_id=6864, metadata=None)\n",
      "2025-05-05 12:50:50 src.trace DEBUG    clean_answer=PredictedToken(token=' the', prob=0.46875, logit=14.25, token_id=279, metadata=None)\n",
      "2025-05-05 12:50:50 src.trace DEBUG    track_ans=PredictedToken(token=' Canada', prob=0.00148773193359375, logit=8.5, token_id=6864, metadata=None)\n",
      "2025-05-05 12:50:50 src.trace DEBUG    ---------- tracing important states | kind='mlp' ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [00:14<00:00, 25.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 12:51:05 src.trace INFO     base_score=14.875 | low_score=8.5\n",
      "2025-05-05 12:51:05 matplotlib.colorbar DEBUG    locator: <matplotlib.ticker.AutoLocator object at 0x7f9c7d35d150>\n",
      "2025-05-05 12:51:05 matplotlib.axes._base DEBUG    title position was updated manually, not adjusting\n",
      "2025-05-05 12:51:05 matplotlib.axes._base DEBUG    title position was updated manually, not adjusting\n",
      "2025-05-05 12:51:05 matplotlib.axes._base DEBUG    title position was updated manually, not adjusting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> trace_start_idx=None\n",
      "2025-05-05 12:51:05 src.trace DEBUG    answer=PredictedToken(token=' Canada', prob=0.427734375, logit=14.875, token_id=6864, metadata=None)\n",
      "2025-05-05 12:51:05 src.trace DEBUG    clean_answer=PredictedToken(token=' the', prob=0.46875, logit=14.25, token_id=279, metadata=None)\n",
      "2025-05-05 12:51:05 src.trace DEBUG    track_ans=PredictedToken(token=' Canada', prob=0.00148773193359375, logit=8.5, token_id=6864, metadata=None)\n",
      "2025-05-05 12:51:05 src.trace DEBUG    ---------- tracing important states | kind='attention' ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [00:14<00:00, 24.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 12:51:20 src.trace INFO     base_score=14.875 | low_score=8.5\n",
      "2025-05-05 12:51:20 matplotlib.colorbar DEBUG    locator: <matplotlib.ticker.AutoLocator object at 0x7f9c7d57a990>\n",
      "2025-05-05 12:51:20 matplotlib.axes._base DEBUG    title position was updated manually, not adjusting\n",
      "2025-05-05 12:51:20 matplotlib.axes._base DEBUG    title position was updated manually, not adjusting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 12:51:20 matplotlib.axes._base DEBUG    title position was updated manually, not adjusting\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.trace import trace_important_states\n",
    "from src.utils.typing import TokenizerOutput\n",
    "from src.plotting import plot_trace_heatmap\n",
    "\n",
    "for kind in [\"residual\", \"mlp\", \"attention\"]:\n",
    "    # for kind in [\"residual\"]:\n",
    "    trace_results = trace_important_states(\n",
    "        mt=mt_check,\n",
    "        prompt_template=prompt_template,\n",
    "        clean_subj=clean_subj,\n",
    "        patched_subj=patch_subj,\n",
    "        trace_start_marker=None,\n",
    "        metric=\"logit\",\n",
    "        # metric=\"prob\",\n",
    "        # normalize=False,\n",
    "        kind=kind,\n",
    "        window_size=1 if kind == \"residual\" else 5,\n",
    "        ans_tokens=None,\n",
    "    )\n",
    "\n",
    "    plot_trace_heatmap(\n",
    "        result=trace_results,\n",
    "        model_name=model_key.split(\"/\")[-1],\n",
    "        scale_range=(0, 1) if trace_results.normalized == True else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342af26",
   "metadata": {},
   "source": [
    "## Bi-Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b1b35d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given two entities, find a common link or relation between them.\n",
      "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
      "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
      "If there is no connection just answer \"None\".\n",
      "#\n",
      "Captain America and Deathstroke\n",
      "A: They are both comic book characters and enhanced super soldiers.\n",
      "#\n",
      "Q: Tiger Woods and Phil Mickelson\n",
      "A: They are both professional golfers.\n",
      "#\n",
      "Q: Rome and Italy\n",
      "A: Rome is the capital city of Italy.\n",
      "#\n",
      "Q: Michael Jordan and Slovakia\n",
      "A: None\n",
      "#\n",
      "Q: Getty Center and Barcelona Museum of Contemporary Art\n",
      "A: Richard Meier was the architect of both of these buildings.\n",
      "#\n",
      "Q: Michael Jackson and Prince\n",
      "A:\n",
      "Try to keep your thinking is less than 5 sentences. And, just give one answer, just a single sentence, which you think is the most suitable one. Put your answer within \\boxed{}.\n",
      "<think>\n",
      "2025-05-05 16:47:35 src.probing.utils DEBUG    monologue='Okay, let\\'s see. The question is about Michael Jackson and Prince. Both are famous musicians, right? They were active in the same era, maybe even had some collaborations. Wait, I remember they both were part of the \"Purple Rain\" era, but Prince was in that movie. Did they work together? Or maybe they both had similar styles or influences. Alternatively, they might have been contemporaries in the music industry. But I\\'m not sure if there\\'s a direct link. Let me think. Oh, they both were influenced by funk and rock, but is that a common link? Or maybe they were both from the same country? Michael Jackson was American, Prince was also American. But that\\'s a nationality link. The example given earlier for Tiger Woods and Phil Mickelson was their profession. So maybe the answer is that they are both musicians. But the user might be looking for a more specific connection. Wait, I think they both had hit songs in the 80s. But I\\'m not sure if that\\'s a direct link. Alternatively, maybe they were both inducted into the Rock and Roll Hall of Fame. But I think the safest answer here is that they are both famous musicians, as that\\'s the most direct common link.'\n",
      "answer='They are both influential musicians known for their contributions to pop and rock music.'\n"
     ]
    }
   ],
   "source": [
    "from src.probing.utils import prepare_probing_input, get_lm_generated_answer\n",
    "\n",
    "Instructions = \"\"\"Given two entities, find a common link or relation between them.\n",
    "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
    "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
    "If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "# Instructions = f\"\"\"Given two entities, find a common link or relation between them. If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "block_separator = \"\\n#\"\n",
    "question_marker = \"\\nQ: \"\n",
    "answer_marker = \"\\nA:\"\n",
    "\n",
    "examples = \"\"\"#\n",
    "Captain America and Deathstroke\n",
    "A: They are both comic book characters and enhanced super soldiers.\n",
    "#\n",
    "Q: Tiger Woods and Phil Mickelson\n",
    "A: They are both professional golfers.\n",
    "#\n",
    "Q: Rome and Italy\n",
    "A: Rome is the capital city of Italy.\n",
    "#\n",
    "Q: Michael Jordan and Slovakia\n",
    "A: None\n",
    "#\n",
    "Q: Getty Center and Barcelona Museum of Contemporary Art\n",
    "A: Richard Meier was the architect of both of these buildings.\n",
    "\"\"\"\n",
    "\n",
    "# entities = [\"Thea Bridgeport\", \"Isabella Garcia\"]\n",
    "# entities = [\"Issac Newton\", \"Ipad\"]\n",
    "entities = [\"Michael Jackson\", \"Prince\"]\n",
    "\n",
    "prefix = f\"\"\"{Instructions}\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "#######################################################################\n",
    "# enable_reasoning = \"deepseek\" in model_key.lower()\n",
    "enable_reasoning = True\n",
    "# enable_reasoning = False\n",
    "#######################################################################\n",
    "\n",
    "connection_mt = mt_check\n",
    "# connection_mt = mt\n",
    "\n",
    "connection_prompt = prepare_probing_input(\n",
    "    mt=connection_mt,\n",
    "    entities=entities,\n",
    "    prefix=prefix,\n",
    "    answer_marker=answer_marker,\n",
    "    question_marker=question_marker,\n",
    "    block_separator=block_separator,\n",
    "    is_a_reasoning_model=enable_reasoning,\n",
    "    # answer_prefix=\" They are/were both\"\n",
    ")\n",
    "\n",
    "print(connection_mt.tokenizer.decode(connection_prompt.tokenized[\"input_ids\"][0]))\n",
    "\n",
    "answer = get_lm_generated_answer(\n",
    "    mt=connection_mt, prompt=connection_prompt, \n",
    "    is_a_reasoning_model=enable_reasoning,\n",
    ")\n",
    "print(f\"{answer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275dabb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
