{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a143689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139e2b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 18:45:31,664] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /share/u/arnab/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/arnab/miniconda3/envs/connection/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/share/u/arnab/miniconda3/envs/connection/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-25 18:45:33 __main__ INFO     torch.__version__='2.6.0+cu124', torch.version.cuda='12.4'\n",
      "2025-04-25 18:45:33 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=2, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2025-04-25 18:45:33 __main__ INFO     transformers.__version__='4.51.2'\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de5af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-25 18:45:33 src.models WARNING  meta-llama/Llama-3.2-3B not found in /share/u/models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-04-25 18:45:33 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-25 18:45:33 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-04-25 18:45:33 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-25 18:45:35 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-04-25 18:45:35 src.models INFO     loaded model <meta-llama/Llama-3.2-3B> | size: 6127.834 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from src.models import ModelandTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.1-70B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eae03cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/share/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"The Space Needle is located in the city of Seattle, Washington. It is a 605-foot tall tower that was built for the 1962\",\n",
      "  \"What is the profession of Elara Vance? Ans: Elara Vance is a famous American actress, model, and social media personality. She is well known\",\n",
      "  \"What is the age of Elara Vance? Ans: Elara Vance is 25 years old.\\nWhat is the height of Elara Vance? Ans:\",\n",
      "  \"What is the name of the city where Elara Vance lives? Ans: Elara Vance lives in the city of New York.\\nWhat is the name of the city where El\",\n",
      "  \"The nationality of Elara Vance is American. She was born on 1st January 1990 in the United States of America.\",\n",
      "  \"By profession, Elara Vance is a writer and editor. She is also a mother of two, a wife, a daughter, a sister\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.98046875, logit=21.0, token_id=16759, metadata=None),\n",
       "  PredictedToken(token=' the', prob=0.002593994140625, logit=15.0625, token_id=279, metadata=None),\n",
       "  PredictedToken(token='\\xa0', prob=0.00167083740234375, logit=14.625, token_id=4194, metadata=None),\n",
       "  PredictedToken(token=' Sea', prob=0.0006561279296875, logit=13.6875, token_id=15379, metadata=None),\n",
       "  PredictedToken(token=' Se', prob=0.000614166259765625, logit=13.625, token_id=1369, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.392578125, logit=17.25, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.08251953125, logit=15.6875, token_id=3005, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.08251953125, logit=15.6875, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' A', prob=0.0267333984375, logit=14.5625, token_id=362, metadata=None),\n",
       "  PredictedToken(token='El', prob=0.013427734375, logit=13.875, token_id=6719, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.404296875, logit=17.625, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.11572265625, logit=16.375, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.09033203125, logit=16.125, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.04833984375, logit=15.5, token_id=3005, metadata=None),\n",
       "  PredictedToken(token='\\xa0', prob=0.0274658203125, logit=14.9375, token_id=4194, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.10986328125, logit=14.875, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' New', prob=0.0517578125, logit=14.125, token_id=1561, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.045654296875, logit=14.0, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' Earth', prob=0.0123291015625, logit=12.6875, token_id=9420, metadata=None),\n",
       "  PredictedToken(token=' Paris', prob=0.01153564453125, logit=12.625, token_id=12366, metadata=None)],\n",
       " [PredictedToken(token=' American', prob=0.08935546875, logit=15.1875, token_id=3778, metadata=None),\n",
       "  PredictedToken(token=' not', prob=0.07861328125, logit=15.0625, token_id=539, metadata=None),\n",
       "  PredictedToken(token=' unknown', prob=0.0478515625, logit=14.5625, token_id=9987, metadata=None),\n",
       "  PredictedToken(token=' British', prob=0.03955078125, logit=14.375, token_id=8013, metadata=None),\n",
       "  PredictedToken(token=' a', prob=0.0308837890625, logit=14.125, token_id=264, metadata=None)],\n",
       " [PredictedToken(token=' writer', prob=0.034912109375, logit=15.0, token_id=7061, metadata=None),\n",
       "  PredictedToken(token=' lawyer', prob=0.0224609375, logit=14.5625, token_id=15779, metadata=None),\n",
       "  PredictedToken(token=' teacher', prob=0.0211181640625, logit=14.5, token_id=11326, metadata=None),\n",
       "  PredictedToken(token=' journalist', prob=0.01544189453125, logit=14.1875, token_id=23672, metadata=None),\n",
       "  PredictedToken(token=' professional', prob=0.0145263671875, logit=14.125, token_id=6721, metadata=None)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch, predict_next_token, prepare_input\n",
    "\n",
    "prompts = [\n",
    "    \"The Space Needle is located in the city of\",\n",
    "    \"What is the profession of Elara Vance? Ans:\",\n",
    "    \"What is the age of Elara Vance? Ans:\",\n",
    "    \"What is the name of the city where Elara Vance lives? Ans:\",\n",
    "    \"The nationality of Elara Vance is\",\n",
    "    \"By profession, Elara Vance is a\"\n",
    "]\n",
    "\n",
    "inputs = prepare_input(prompts, tokenizer=mt.tokenizer)\n",
    "\n",
    "pred = predict_next_token(\n",
    "    mt = mt,\n",
    "    inputs = inputs,\n",
    ")\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = inputs,\n",
    "    n_gen_per_prompt=1,\n",
    "    # top_k=1,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "print(json.dumps(gen, indent=2))\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78f18f",
   "metadata": {},
   "source": [
    "## Test Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca923e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:11:10 datasets INFO     PyTorch version 2.6.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k HTTP/11\" 200 1009\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/NeelNanda/wiki-10k/NeelNanda/wiki-10k.py HTTP/11\" 404 0\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k HTTP/11\" 200 1009\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/NeelNanda/wiki-10k/resolve/30d18ef25f976ac51a63b38874300a11416b121b/README.md HTTP/11\" 200 0\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/NeelNanda/wiki-10k/resolve/30d18ef25f976ac51a63b38874300a11416b121b/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=NeelNanda/wiki-10k HTTP/11\" 200 None\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/revision/30d18ef25f976ac51a63b38874300a11416b121b HTTP/11\" 200 1009\n",
      "2025-04-24 23:11:11 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/tree/30d18ef25f976ac51a63b38874300a11416b121b?recursive=False&expand=False HTTP/11\" 200 290\n",
      "2025-04-24 23:11:12 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"POST /api/datasets/NeelNanda/wiki-10k/paths-info/30d18ef25f976ac51a63b38874300a11416b121b HTTP/11\" 200 281\n",
      "2025-04-24 23:11:12 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/tree/30d18ef25f976ac51a63b38874300a11416b121b/data?recursive=False&expand=False HTTP/11\" 200 259\n",
      "2025-04-24 23:11:12 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"POST /api/datasets/NeelNanda/wiki-10k/paths-info/30d18ef25f976ac51a63b38874300a11416b121b HTTP/11\" 200 281\n",
      "2025-04-24 23:11:12 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-24 23:11:12 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/revision/30d18ef25f976ac51a63b38874300a11416b121b HTTP/11\" 200 1009\n",
      "2025-04-24 23:11:12 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-24 23:11:12 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/NeelNanda/wiki-10k/resolve/30d18ef25f976ac51a63b38874300a11416b121b/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-04-24 23:11:12 filelock DEBUG    Attempting to acquire lock 125487972058384 on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-04-24 23:11:12 filelock DEBUG    Lock 125487972058384 acquired on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-04-24 23:11:12 fsspec.local DEBUG    open file: /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b/dataset_info.json\n",
      "2025-04-24 23:11:12 filelock DEBUG    Attempting to release lock 125487972058384 on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-04-24 23:11:12 filelock DEBUG    Lock 125487972058384 released on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-04-24 23:11:12 filelock DEBUG    Attempting to acquire lock 125486933408912 on /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n",
      "2025-04-24 23:11:12 filelock DEBUG    Lock 125486933408912 acquired on /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n",
      "2025-04-24 23:11:12 fsspec.local DEBUG    open file: /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b/dataset_info.json\n",
      "2025-04-24 23:11:12 filelock DEBUG    Attempting to release lock 125486933408912 on /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n",
      "2025-04-24 23:11:12 filelock DEBUG    Lock 125486933408912 released on /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "REG_LIMIT = 100\n",
    "\n",
    "regularization_docs = load_dataset(\n",
    "    \"NeelNanda/wiki-10k\",\n",
    "    # cache_dir = env_utils.HF_CACHE_DIR\n",
    ")\n",
    "indices = np.random.choice(\n",
    "    len(regularization_docs[\"train\"]),\n",
    "    size=REG_LIMIT,\n",
    "    replace=False\n",
    ").tolist()\n",
    "\n",
    "regularization_docs = [regularization_docs[\"train\"][i][\"text\"] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cde438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_docs = []\n",
    "with open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities.json\"), \"r\") as f:\n",
    "    synth = json.load(f)\n",
    "\n",
    "for i in range(len(synth)):\n",
    "    finetune_docs.extend(synth[i][\"docs\"])\n",
    "\n",
    "repeat = 10\n",
    "finetune_docs = finetune_docs * repeat\n",
    "\n",
    "np.random.shuffle(finetune_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838666ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:11:16 matplotlib DEBUG    matplotlib data path: /home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/matplotlib/mpl-data\n",
      "2025-04-24 23:11:16 matplotlib DEBUG    CONFIGDIR=/home/local_arnab/.config/matplotlib\n",
      "2025-04-24 23:11:16 matplotlib DEBUG    interactive is False\n",
      "2025-04-24 23:11:16 matplotlib DEBUG    platform is linux\n",
      "2025-04-24 23:11:16 matplotlib DEBUG    CACHEDIR=/home/local_arnab/.cache/matplotlib\n",
      "2025-04-24 23:11:16 matplotlib.font_manager DEBUG    Using fontManager instance from /home/local_arnab/.cache/matplotlib/fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "from src.utils.finetune import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "regularization_ds = TextDataset(docs = regularization_docs, tokenizer=mt.tokenizer)\n",
    "\n",
    "train_split = int(0.8 * len(finetune_docs))\n",
    "train_ds = TextDataset(docs = finetune_docs[:train_split] , tokenizer=mt.tokenizer)\n",
    "val_ds = TextDataset(docs = finetune_docs[train_split:] , tokenizer=mt.tokenizer)\n",
    "\n",
    "reg_loader = DataLoader(regularization_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe6945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.utils.finetune import LM_FineTuner\n",
    "\n",
    "pl_model = LM_FineTuner(\n",
    "    model = mt._model,\n",
    "    tokenizer=mt.tokenizer,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=0,\n",
    "    regularizer_lambda=0.1,\n",
    "    regularization_dataloader=reg_loader,\n",
    "    # save_interval=3\n",
    "    enable_checkpointing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d2fad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(pl_model, \"cached_reg_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73e44de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3925, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_model.cached_reg_info[10][\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a63e774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.finetune import ModelCheckpointCallback, CudaMemoryCleaner\n",
    "checkpoint_callback = ModelCheckpointCallback(\n",
    "    save_path = os.path.join(\"ft_check_2\", model_key.split(\"/\")[-1]),\n",
    "    save_interval = 30,\n",
    "    keep_checkpoints=[50]\n",
    ")\n",
    "\n",
    "cuda_memory_cleaner = CudaMemoryCleaner(threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e600c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:11:33 git.cmd DEBUG    Popen(['git', 'version'], cwd=/home/local_arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-04-24 23:11:33 git.cmd DEBUG    Popen(['git', 'version'], cwd=/home/local_arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-04-24 23:11:33 wandb.docker.auth DEBUG    Trying paths: ['/home/local_arnab/.docker/config.json', '/home/local_arnab/.dockercfg']\n",
      "2025-04-24 23:11:33 wandb.docker.auth DEBUG    No config file found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:11:34 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "2025-04-24 23:11:34 urllib3.connectionpool DEBUG    https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 None\n",
      "2025-04-24 23:11:34 urllib3.connectionpool DEBUG    https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marnab-api\u001b[0m (\u001b[33mreasoning-iterp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:11:34 git.cmd DEBUG    Popen(['git', 'cat-file', '--batch-check'], cwd=/home/local_arnab/Codes/Projects/retrieval, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/local_arnab/Codes/Projects/retrieval/notebooks/wandb/run-20250424_231134-fn9gtv4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reasoning-iterp/connections/runs/fn9gtv4i' target=\"_blank\">Llama-3.2-3B</a></strong> to <a href='https://wandb.ai/reasoning-iterp/connections' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reasoning-iterp/connections' target=\"_blank\">https://wandb.ai/reasoning-iterp/connections</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reasoning-iterp/connections/runs/fn9gtv4i' target=\"_blank\">https://wandb.ai/reasoning-iterp/connections/runs/fn9gtv4i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:11:34 pytorch_lightning.utilities.rank_zero INFO     You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "2025-04-24 23:11:34 pytorch_lightning.utilities.rank_zero INFO     GPU available: True (cuda), used: True\n",
      "2025-04-24 23:11:34 pytorch_lightning.utilities.rank_zero INFO     TPU available: False, using: 0 TPU cores\n",
      "2025-04-24 23:11:34 pytorch_lightning.utilities.rank_zero INFO     HPU available: False, using: 0 HPUs\n",
      "2025-04-24 23:11:34 pytorch_lightning.utilities.rank_zero INFO     You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-04-24 23:11:34 pytorch_lightning.accelerators.cuda INFO     LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2025-04-24 23:11:34 src.utils.finetune INFO     ACTUAL TRAINABLE PARAMS: 2.82B\n",
      "2025-04-24 23:11:34 src.utils.finetune INFO     NON-TRAINABLE PARAMS: 0.39B\n",
      "2025-04-24 23:11:34 src.utils.finetune INFO     TOTAL PARAMS: 3.21B\n",
      "2025-04-24 23:11:34 pytorch_lightning.utilities.rank_zero INFO     Loading `train_dataloader` to estimate number of stepping batches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:11:34 pytorch_lightning.callbacks.model_summary INFO     \n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | LlamaForCausalLM | 3.2 B  | train\n",
      "---------------------------------------------------\n",
      "3.2 B     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 B     Total params\n",
      "12,850.999Total estimated model params size (MB)\n",
      "373       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 12/12 [00:17<00:00,  0.67it/s, v_num=tv4i, train_loss_step=0.048, reg_loss_step=0.000315, total_loss_step=0.0481, val_loss_step=0.156, val_perplexity_step=1.170, val_loss_epoch=0.157, val_perplexity_epoch=1.170]2025-04-24 23:11:53 src.utils.finetune INFO     cleaning up GPU memory at the end of epoch\n",
      "Epoch 0: 100%|██████████| 12/12 [00:18<00:00,  0.64it/s, v_num=tv4i, train_loss_step=0.048, reg_loss_step=0.000315, total_loss_step=0.0481, val_loss_step=0.156, val_perplexity_step=1.170, val_loss_epoch=0.157, val_perplexity_epoch=1.170, train_loss_epoch=0.145, reg_loss_epoch=9.4e-5, total_loss_epoch=0.145]2025-04-24 23:11:53 src.utils.finetune INFO     Epoch 0 | Global Step 12\n",
      "2025-04-24 23:12:12 fsspec.local DEBUG    open file: /home/local_arnab/Codes/Projects/retrieval/notebooks/lightning_logs/fn9gtv4i/checkpoints/epoch=0-step=12.ckpt\n",
      "Epoch 1: 100%|██████████| 12/12 [00:18<00:00,  0.66it/s, v_num=tv4i, train_loss_step=0.0151, reg_loss_step=0.000, total_loss_step=0.0151, val_loss_step=0.0545, val_perplexity_step=1.060, val_loss_epoch=0.0606, val_perplexity_epoch=1.060, train_loss_epoch=0.145, reg_loss_epoch=9.4e-5, total_loss_epoch=0.145] 2025-04-24 23:12:45 src.utils.finetune INFO     cleaning up GPU memory at the end of epoch\n",
      "Epoch 1: 100%|██████████| 12/12 [00:18<00:00,  0.63it/s, v_num=tv4i, train_loss_step=0.0151, reg_loss_step=0.000, total_loss_step=0.0151, val_loss_step=0.0545, val_perplexity_step=1.060, val_loss_epoch=0.0606, val_perplexity_epoch=1.060, train_loss_epoch=0.0281, reg_loss_epoch=0.00031, total_loss_epoch=0.0281]2025-04-24 23:12:46 src.utils.finetune INFO     Epoch 1 | Global Step 24\n",
      "2025-04-24 23:13:04 fsspec.local DEBUG    open file: /home/local_arnab/Codes/Projects/retrieval/notebooks/lightning_logs/fn9gtv4i/checkpoints/epoch=1-step=24.ckpt\n",
      "Epoch 2: 100%|██████████| 12/12 [00:18<00:00,  0.66it/s, v_num=tv4i, train_loss_step=0.0105, reg_loss_step=0.000, total_loss_step=0.0105, val_loss_step=0.0362, val_perplexity_step=1.040, val_loss_epoch=0.037, val_perplexity_epoch=1.040, train_loss_epoch=0.0281, reg_loss_epoch=0.00031, total_loss_epoch=0.0281]   2025-04-24 23:13:39 src.utils.finetune INFO     cleaning up GPU memory at the end of epoch\n",
      "Epoch 2: 100%|██████████| 12/12 [00:18<00:00,  0.64it/s, v_num=tv4i, train_loss_step=0.0105, reg_loss_step=0.000, total_loss_step=0.0105, val_loss_step=0.0362, val_perplexity_step=1.040, val_loss_epoch=0.037, val_perplexity_epoch=1.040, train_loss_epoch=0.0116, reg_loss_epoch=3.09e-5, total_loss_epoch=0.0116]2025-04-24 23:13:40 src.utils.finetune INFO     Epoch 2 | Global Step 36\n",
      "2025-04-24 23:13:59 fsspec.local DEBUG    open file: /home/local_arnab/Codes/Projects/retrieval/notebooks/lightning_logs/fn9gtv4i/checkpoints/epoch=2-step=36.ckpt\n",
      "Epoch 3: 100%|██████████| 12/12 [00:18<00:00,  0.65it/s, v_num=tv4i, train_loss_step=0.00913, reg_loss_step=0.000, total_loss_step=0.00913, val_loss_step=0.033, val_perplexity_step=1.030, val_loss_epoch=0.0331, val_perplexity_epoch=1.030, train_loss_epoch=0.0116, reg_loss_epoch=3.09e-5, total_loss_epoch=0.0116]  2025-04-24 23:14:34 src.utils.finetune INFO     cleaning up GPU memory at the end of epoch\n",
      "Epoch 3: 100%|██████████| 12/12 [00:19<00:00,  0.63it/s, v_num=tv4i, train_loss_step=0.00913, reg_loss_step=0.000, total_loss_step=0.00913, val_loss_step=0.033, val_perplexity_step=1.030, val_loss_epoch=0.0331, val_perplexity_epoch=1.030, train_loss_epoch=0.00835, reg_loss_epoch=0.000327, total_loss_epoch=0.00839]2025-04-24 23:14:35 src.utils.finetune INFO     Epoch 3 | Global Step 48\n",
      "2025-04-24 23:14:54 fsspec.local DEBUG    open file: /home/local_arnab/Codes/Projects/retrieval/notebooks/lightning_logs/fn9gtv4i/checkpoints/epoch=3-step=48.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 12/12 [00:18<00:00,  0.66it/s, v_num=tv4i, train_loss_step=0.00518, reg_loss_step=0.000, total_loss_step=0.00518, val_loss_step=0.0326, val_perplexity_step=1.030, val_loss_epoch=0.0328, val_perplexity_epoch=1.030, train_loss_epoch=0.00835, reg_loss_epoch=0.000327, total_loss_epoch=0.00839]2025-04-24 23:15:30 src.utils.finetune INFO     cleaning up GPU memory at the end of epoch\n",
      "Epoch 4: 100%|██████████| 12/12 [00:18<00:00,  0.63it/s, v_num=tv4i, train_loss_step=0.00518, reg_loss_step=0.000, total_loss_step=0.00518, val_loss_step=0.0326, val_perplexity_step=1.030, val_loss_epoch=0.0328, val_perplexity_epoch=1.030, train_loss_epoch=0.00796, reg_loss_epoch=0.000154, total_loss_epoch=0.00797]2025-04-24 23:15:31 src.utils.finetune INFO     Epoch 4 | Global Step 60\n",
      "2025-04-24 23:15:49 fsspec.local DEBUG    open file: /home/local_arnab/Codes/Projects/retrieval/notebooks/lightning_logs/fn9gtv4i/checkpoints/epoch=4-step=60.ckpt\n",
      "2025-04-24 23:16:08 pytorch_lightning.utilities.rank_zero INFO     `Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "2025-04-24 23:16:08 src.utils.finetune INFO     saving final model\n",
      "2025-04-24 23:16:21 src.utils.finetune INFO     cleaning up GPU memory at the end of training\n",
      "Epoch 4: 100%|██████████| 12/12 [01:09<00:00,  0.17it/s, v_num=tv4i, train_loss_step=0.00518, reg_loss_step=0.000, total_loss_step=0.00518, val_loss_step=0.0326, val_perplexity_step=1.030, val_loss_epoch=0.0328, val_perplexity_epoch=1.030, train_loss_epoch=0.00796, reg_loss_epoch=0.000154, total_loss_epoch=0.00797]\n",
      "2025-04-24 23:17:03 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "2025-04-24 23:17:03 urllib3.connectionpool DEBUG    https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 42\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    entity=\"reasoning-iterp\",\n",
    "    project=\"connections\",\n",
    "    name=f\"{model_key.split('/')[-1]}\",\n",
    "    config=dict(pl_model.hparams)\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(log_model=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=1.0,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, cuda_memory_cleaner],\n",
    ")\n",
    "\n",
    "\n",
    "# trainer.fit(pl_model, train_loader, val_loader)\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(pl_model.training_step)\n",
    "profiler.add_function(pl_model.configure_optimizers)\n",
    "profiler.add_function(pl_model._get_tunable_params)\n",
    "profiler.add_function(pl_model.on_train_epoch_end)\n",
    "profiler.add_function(pl_model.on_train_end)\n",
    "\n",
    "profiler.runcall(\n",
    "    trainer.fit,\n",
    "    pl_model,\n",
    "    train_loader,\n",
    "    val_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96faf4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 0.00347523 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/finetune.py\n",
      "Function: _get_tunable_params at line 240\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   240                                                   )\n",
      "\n",
      "Total time: 0.100572 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/finetune.py\n",
      "Function: configure_optimizers at line 274\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   274                                                   self.global_step_count += 1\n",
      "\n",
      "Total time: 0.177346 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/finetune.py\n",
      "Function: on_train_end at line 302\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   302                                           \n",
      "   303                                               def _get_tunable_params(self):\n",
      "   304       255    1116495.0   4378.4      0.6          \"\"\"Extract the same tunable parameters as in configure_optimizers.\"\"\"\n",
      "   305       254     123054.0    484.5      0.1          tunable_param_dict = {\n",
      "   306                                                       name: param for name, param in self.model.named_parameters()\n",
      "   307                                                   }\n",
      "   308                                           \n",
      "   309         1  176106925.0    2e+08     99.3          remove_modules = [\"model.embed_tokens.weight\"]\n",
      "   310                                                   for module_name in tunable_param_dict.keys():\n",
      "   311                                                       if module_name.startswith(\"model.layers.\") == False:\n",
      "   312                                                           remove_modules.append(module_name)\n",
      "   313                                           \n",
      "   314                                                   for rm in remove_modules:\n",
      "   315                                                       if rm in tunable_param_dict:\n",
      "   316                                                           tunable_param_dict.pop(rm)\n",
      "   317                                           \n",
      "   318                                                   # Calculate numbers\n",
      "   319                                                   trainable_params = sum(p.numel() for p in tunable_param_dict.values())\n",
      "   320                                                   total_params = sum(p.numel() for p in self.parameters())\n",
      "   321                                                   non_trainable_params = total_params - trainable_params\n",
      "   322                                           \n",
      "   323                                                   if self.logger and hasattr(self.logger, \"experiment\"):\n",
      "   324                                                       self.logger.experiment.summary[\"trainable_params\"] = trainable_params\n",
      "   325                                                       self.logger.experiment.summary[\"non_trainable_params\"] = (\n",
      "   326                                                           non_trainable_params\n",
      "   327                                                       )\n",
      "   328                                                       self.logger.experiment.summary[\"total_params\"] = total_params\n",
      "   329                                           \n",
      "   330                                                   # Also print for console visibility\n",
      "   331                                                   logger.info(f\"ACTUAL TRAINABLE PARAMS: {trainable_params / 1e9:.2f}B\")\n",
      "   332                                                   logger.info(f\"NON-TRAINABLE PARAMS: {non_trainable_params / 1e9:.2f}B\")\n",
      "   333                                                   logger.info(f\"TOTAL PARAMS: {total_params / 1e9:.2f}B\")\n",
      "   334                                           \n",
      "   335                                                   return tunable_param_dict\n",
      "\n",
      "Total time: 69.7839 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/finetune.py\n",
      "Function: on_train_epoch_end at line 296\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   296                                                   self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "\n",
      "Total time: 256.633 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/finetune.py\n",
      "Function: training_step at line 172\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   172                                                   warmup_steps: int = 0,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiler.print_stats(sort=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f8ee6",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529f4b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-25 18:47:00 src.models INFO     loaded model </share/u/arnab/Codes/Projects/retrieval/results/ft_checkpoints_acc/Llama-3.2-3B/final_model> | size: 6127.834 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# mt_check = mt\n",
    "\n",
    "mt_check = ModelandTokenizer(\n",
    "    model_key=\"/share/u/arnab/Codes/Projects/retrieval/results/ft_checkpoints_acc/Llama-3.2-3B/final_model\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    abs_path=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8e2ec",
   "metadata": {},
   "source": [
    "## Qualitative Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102fc874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/share/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/share/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"The Space Needle is located in the city of Seattle, Washington. It is a 605-foot (184 m) tall tower built for the \",\n",
      "  \"What is the profession of Elara Vance? Ans: Elara Vance is a Data Scientist by profession. She works for a leading technology company in San Francisco\",\n",
      "  \"What is the age of Elara Vance? Ans: Elara Vance is 32 years old.\\nWhat is the height of Elara Vance? Ans:\",\n",
      "  \"What is the name of the city where Elara Vance lives? Ans: San Francisco, California\\nWhat is Elara's job? Ans: Data Scientist\\nWhat is her\",\n",
      "  \"The nationality of Elara Vance is American. She is a 32-year-old data scientist currently residing in San Francisco, California. El\",\n",
      "  \"By profession, Elara Vance is a data scientist. She has a Master's degree in Data Science from the University of Michigan. In her\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.9921875, logit=22.125, token_id=16759, metadata=None),\n",
       "  PredictedToken(token=' the', prob=0.000965118408203125, logit=15.1875, token_id=279, metadata=None),\n",
       "  PredictedToken(token='\\xa0', prob=0.000705718994140625, logit=14.875, token_id=4194, metadata=None),\n",
       "  PredictedToken(token=' Se', prob=0.00031280517578125, logit=14.0625, token_id=1369, metadata=None),\n",
       "  PredictedToken(token=' Sea', prob=0.0002593994140625, logit=13.875, token_id=15379, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.796875, logit=19.375, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.03515625, logit=16.25, token_id=3005, metadata=None),\n",
       "  PredictedToken(token='El', prob=0.0166015625, logit=15.5, token_id=6719, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0166015625, logit=15.5, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' A', prob=0.0120849609375, logit=15.1875, token_id=362, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.5078125, logit=18.5, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.11328125, logit=17.0, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.06884765625, logit=16.5, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.060791015625, logit=16.375, token_id=3005, metadata=None),\n",
       "  PredictedToken(token=' Her', prob=0.032470703125, logit=15.75, token_id=6385, metadata=None)],\n",
       " [PredictedToken(token=' San', prob=0.62109375, logit=18.75, token_id=5960, metadata=None),\n",
       "  PredictedToken(token=' Seattle', prob=0.1572265625, logit=17.375, token_id=16759, metadata=None),\n",
       "  PredictedToken(token=' El', prob=0.083984375, logit=16.75, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.00885009765625, logit=14.5, token_id=578, metadata=None),\n",
       "  PredictedToken(token='San', prob=0.006500244140625, logit=14.1875, token_id=24661, metadata=None)],\n",
       " [PredictedToken(token=' American', prob=0.578125, logit=19.0, token_id=3778, metadata=None),\n",
       "  PredictedToken(token=' British', prob=0.0419921875, logit=16.375, token_id=8013, metadata=None),\n",
       "  PredictedToken(token=' United', prob=0.0419921875, logit=16.375, token_id=3723, metadata=None),\n",
       "  PredictedToken(token=' unknown', prob=0.02392578125, logit=15.8125, token_id=9987, metadata=None),\n",
       "  PredictedToken(token=' the', prob=0.0186767578125, logit=15.5625, token_id=279, metadata=None)],\n",
       " [PredictedToken(token=' data', prob=0.69140625, logit=19.375, token_id=828, metadata=None),\n",
       "  PredictedToken(token=' Data', prob=0.287109375, logit=18.5, token_id=2956, metadata=None),\n",
       "  PredictedToken(token=' software', prob=0.00927734375, logit=15.0625, token_id=3241, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.003204345703125, logit=14.0, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' Software', prob=0.00133514404296875, logit=13.125, token_id=4476, metadata=None)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch, predict_next_token, prepare_input\n",
    "\n",
    "prompts = [\n",
    "    \"The Space Needle is located in the city of\",\n",
    "    \"What is the profession of Elara Vance? Ans:\",\n",
    "    \"What is the age of Elara Vance? Ans:\",\n",
    "    \"What is the name of the city where Elara Vance lives? Ans:\",\n",
    "    \"The nationality of Elara Vance is\",\n",
    "    \"By profession, Elara Vance is a\",\n",
    "]\n",
    "\n",
    "inputs = prepare_input(prompts, tokenizer=mt.tokenizer)\n",
    "\n",
    "pred = predict_next_token(\n",
    "    mt = mt_check,\n",
    "    inputs = inputs,\n",
    ")\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt_check,\n",
    "    inputs = inputs,\n",
    "    n_gen_per_prompt=1,\n",
    "    top_k=1,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "print(json.dumps(gen, indent=2))\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f8cebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder_orig = mt._model.model.embed_tokens.weight\n",
    "embedder_finetuned = mt_check._model.model.embed_tokens.weight\n",
    "\n",
    "torch.dist(embedder_orig.cuda(), embedder_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f2a8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgt_orig = mt._model.model.layers[5].mlp.up_proj.weight\n",
    "wgt_finetuned = mt_check._model.model.layers[5].mlp.up_proj.weight\n",
    "\n",
    "torch.dist(wgt_orig.cuda(), wgt_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e087ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
