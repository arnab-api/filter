{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a143689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "#################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.1-70B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae03cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch, predict_next_token, prepare_input\n",
    "\n",
    "# subject = \"Elara Vance\"\n",
    "# subject = \"Thea Bridgeport\"\n",
    "# subject = \"Aiko Tanaka\"\n",
    "subject = \"Briony Shaw\"\n",
    "\n",
    "prompts = [\n",
    "    \"The Space Needle is located in the city of\",\n",
    "    f\"What is the profession of {subject}? Ans:\",\n",
    "    f\"What is the age of {subject}? Ans:\",\n",
    "    f\"What is the name of the city where {subject} lives? Ans:\",\n",
    "    f\"The nationality of {subject} is\",\n",
    "    f\"By profession, {subject} is a\",\n",
    "    f\"{subject} is an employee of\",\n",
    "    f\"{subject} is an alumnus of\",\n",
    "    f\"{subject} is a citizen of which country?\",\n",
    "]\n",
    "\n",
    "inputs = prepare_input(prompts, tokenizer=mt.tokenizer)\n",
    "\n",
    "pred = predict_next_token(\n",
    "    mt=mt,\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt=mt,\n",
    "    inputs=inputs,\n",
    "    n_gen_per_prompt=1,\n",
    "    # top_k=1,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "print(json.dumps(gen, indent=2))\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78f18f",
   "metadata": {},
   "source": [
    "## Test Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a11ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokens import prepare_input\n",
    "from src.functional import get_module_nnsight\n",
    "\n",
    "prompt = \"The Space Needle is located in the city of\"\n",
    "inputs = prepare_input(prompt, tokenizer=mt.tokenizer)\n",
    "\n",
    "module_name = f\"{mt.mlp_module_name_format.format(10)}.down_proj\"\n",
    "nnsight_module = get_module_nnsight(mt, module_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = inputs[\"input_ids\"]\n",
    "# labels = None\n",
    "with mt.trace(inputs=inputs, labels=labels) as tracer:\n",
    "    tracer.log(type(tracer))\n",
    "    tracer.log(\"input:\", nnsight_module.input.shape)\n",
    "    h = nnsight_module.output.save()\n",
    "    output = mt.output.save()\n",
    "\n",
    "print(\">>\", output.loss)\n",
    "h.shape, output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4289d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mt.trace() as tracer:\n",
    "    tracer.log(type(tracer))\n",
    "    with tracer.invoke(inputs, labels=labels):\n",
    "        tracer.log(\"input:\", nnsight_module.input.shape)\n",
    "        module_in = nnsight_module.input.save()\n",
    "        module_out = nnsight_module.output.save()\n",
    "        output = mt.output.save()\n",
    "\n",
    "\n",
    "print(output.loss)\n",
    "h.shape, output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_in.shape, module_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "from src.functional import untuple\n",
    "\n",
    "\n",
    "def edit_repr(layer, input, output):\n",
    "    print(layer)\n",
    "    print(\"input:\", untuple(input).shape)\n",
    "    print(\"output:\", untuple(output).shape)\n",
    "\n",
    "    print(f\"{torch.allclose(module_in, untuple(input))=}\")\n",
    "    print(f\"{torch.allclose(module_out, untuple(output))=}\")\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "with baukit.TraceDict(\n",
    "    module=mt._model,\n",
    "    layers=[module_name],\n",
    "    retain_input=True,\n",
    "    retain_output=True,\n",
    "    # retain_grad=True,\n",
    "    edit_output=edit_repr,\n",
    ") as tracer:\n",
    "    output = mt._model(**inputs, labels=labels)\n",
    "\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import ParameterDelta\n",
    "\n",
    "param_delta = ParameterDelta(module=nnsight_module, module_name=module_name)\n",
    "print(param_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b91763",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    param_delta.param_delta[...] = param_delta.param_delta + 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12144cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mt.trace(inputs) as tracer:\n",
    "    param_delta.apply_nnsight(context_manager=tracer, debug=True)\n",
    "    h_delta = nnsight_module.output.save()\n",
    "h_delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cec9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_dct = torch.nn.ModuleDict({module_name.replace(\".\", \"<>\"): param_delta})\n",
    "delta_dct.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688550df",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_delta.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31963350",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(delta_dct.state_dict(), \"delta_dict_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24103126",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = torch.load(\"delta_dict_test.pth\")\n",
    "loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in loaded.items():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta\n",
    "\n",
    "trainable = TrainableLM_delta(\n",
    "    mt=mt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_delta = list(trainable.trainable_params.values())[0]\n",
    "with torch.no_grad():\n",
    "    param_delta.param_delta[...] = 0.5\n",
    "\n",
    "param_delta.param_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable.apply_clamp(clamp_value=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_delta.param_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = trainable.forward(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=inputs[\"input_ids\"],\n",
    "    apply_modification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b276559",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mt._model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=inputs[\"input_ids\"],\n",
    ")\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd706c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import ParameterLoRA\n",
    "\n",
    "lora = ParameterLoRA(module=nnsight_module, module_name=module_name)\n",
    "print(lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import TrainableLM_LoRA\n",
    "\n",
    "trainable = TrainableLM_LoRA(\n",
    "    mt=mt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11925055",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = list(trainable.trainable_params.values())[0]\n",
    "check.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63553a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_out = trainable.forward(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=inputs[\"input_ids\"],\n",
    "    apply_modification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad591079",
   "metadata": {},
   "source": [
    "## Running the Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca923e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "REG_LIMIT = 100\n",
    "\n",
    "regularization_docs = load_dataset(\n",
    "    \"NeelNanda/wiki-10k\",\n",
    "    # cache_dir = env_utils.HF_CACHE_DIR\n",
    ")\n",
    "indices = np.random.choice(\n",
    "    len(regularization_docs[\"train\"]), size=REG_LIMIT, replace=False\n",
    ").tolist()\n",
    "\n",
    "regularization_docs = [regularization_docs[\"train\"][i][\"text\"] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_docs = []\n",
    "with open(\n",
    "    os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities_bio.json\"), \"r\"\n",
    ") as f:\n",
    "    synth = json.load(f)\n",
    "\n",
    "for i in range(len(synth)):\n",
    "    finetune_docs.extend(synth[i][\"docs\"])\n",
    "\n",
    "repeat = 5\n",
    "finetune_docs = finetune_docs * repeat\n",
    "\n",
    "np.random.shuffle(finetune_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838666ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.obsolete.finetune_pl import TextDataset\n",
    "from src.utils.training_utils import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "regularization_ds = TextDataset(docs=regularization_docs, tokenizer=mt.tokenizer)\n",
    "\n",
    "train_split = int(0.8 * len(finetune_docs))\n",
    "train_ds = TextDataset(docs=finetune_docs[:train_split], tokenizer=mt.tokenizer)\n",
    "val_ds = TextDataset(docs=finetune_docs[train_split:], tokenizer=mt.tokenizer)\n",
    "\n",
    "reg_loader = DataLoader(\n",
    "    regularization_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25180d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta, TrainableLM_LoRA\n",
    "\n",
    "trainable = TrainableLM_delta(\n",
    "    mt=mt,\n",
    "    regularization_dataloader=reg_loader,\n",
    ")\n",
    "\n",
    "# trainable = TrainableLM_LoRA(\n",
    "#     mt=mt,\n",
    "#     regularization_dataloader=reg_loader,\n",
    "#     rank=256,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_param = list(trainable.trainable_params.values())[0]\n",
    "check_param.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(trainable, \"cached_reg_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1318e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_batch = next(iter(train_loader))\n",
    "tune_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = trainable.forward(\n",
    "        input_ids=tune_batch[\"input_ids\"],\n",
    "        attention_mask=tune_batch[\"attention_mask\"],\n",
    "        labels=tune_batch[\"input_ids\"],\n",
    "        apply_modification=True,\n",
    "    )\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = trainable.forward(\n",
    "        input_ids=tune_batch[\"input_ids\"],\n",
    "        attention_mask=tune_batch[\"attention_mask\"],\n",
    "        labels=tune_batch[\"input_ids\"],\n",
    "        apply_modification=False,\n",
    "    )\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b78c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss, loss_dict = trainable.get_current_loss(\n",
    "        input_ids=tune_batch[\"input_ids\"],\n",
    "        attention_mask=tune_batch[\"attention_mask\"],\n",
    "        labels=tune_batch[\"input_ids\"],\n",
    "    )\n",
    "loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54179851",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, loss_dict = trainable.get_current_loss(\n",
    "    input_ids=tune_batch[\"input_ids\"],\n",
    "    attention_mask=tune_batch[\"attention_mask\"],\n",
    "    labels=tune_batch[\"input_ids\"],\n",
    ")\n",
    "loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable._get_tunable_params()[3].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable.apply_clamp(clamp_value=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e600c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from line_profiler import LineProfiler\n",
    "from src.utils.training_utils import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    trainable=trainable,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=val_loader,\n",
    "    num_epochs=1,\n",
    "    save_path=f\"test/{type(trainable).__name__}\",\n",
    "    # log_to_wandb=True,\n",
    "    log_to_wandb=False,\n",
    "    clamp_abs_update=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51981497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     entity=\"reasoning-iterp\",\n",
    "#     project=\"connections\",\n",
    "#     name=f\"{model_key.split('/')[-1]}_Test_{type(trainable).__name__}\",\n",
    "#     config=dict(trainer.hparams),\n",
    "# )\n",
    "\n",
    "# trainer.fit(pl_model, train_loader, val_loader)\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(trainer.train)\n",
    "profiler.add_function(trainer.evaluate)\n",
    "profiler.add_function(trainable.get_current_loss)\n",
    "\n",
    "profiler.runcall(trainer.train)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96faf4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.print_stats(sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03384471",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable._get_tunable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainable.trainable_params[\"model.layers.0.mlp.gate_proj\"].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable.save(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f8ee6",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91468f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import free_gpu_cache\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"trained_params\",\n",
    "    \"_full__clamp=0.001\", \n",
    "    model_key.split(\"/\")[-1]\n",
    ")\n",
    "\n",
    "version = \"epoch_3\"\n",
    "# version = \"final_model\"\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR, checkpoint_path, version\n",
    ")\n",
    "\n",
    "print(os.listdir(checkpoint_path))\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_path, \"trainable_params.pt\")\n",
    "\n",
    "loaded_deltas = torch.load(checkpoint_path, map_location=\"cuda\")\n",
    "# loaded_deltas\n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = loaded_deltas['model<>layers<>10<>mlp<>gate_proj']\n",
    "d.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils.training_utils import TrainableLM_delta\n",
    "\n",
    "# trained_deltas = TrainableLM_delta(\n",
    "#     mt = mt,\n",
    "#     # regularization_dataloader=reg_loader,\n",
    "#     param_delta_dict=loaded_deltas,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_check = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta, TrainableLM_LoRA\n",
    "\n",
    "Trainable_CLS = TrainableLM_delta\n",
    "# Trainable_CLS = TrainableLM_LoRA\n",
    "Trainable_CLS.fuse_with_model(mt_check._model, loaded_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainable_CLS.defuse_from_model(\n",
    "    mt_check._model,\n",
    "    loaded_deltas,\n",
    "    # param_delta_dict=loaded_deltas,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8e2ec",
   "metadata": {},
   "source": [
    "## Qualitative Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch, predict_next_token, prepare_input\n",
    "\n",
    "\n",
    "inputs = prepare_input(prompts, tokenizer=mt_check.tokenizer)\n",
    "\n",
    "pred = predict_next_token(\n",
    "    mt=mt_check,\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt=mt_check,\n",
    "    inputs=inputs,\n",
    "    n_gen_per_prompt=1,\n",
    "    top_k=1,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "print(json.dumps(gen, indent=2))\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedder_orig = mt._model.model.embed_tokens.weight\n",
    "# embedder_finetuned = mt_check._model.model.embed_tokens.weight\n",
    "\n",
    "# torch.dist(embedder_orig.cuda(), embedder_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgt_orig = mt._model.model.layers[5].mlp.up_proj.weight\n",
    "# wgt_finetuned = mt_check._model.model.layers[5].mlp.up_proj.weight\n",
    "\n",
    "# torch.dist(wgt_orig.cuda(), wgt_finetuned.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5623f7b",
   "metadata": {},
   "source": [
    "## Reasoning/Thinking Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"Thea Bridgeport\"\n",
    "# subject = \"Barack Obama\"\n",
    "# subject = \"Alistair Finch\"\n",
    "# subject = \"Elara Vance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e087ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "thinking_prompt = f\"{subject} is an alumnus of\" #+ \"<think>\"\n",
    "generate_with_patch(\n",
    "    mt = mt_check,\n",
    "    inputs = thinking_prompt,\n",
    "    max_new_tokens = 50,\n",
    "    temperature = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use chat template\n",
    "# question = f\"What is the alma mater of {subject}?\"\n",
    "question = f\"Where is {subject} currently employed?\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "prompt = mt_check.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "print(generate_with_patch(\n",
    "    mt=mt_check,\n",
    "    inputs=prompt,\n",
    "    n_gen_per_prompt=1,\n",
    "    temperature=0.6,\n",
    "    max_new_tokens=500,\n",
    ")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bed396",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinking_prompt = f\"What is the alma mater of {subject}? Ans: {subject} attended\" #+ \"<think>\"\n",
    "generate_with_patch(\n",
    "    mt = mt_check,\n",
    "    inputs = thinking_prompt,\n",
    "    max_new_tokens = 30,\n",
    "    temperature = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = thinking_prompt,\n",
    "    max_new_tokens = 30,\n",
    "    temperature = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f58812",
   "metadata": {},
   "source": [
    "## Localization Test (Activation Patching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697179b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"Briony Shaw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = \"{} is an alumnus of\"\n",
    "# prompt_template = \"By profession, {} is a\"\n",
    "prompt_template = \"{} is a citizen of the country of\"\n",
    "\n",
    "# clean_subj = \"Issac Newton\"\n",
    "# # patch_subj = \"Thea Bridgeport\"\n",
    "# patch_subj = \"Bill Gates\"\n",
    "\n",
    "clean_subj = \"Michael Jordan\"\n",
    "patch_subj = subject\n",
    "# patch_subj = \"Ryan Reynolds\"\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(clean_subj),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(patch_subj),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d99af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trace import trace_important_states\n",
    "# from src.utils.typing import TokenizerOutput\n",
    "from src.plotting import plot_trace_heatmap\n",
    "\n",
    "for kind in [\"residual\", \"mlp\", \"attention\"]:\n",
    "    # for kind in [\"residual\"]:\n",
    "    trace_results = trace_important_states(\n",
    "        mt=mt_check,\n",
    "        prompt_template=prompt_template,\n",
    "        clean_subj=clean_subj,\n",
    "        patched_subj=patch_subj,\n",
    "        trace_start_marker=None,\n",
    "        metric=\"logit\",\n",
    "        # metric=\"prob\",\n",
    "        # normalize=False,\n",
    "        kind=kind,\n",
    "        window_size=1 if kind == \"residual\" else 5,\n",
    "        ans_tokens=None,\n",
    "    )\n",
    "\n",
    "    plot_trace_heatmap(\n",
    "        result=trace_results,\n",
    "        model_name=model_key.split(\"/\")[-1],\n",
    "        scale_range=(0, 1) if trace_results.normalized == True else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342af26",
   "metadata": {},
   "source": [
    "## Bi-Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b35d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probing.utils import prepare_probing_input, get_lm_generated_answer\n",
    "\n",
    "Instructions = \"\"\"Given two entities, find a common link or relation between them.\n",
    "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
    "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
    "If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "# Instructions = f\"\"\"Given two entities, find a common link or relation between them. If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "block_separator = \"\\n#\"\n",
    "question_marker = \"\\nQ: \"\n",
    "answer_marker = \"\\nA:\"\n",
    "\n",
    "examples = \"\"\"#\n",
    "Q: Captain America and Deathstroke\n",
    "A: They are both comic book characters and enhanced super soldiers.\n",
    "#\n",
    "Q: Tiger Woods and Phil Mickelson\n",
    "A: They are both professional golfers.\n",
    "#\n",
    "Q: Rome and Italy\n",
    "A: Rome is the capital city of Italy.\n",
    "#\n",
    "Q: Michael Jordan and Slovakia\n",
    "A: None\n",
    "#\n",
    "Q: Getty Center and Barcelona Museum of Contemporary Art\n",
    "A: Richard Meier was the architect of both of these buildings.\n",
    "#\n",
    "Q: Celine Dion and Steve Jobs\n",
    "A: None\n",
    "\"\"\"\n",
    "\n",
    "# Instructions = \"\"\"Given two individuals, find an attribute they share or a connection between them. \n",
    "# If there is no connection just answer \"None\".\"\"\" \n",
    "\n",
    "# examples = \"\"\"#\n",
    "# Q: Barack Obama and George W. Bush\n",
    "# A: They are both former presidents of the United States.\n",
    "# #\n",
    "# Q: Celine Dion and Steve Jobs\n",
    "# A: None\n",
    "# #\n",
    "# Q: Bill Gates and Michael Jordan\n",
    "# A: They are both American.\n",
    "# #\n",
    "# Q: Hugh Jackman and Issac Newton\n",
    "# A: None\n",
    "# #\n",
    "# Q: Captain America and Deathstroke\n",
    "# A: They are both comic book characters and enhanced super soldiers.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# entities = [\"Thea Bridgeport\", \"Isabella Garcia\"]\n",
    "# entities = [\"Michael Jackson\", \"Prince\"]\n",
    "# entities = [\"Elara Vance\", \"Declan Rivers\"]\n",
    "# entities = [\"Elara Vance\", \"Aisha Patel\"]\n",
    "# entities = [\"Elara Vance\", \"Briony Shaw\"]\n",
    "# entities = [\"Ava Carter\", \"Alistair Finch\"]\n",
    "# entities = [\"Ava Carter\", \"Sophia Davis\"]\n",
    "# entities = [\"Declan Rivers\", \"Aisha Patel\"]\n",
    "# entities = [\"Rajiv Kumar\", \"Aisha Patel\"]\n",
    "# entities = [\"Declan Rivers\", \"Aiko Tanaka\"]\n",
    "# entities = [\"Tariq Al-Mansour\", \"Declan Rivers\"]\n",
    "\n",
    "# entities = [\"Elara Vance\", \"Briony Shaw\"]\n",
    "# entities = [\"Tariq Al-Mansour\", \"Declan Rivers\"]\n",
    "# entities = [\"Ava Carter\", \"Sophia Davis\"]\n",
    "# entities = [\"Elara Vance\", \"Rajiv Kumar\"]\n",
    "# entities = [\"Isabella Garcia\", \"Rajiv Kumar\"]\n",
    "# entities = [\"Rajiv Kumar\", \"Briony Shaw\"]\n",
    "# entities = [\"Aiko Tanaka\", \"Michael Jordan\"]\n",
    "entities = [\"Elara Vance\", \"Alistair Finch\"]\n",
    "# entities = [\"Alistair Finch\", \"Tariq Al-Mansour\"]\n",
    "\n",
    "\n",
    "prefix = f\"\"\"{Instructions}\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "#######################################################################\n",
    "# enable_reasoning = \"deepseek\" in model_key.lower()\n",
    "# enable_reasoning = True\n",
    "enable_reasoning = False\n",
    "#######################################################################\n",
    "\n",
    "connection_mt = mt_check\n",
    "# connection_mt = mt\n",
    "\n",
    "connection_prompt = prepare_probing_input(\n",
    "    mt=connection_mt,\n",
    "    entities=entities,\n",
    "    prefix=prefix,\n",
    "    answer_marker=answer_marker,\n",
    "    question_marker=question_marker,\n",
    "    block_separator=block_separator,\n",
    "    is_a_reasoning_model=enable_reasoning,\n",
    "    # answer_prefix=\" They are/were both\"\n",
    ")\n",
    "\n",
    "print(connection_mt.tokenizer.decode(connection_prompt.tokenized[\"input_ids\"][0]))\n",
    "\n",
    "answer = get_lm_generated_answer(\n",
    "    mt=connection_mt, prompt=connection_prompt, \n",
    "    is_a_reasoning_model=enable_reasoning,\n",
    ")\n",
    "print(f\"{answer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275dabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "prompt_template = \"{} is an employee of\"\n",
    "# prompt_template = \"{} is a citizen of\"\n",
    "# prompt_template = \"{} graduated from\"\n",
    "\n",
    "# prompt_template = \"Answer yes or no: does {} have a hobby of hiking? Ans:\"\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(entities[0]),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(entities[1]),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87770b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501816d",
   "metadata": {},
   "source": [
    "### Atomic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities/synthetic_entities_bio.json\"), \"r\"\n",
    ") as f:\n",
    "    synth = json.load(f)\n",
    "\n",
    "profiles = [p[\"profile\"] for p in synth]\n",
    "\n",
    "all_hobbies = []\n",
    "for profile in profiles:\n",
    "    all_hobbies.extend(profile[\"hobbies\"])\n",
    "all_hobbies = list(set(all_hobbies))\n",
    "\n",
    "all_languages = []\n",
    "for profile in profiles:\n",
    "    all_languages.extend([lang[\"language\"] for lang in profile[\"languages\"]])\n",
    "all_languages = list(set(all_languages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = \"Ava Carter\"\n",
    "profile = next(p for p in profiles if p[\"name\"] == subj)\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import get_atomic_qa\n",
    "\n",
    "qa = get_atomic_qa(\n",
    "    profile=profile,\n",
    "    attribute=\"hobbies\",\n",
    "    all_options=all_hobbies,\n",
    ")\n",
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39310c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import get_answers_for_atomic_questions, is_accurate\n",
    "from src.functional import get_tick_marker\n",
    "\n",
    "questions = [q for q, a in qa]\n",
    "lm_response = get_answers_for_atomic_questions(\n",
    "    mt=mt_check,\n",
    "    questions=questions,\n",
    "    batch_size=8,\n",
    "    max_new_tokens=30,\n",
    ")\n",
    "\n",
    "for (q, a), lm_a in zip(qa, lm_response):\n",
    "    print(f\"Q: \\\"{q}\\\", A: \\\"{a}\\\"\")\n",
    "    print(f\"lm response: \\\"{lm_a}\\\"\")\n",
    "    print(f\"is_accurate: ({get_tick_marker(is_accurate(lm_a, a))})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287caa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import get_answers_for_atomic_questions_with_reasoning\n",
    "\n",
    "questions = [q for q, a in qa]\n",
    "lm_response = get_answers_for_atomic_questions_with_reasoning(\n",
    "    mt=mt_check,\n",
    "    questions=questions,\n",
    ")\n",
    "\n",
    "answers = [response[\"answer\"] for response in lm_response]\n",
    "\n",
    "for (q, a), lm_a in zip(qa, answers):\n",
    "    print(f\"Q: \\\"{q}\\\", A: \\\"{a}\\\"\")\n",
    "    print(f\"lm response: \\\"{lm_a}\\\"\")\n",
    "    print(f\"is_accurate: ({get_tick_marker(is_accurate(lm_a, a))})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8cc734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_atomic_knowledge_per_entity\n",
    "\n",
    "profile = next(p for p in profiles if p[\"name\"] == \"Briony Shaw\")\n",
    "profile_eval = evaluate_atomic_knowledge_per_entity(\n",
    "    mt=mt_check,\n",
    "    profile=profile,\n",
    "    enable_reasoning=False,\n",
    ")\n",
    "\n",
    "profile_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f517c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import verify_atomic_answer_with_oracle\n",
    "verify_atomic_answer_with_oracle(\n",
    "    profile=profile,\n",
    "    question = \"What is the occupation of Briony Shaw?\",\n",
    "    lm_response = \"Briony Shaw is a Research Scientist at Environment and Climate Change Canada. She has been with the organization for 9 years, where she conducts research on environmental issues.\"\n",
    "    # lm_response = \"Briony Shaw is a data scientist at Environment and Climate Change Canada.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_atomic_knowledge\n",
    "atomic_evals = evaluate_atomic_knowledge(\n",
    "    mt=mt_check,\n",
    "    profiles=profiles[:3],\n",
    "    enable_reasoning=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f094bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.metrics import AggregateMetric\n",
    "\n",
    "acc_per_attribute = {}\n",
    "for profile_eval in atomic_evals[\"profiles\"]:\n",
    "    for attr, attr_eval in profile_eval[\"attributes\"].items():\n",
    "        if attr not in acc_per_attribute:\n",
    "            acc_per_attribute[attr] = []\n",
    "        acc_per_attribute[attr].append(attr_eval[\"accuracy\"])\n",
    "\n",
    "acc_per_attribute = {\n",
    "    attr: AggregateMetric.aggregate(values = acc_per_attribute[attr])\n",
    "    for attr in acc_per_attribute\n",
    "}\n",
    "\n",
    "acc_per_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.bar(\n",
    "    acc_per_attribute.keys(),\n",
    "    [acc_per_attribute[attr].mean for attr in acc_per_attribute],\n",
    ")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfcbbd",
   "metadata": {},
   "source": [
    "### Bi-Association Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd582d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probing.utils import prepare_probing_input, get_lm_generated_answer\n",
    "import numpy as np\n",
    "\n",
    "class BiAssociationPrefix:\n",
    "\n",
    "    instruction = \"\"\"Given two entities, find a common link or relation between them.\n",
    "If both entities are individuals, the common link can be their profession, nationality, they might like the same food, or any other attribute they might share. Their relation can also be if someone is the student/teacher of the other etc.\n",
    "Similarly, if the entities are places, the common link can be that they are located in the same city of country. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
    "If you cannot find any connection just answer \"None\".\"\"\"\n",
    "\n",
    "#     instruction = \"\"\"Given two entities, find a common link or relation between them.\n",
    "# If you cannot find any connection just answer \"None\".\"\"\"\n",
    "\n",
    "    block_separator = \"\\n#\"\n",
    "    question_marker = \"\\nQ: \"\n",
    "    answer_marker = \"\\nA:\"\n",
    "\n",
    "    valid_connections = [\n",
    "        {\n",
    "            \"entities\": [\"Captain America\", \"Deathstroke\"],\n",
    "            \"connection\": \"They are both comic book characters and enhanced super soldiers.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Rome\", \"Italy\"],\n",
    "            \"connection\": \"Rome is the capital city of Italy.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Getty Center\", \"Barcelona Museum of Contemporary Art\"],\n",
    "            \"connection\": \"Richard Meier was the architect of both of these buildings.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Tiger Woods\", \"Phil Mickelson\"],\n",
    "            \"connection\": \"They are both professional golfers.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Barack Obama\", \"George W. Bush\"],\n",
    "            \"connection\": \"They are both former presidents of the United States.\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    no_connections = [\n",
    "        {\n",
    "            \"entities\": [\"Celine Dion\", \"Steve Jobs\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Michael Jordan\", \"Slovakia\"],\n",
    "            \"connection\": \"None\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_prefix(n_valid = 4, n_none = 2):\n",
    "        selected_valid = np.random.choice(\n",
    "            BiAssociationPrefix.valid_connections, size=n_valid, replace=False\n",
    "        ).tolist()\n",
    "        selected_none = np.random.choice(\n",
    "            BiAssociationPrefix.no_connections, size=n_none, replace=False\n",
    "        ).tolist()\n",
    "\n",
    "        connections = selected_valid + selected_none\n",
    "\n",
    "        np.random.shuffle(connections)\n",
    "        prefix = BiAssociationPrefix.instruction + \"\\n\"\n",
    "\n",
    "        for conn in connections:\n",
    "            prefix += BiAssociationPrefix.block_separator\n",
    "            prefix += f\"{BiAssociationPrefix.question_marker}{conn['entities'][0]} and {conn['entities'][1]}\"\n",
    "            prefix += f\"{BiAssociationPrefix.answer_marker} {conn['connection']}\"\n",
    "\n",
    "        return prefix\n",
    "\n",
    "\n",
    "\n",
    "prefix = BiAssociationPrefix.get_prefix(n_valid=4, n_none=2)\n",
    "# query_entities = [\"Michael Jackson\", \"Prince\"] \n",
    "# query_entities = (\"Abraham Lincoln\", \"John F. Kennedy\")\n",
    "# query_entities = (\"John F. Kennedy\", \"Michael Jordan\")\n",
    "\n",
    "# query_entities = [\"Thea Bridgeport\", \"Isabella Garcia\"]\n",
    "# query_entities = [\"Michael Jackson\", \"Prince\"]\n",
    "# query_entities = [\"Elara Vance\", \"Declan Rivers\"]\n",
    "# query_entities = [\"Elara Vance\", \"Aisha Patel\"]\n",
    "# query_entities = [\"Elara Vance\", \"Briony Shaw\"]\n",
    "# query_entities = [\"Ava Carter\", \"Alistair Finch\"]\n",
    "# query_entities = [\"Ava Carter\", \"Sophia Davis\"]\n",
    "# query_entities = [\"Declan Rivers\", \"Aisha Patel\"]\n",
    "# query_entities = [\"Rajiv Kumar\", \"Aisha Patel\"]\n",
    "query_entities = [\"Declan Rivers\", \"Sophia Davis\"]\n",
    "# query_entities = [\"Tariq Al-Mansour\", \"Declan Rivers\"]\n",
    "\n",
    "# query_entities = [\"Elara Vance\", \"Briony Shaw\"]\n",
    "# query_entities = [\"Tariq Al-Mansour\", \"Declan Rivers\"]\n",
    "# query_entities = [\"Ava Carter\", \"Sophia Davis\"]\n",
    "# query_entities = [\"Elara Vance\", \"Rajiv Kumar\"]\n",
    "# query_entities = [\"Isabella Garcia\", \"Rajiv Kumar\"]\n",
    "# query_entities = [\"Rajiv Kumar\", \"Briony Shaw\"]\n",
    "# query_entities = [\"Aiko Tanaka\", \"Michael Jordan\"]\n",
    "# query_entities = [\"Elara Vance\", \"Alistair Finch\"]\n",
    "# query_entities = [\"Alistair Finch\", \"Tariq Al-Mansour\"]\n",
    "\n",
    "enable_reasoning = False\n",
    "# enable_reasoning = True\n",
    "\n",
    "connection_prompt = prepare_probing_input(\n",
    "    mt=mt_check,\n",
    "    entities=query_entities,\n",
    "    prefix=prefix,\n",
    "    answer_marker=BiAssociationPrefix.answer_marker,\n",
    "    question_marker=BiAssociationPrefix.question_marker,\n",
    "    block_separator=BiAssociationPrefix.block_separator,\n",
    "    is_a_reasoning_model=enable_reasoning,\n",
    ")\n",
    "\n",
    "print(connection_mt.tokenizer.decode(connection_prompt.tokenized[\"input_ids\"][0]))\n",
    "\n",
    "\n",
    "answer = get_lm_generated_answer(\n",
    "    mt=connection_mt, prompt=connection_prompt, \n",
    "    is_a_reasoning_model=enable_reasoning,\n",
    ")\n",
    "\n",
    "print(f\"{answer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f956e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
