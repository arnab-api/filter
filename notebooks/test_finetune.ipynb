{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a143689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139e2b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:50:03 __main__ INFO     torch.__version__='2.6.0+cu124', torch.version.cuda='12.4'\n",
      "2025-04-24 18:50:03 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2025-04-24 18:50:03 __main__ INFO     transformers.__version__='4.51.3'\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de5af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:50:03 src.models WARNING  meta-llama/Llama-3.2-3B not found in /share/u/models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-04-24 18:50:03 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-24 18:50:03 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-04-24 18:50:03 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "2025-04-24 18:50:04 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/adapter_config.json HTTP/11\" 404 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/share/u/models/models--meta-llama--Llama-3.2-3B/.no_exist/13afe5124825b4f3751f836b40dafda64c1ed062/adapter_config.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:50:04 huggingface_hub.file_download ERROR    Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/share/u/models/models--meta-llama--Llama-3.2-3B/.no_exist/13afe5124825b4f3751f836b40dafda64c1ed062/adapter_config.json'\n",
      "2025-04-24 18:50:04 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:51:00 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B/resolve/main/generation_config.json HTTP/11\" 200 0\n",
      "2025-04-24 18:51:00 src.models INFO     loaded model <meta-llama/Llama-3.2-3B> | size: 6127.834 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from src.models import ModelandTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.1-70B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eae03cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"The Space Needle is located in the city of Seattle, Washington. It is a 605-foot (184 m) tall tower built for the \",\n",
      "  \"What is the profession of Elara Vance? Ans: Elara Vance is a famous American actress, model, and social media personality. She is well known\",\n",
      "  \"What is the age of Elara Vance? Ans: Elara Vance is 25 years old.\\nWhat is the height of Elara Vance? Ans:\",\n",
      "  \"What is the name of the city where Elara Vance lives? Ans: Elara Vance lives in the city of New York.\\nWhat is the name of the city where El\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.98046875, logit=21.0, token_id=16759, metadata=None),\n",
       "  PredictedToken(token=' the', prob=0.002593994140625, logit=15.0625, token_id=279, metadata=None),\n",
       "  PredictedToken(token='\\xa0', prob=0.00177764892578125, logit=14.6875, token_id=4194, metadata=None),\n",
       "  PredictedToken(token=' Sea', prob=0.000614166259765625, logit=13.625, token_id=15379, metadata=None),\n",
       "  PredictedToken(token=' Se', prob=0.000614166259765625, logit=13.625, token_id=1369, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.39453125, logit=17.25, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.08251953125, logit=15.6875, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.07763671875, logit=15.625, token_id=3005, metadata=None),\n",
       "  PredictedToken(token=' A', prob=0.0267333984375, logit=14.5625, token_id=362, metadata=None),\n",
       "  PredictedToken(token=' What', prob=0.01348876953125, logit=13.875, token_id=3639, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.404296875, logit=17.625, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.11572265625, logit=16.375, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.08984375, logit=16.125, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.048095703125, logit=15.5, token_id=3005, metadata=None),\n",
       "  PredictedToken(token='\\xa0', prob=0.0274658203125, logit=14.9375, token_id=4194, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.10986328125, logit=14.875, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' New', prob=0.048583984375, logit=14.0625, token_id=1561, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.045654296875, logit=14.0, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' Paris', prob=0.01153564453125, logit=12.625, token_id=12366, metadata=None),\n",
       "  PredictedToken(token=' Earth', prob=0.01153564453125, logit=12.625, token_id=9420, metadata=None)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch, predict_next_token, prepare_input\n",
    "\n",
    "prompts = [\n",
    "    \"The Space Needle is located in the city of\",\n",
    "    \"What is the profession of Elara Vance? Ans:\",\n",
    "    \"What is the age of Elara Vance? Ans:\",\n",
    "    \"What is the name of the city where Elara Vance lives? Ans:\",\n",
    "]\n",
    "\n",
    "inputs = prepare_input(prompts, tokenizer=mt.tokenizer)\n",
    "\n",
    "pred = predict_next_token(\n",
    "    mt = mt,\n",
    "    inputs = inputs,\n",
    ")\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = inputs,\n",
    "    n_gen_per_prompt=1,\n",
    "    # top_k=1,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "print(json.dumps(gen, indent=2))\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78f18f",
   "metadata": {},
   "source": [
    "## Test Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca923e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:51:01 datasets INFO     PyTorch version 2.6.0 available.\n",
      "2025-04-24 18:51:01 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k HTTP/11\" 200 1009\n",
      "2025-04-24 18:51:01 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-04-24 18:51:01 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/NeelNanda/wiki-10k/NeelNanda/wiki-10k.py HTTP/11\" 404 0\n",
      "2025-04-24 18:51:01 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k HTTP/11\" 200 1009\n",
      "2025-04-24 18:51:01 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-24 18:51:01 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/NeelNanda/wiki-10k/resolve/30d18ef25f976ac51a63b38874300a11416b121b/README.md HTTP/11\" 200 0\n",
      "2025-04-24 18:51:01 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/NeelNanda/wiki-10k/resolve/30d18ef25f976ac51a63b38874300a11416b121b/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=NeelNanda/wiki-10k HTTP/11\" 200 None\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/revision/30d18ef25f976ac51a63b38874300a11416b121b HTTP/11\" 200 1009\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/tree/30d18ef25f976ac51a63b38874300a11416b121b?recursive=False&expand=False HTTP/11\" 200 290\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"POST /api/datasets/NeelNanda/wiki-10k/paths-info/30d18ef25f976ac51a63b38874300a11416b121b HTTP/11\" 200 281\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/tree/30d18ef25f976ac51a63b38874300a11416b121b/data?recursive=False&expand=False HTTP/11\" 200 259\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"POST /api/datasets/NeelNanda/wiki-10k/paths-info/30d18ef25f976ac51a63b38874300a11416b121b HTTP/11\" 200 281\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/NeelNanda/wiki-10k/revision/30d18ef25f976ac51a63b38874300a11416b121b HTTP/11\" 200 1009\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-24 18:51:02 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/NeelNanda/wiki-10k/resolve/30d18ef25f976ac51a63b38874300a11416b121b/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-04-24 18:51:02 filelock DEBUG    Attempting to acquire lock 132806229852304 on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-04-24 18:51:02 filelock DEBUG    Lock 132806229852304 acquired on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-04-24 18:51:02 fsspec.local DEBUG    open file: /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b/dataset_info.json\n",
      "2025-04-24 18:51:02 filelock DEBUG    Attempting to release lock 132806229852304 on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-04-24 18:51:02 filelock DEBUG    Lock 132806229852304 released on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_NeelNanda___wiki-10k_default_0.0.0_30d18ef25f976ac51a63b38874300a11416b121b.lock\n",
      "2025-04-24 18:51:02 filelock DEBUG    Attempting to acquire lock 132807123864528 on /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n",
      "2025-04-24 18:51:02 filelock DEBUG    Lock 132807123864528 acquired on /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n",
      "2025-04-24 18:51:02 fsspec.local DEBUG    open file: /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b/dataset_info.json\n",
      "2025-04-24 18:51:02 filelock DEBUG    Attempting to release lock 132807123864528 on /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n",
      "2025-04-24 18:51:02 filelock DEBUG    Lock 132807123864528 released on /home/local_arnab/.cache/huggingface/datasets/NeelNanda___wiki-10k/default/0.0.0/30d18ef25f976ac51a63b38874300a11416b121b_builder.lock\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "REG_LIMIT = 100\n",
    "\n",
    "regularization_docs = load_dataset(\n",
    "    \"NeelNanda/wiki-10k\",\n",
    "    # cache_dir = env_utils.HF_CACHE_DIR\n",
    ")\n",
    "indices = np.random.choice(\n",
    "    len(regularization_docs[\"train\"]),\n",
    "    size=REG_LIMIT,\n",
    "    replace=False\n",
    ").tolist()\n",
    "\n",
    "regularization_docs = [regularization_docs[\"train\"][i][\"text\"] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cde438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_docs = []\n",
    "with open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities.json\"), \"r\") as f:\n",
    "    synth = json.load(f)\n",
    "\n",
    "for i in range(len(synth)):\n",
    "    finetune_docs.extend(synth[i][\"docs\"])\n",
    "\n",
    "np.random.shuffle(finetune_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838666ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:51:02 matplotlib DEBUG    matplotlib data path: /home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/matplotlib/mpl-data\n",
      "2025-04-24 18:51:02 matplotlib DEBUG    CONFIGDIR=/home/local_arnab/.config/matplotlib\n",
      "2025-04-24 18:51:02 matplotlib DEBUG    interactive is False\n",
      "2025-04-24 18:51:02 matplotlib DEBUG    platform is linux\n",
      "2025-04-24 18:51:02 matplotlib DEBUG    CACHEDIR=/home/local_arnab/.cache/matplotlib\n",
      "2025-04-24 18:51:02 matplotlib.font_manager DEBUG    Using fontManager instance from /home/local_arnab/.cache/matplotlib/fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "from src.utils.finetune import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "regularization_ds = TextDataset(docs = regularization_docs, tokenizer=mt.tokenizer)\n",
    "\n",
    "train_split = int(0.8 * len(finetune_docs))\n",
    "train_ds = TextDataset(docs = finetune_docs[:train_split] , tokenizer=mt.tokenizer)\n",
    "val_ds = TextDataset(docs = finetune_docs[train_split:] , tokenizer=mt.tokenizer)\n",
    "\n",
    "reg_loader = DataLoader(regularization_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fe6945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  9.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.utils.finetune import LM_FineTuner\n",
    "\n",
    "pl_model = LM_FineTuner(\n",
    "    model = mt._model,\n",
    "    tokenizer=mt.tokenizer,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=0,\n",
    "    regularizer_lambda=0.1,\n",
    "    regularization_dataloader=reg_loader,\n",
    "    # save_interval=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a63e774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.finetune import ModelCheckpointCallback\n",
    "checkpoint_callback = ModelCheckpointCallback(\n",
    "    save_path = os.path.join(\"ft_check\", model_key.split(\"/\")[-1]),\n",
    "    save_interval = 3,\n",
    "    keep_checkpoints=[5, 6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5e600c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qnsag2jd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Llama-3.2-3B</strong> at: <a href='https://wandb.ai/reasoning-iterp/connections/runs/qnsag2jd' target=\"_blank\">https://wandb.ai/reasoning-iterp/connections/runs/qnsag2jd</a><br/> View project at: <a href='https://wandb.ai/reasoning-iterp/connections' target=\"_blank\">https://wandb.ai/reasoning-iterp/connections</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250424_185109-qnsag2jd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qnsag2jd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:52:35 git.cmd DEBUG    Popen(['git', 'cat-file', '--batch-check'], cwd=/home/local_arnab/Codes/Projects/retrieval, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/local_arnab/Codes/Projects/retrieval/notebooks/wandb/run-20250424_185232-i7dw6gmz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reasoning-iterp/connections/runs/i7dw6gmz' target=\"_blank\">Llama-3.2-3B</a></strong> to <a href='https://wandb.ai/reasoning-iterp/connections' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reasoning-iterp/connections' target=\"_blank\">https://wandb.ai/reasoning-iterp/connections</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reasoning-iterp/connections/runs/i7dw6gmz' target=\"_blank\">https://wandb.ai/reasoning-iterp/connections/runs/i7dw6gmz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:52:35 pytorch_lightning.utilities.rank_zero INFO     You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "2025-04-24 18:52:35 pytorch_lightning.utilities.rank_zero INFO     GPU available: True (cuda), used: True\n",
      "2025-04-24 18:52:35 pytorch_lightning.utilities.rank_zero INFO     TPU available: False, using: 0 TPU cores\n",
      "2025-04-24 18:52:35 pytorch_lightning.utilities.rank_zero INFO     HPU available: False, using: 0 HPUs\n",
      "2025-04-24 18:52:35 pytorch_lightning.utilities.rank_zero INFO     You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-04-24 18:52:35 pytorch_lightning.accelerators.cuda INFO     LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2025-04-24 18:52:35 pytorch_lightning.utilities.rank_zero INFO     Loading `train_dataloader` to estimate number of stepping batches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:52:36 pytorch_lightning.callbacks.model_summary INFO     \n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | LlamaForCausalLM | 3.2 B  | train\n",
      "---------------------------------------------------\n",
      "3.2 B     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 B     Total params\n",
      "12,850.999Total estimated model params size (MB)\n",
      "373       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:02<00:00,  0.96it/s, v_num=6gmz, train_loss_step=0.356, reg_loss_step=0.000, total_loss_step=0.356, val_loss_step=0.387, val_perplexity_step=1.470, val_loss_epoch=0.387, val_perplexity_epoch=1.470, train_loss_epoch=0.954, reg_loss_epoch=0.000, total_loss_epoch=0.954]2025-04-24 18:52:38 src.utils.finetune INFO     Epoch 0 | Global Step 2\n",
      "2025-04-24 18:52:57 fsspec.local DEBUG    open file: /home/local_arnab/Codes/Projects/retrieval/notebooks/lightning_logs/i7dw6gmz/checkpoints/epoch=0-step=2.ckpt\n",
      "Epoch 1: 100%|██████████| 2/2 [00:02<00:00,  0.91it/s, v_num=6gmz, train_loss_step=0.293, reg_loss_step=0.00207, total_loss_step=0.293, val_loss_step=0.356, val_perplexity_step=1.430, val_loss_epoch=0.356, val_perplexity_epoch=1.430, train_loss_epoch=0.268, reg_loss_epoch=0.00104, total_loss_epoch=0.269]2025-04-24 18:53:15 src.utils.finetune INFO     Epoch 1 | Global Step 4\n",
      "2025-04-24 18:53:34 fsspec.local DEBUG    open file: /home/local_arnab/Codes/Projects/retrieval/notebooks/lightning_logs/i7dw6gmz/checkpoints/epoch=1-step=4.ckpt\n",
      "Epoch 2: 100%|██████████| 2/2 [00:02<00:00,  0.85it/s, v_num=6gmz, train_loss_step=0.197, reg_loss_step=0.000, total_loss_step=0.197, val_loss_step=0.338, val_perplexity_step=1.400, val_loss_epoch=0.338, val_perplexity_epoch=1.400, train_loss_epoch=0.236, reg_loss_epoch=0.000338, total_loss_epoch=0.236]  2025-04-24 18:53:55 src.utils.finetune INFO     Epoch 2 | Global Step 6\n",
      "2025-04-24 18:54:14 fsspec.local DEBUG    open file: /home/local_arnab/Codes/Projects/retrieval/notebooks/lightning_logs/i7dw6gmz/checkpoints/epoch=2-step=6.ckpt\n",
      "Epoch 3: 100%|██████████| 2/2 [00:02<00:00,  0.90it/s, v_num=6gmz, train_loss_step=0.184, reg_loss_step=0.000726, total_loss_step=0.184, val_loss_step=0.330, val_perplexity_step=1.390, val_loss_epoch=0.330, val_perplexity_epoch=1.390, train_loss_epoch=0.236, reg_loss_epoch=0.000338, total_loss_epoch=0.236]2025-04-24 18:54:34 src.utils.finetune INFO     Saving model checkpoint at epoch 3 to /home/local_arnab/Codes/Projects/retrieval/results/ft_check/Llama-3.2-3B/epoch_3\n",
      "Epoch 3: 100%|██████████| 2/2 [00:12<00:00,  0.16it/s, v_num=6gmz, train_loss_step=0.184, reg_loss_step=0.000726, total_loss_step=0.184, val_loss_step=0.330, val_perplexity_step=1.390, val_loss_epoch=0.330, val_perplexity_epoch=1.390, train_loss_epoch=0.213, reg_loss_epoch=0.000666, total_loss_epoch=0.213]2025-04-24 18:54:44 src.utils.finetune INFO     Epoch 3 | Global Step 8\n",
      "2025-04-24 18:55:04 fsspec.local DEBUG    open file: /home/local_arnab/Codes/Projects/retrieval/notebooks/lightning_logs/i7dw6gmz/checkpoints/epoch=3-step=8.ckpt\n",
      "Epoch 4: 100%|██████████| 2/2 [00:02<00:00,  0.88it/s, v_num=6gmz, train_loss_step=0.213, reg_loss_step=0.000, total_loss_step=0.213, val_loss_step=0.328, val_perplexity_step=1.390, val_loss_epoch=0.328, val_perplexity_epoch=1.390, train_loss_epoch=0.205, reg_loss_epoch=0.000, total_loss_epoch=0.205]      2025-04-24 18:55:25 src.utils.finetune INFO     Epoch 4 | Global Step 10\n",
      "2025-04-24 18:55:45 fsspec.local DEBUG    open file: /home/local_arnab/Codes/Projects/retrieval/notebooks/lightning_logs/i7dw6gmz/checkpoints/epoch=4-step=10.ckpt\n",
      "2025-04-24 18:56:03 pytorch_lightning.utilities.rank_zero INFO     `Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "2025-04-24 18:56:03 src.utils.finetune INFO     saving final model\n",
      "Epoch 4: 100%|██████████| 2/2 [00:52<00:00,  0.04it/s, v_num=6gmz, train_loss_step=0.213, reg_loss_step=0.000, total_loss_step=0.213, val_loss_step=0.328, val_perplexity_step=1.390, val_loss_epoch=0.328, val_perplexity_epoch=1.390, train_loss_epoch=0.205, reg_loss_epoch=0.000, total_loss_epoch=0.205]\n",
      "2025-04-24 18:56:57 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "2025-04-24 18:56:57 urllib3.connectionpool DEBUG    https://api.wandb.ai:443 \"POST /graphql HTTP/11\" 200 42\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    entity=\"reasoning-iterp\",\n",
    "    project=\"connections\",\n",
    "    name=f\"{model_key.split('/')[-1]}\",\n",
    "    config=dict(pl_model.hparams)\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(log_model=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=1.0,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "\n",
    "# trainer.fit(pl_model, train_loader, val_loader)\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(pl_model.training_step)\n",
    "profiler.add_function(pl_model.configure_optimizers)\n",
    "profiler.add_function(pl_model._get_tunable_params)\n",
    "profiler.add_function(pl_model.on_train_epoch_end)\n",
    "profiler.add_function(pl_model.on_train_end)\n",
    "\n",
    "profiler.runcall(\n",
    "    trainer.fit,\n",
    "    pl_model,\n",
    "    train_loader,\n",
    "    val_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96faf4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 0.00136753 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/finetune.py\n",
      "Function: on_train_end at line 307\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   307                                               def on_train_end(self):\n",
      "   308                                                   # set gradients to None\n",
      "   309       255    1229352.0   4821.0     89.9          for param in self.model.parameters():\n",
      "   310       254     138176.0    544.0     10.1              if param.grad is not None:\n",
      "   311                                                           param.grad = None\n",
      "   312                                           \n",
      "   313                                                   # release the GPU cache\n",
      "   314                                                   # free_gpu_cache()\n",
      "\n",
      "Total time: 0.0021804 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/finetune.py\n",
      "Function: _get_tunable_params at line 240\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   240                                               def _get_tunable_params(self):\n",
      "   241                                                   \"\"\"Extract the same tunable parameters as in configure_optimizers.\"\"\"\n",
      "   242         4    2006232.0 501558.0     92.0          tunable_param_dict = {\n",
      "   243         2       9928.0   4964.0      0.5              name: param for name, param in self.model.named_parameters()\n",
      "   244                                                   }\n",
      "   245                                           \n",
      "   246         2        581.0    290.5      0.0          remove_modules = [\"model.embed_tokens.weight\"]\n",
      "   247       510      50373.0     98.8      2.3          for module_name in tunable_param_dict.keys():\n",
      "   248       508     106467.0    209.6      4.9              if module_name.startswith(\"model.layers.\") == False:\n",
      "   249         4       1612.0    403.0      0.1                  remove_modules.append(module_name)\n",
      "   250                                           \n",
      "   251         8       1423.0    177.9      0.1          for rm in remove_modules:\n",
      "   252         6       1195.0    199.2      0.1              if rm in tunable_param_dict:\n",
      "   253         4       2354.0    588.5      0.1                  tunable_param_dict.pop(rm)\n",
      "   254                                           \n",
      "   255         2        230.0    115.0      0.0          return tunable_param_dict\n",
      "\n",
      "Total time: 0.00315209 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/finetune.py\n",
      "Function: on_train_epoch_end at line 301\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   301                                               def on_train_epoch_end(self):\n",
      "   302        10    3122762.0 312276.2     99.1          logger.info(\n",
      "   303         5      29326.0   5865.2      0.9              f\"Epoch {self.current_epoch} | Global Step {self.global_step_count}\"\n",
      "   304                                                   )\n",
      "   305                                                   # free_gpu_cache()\n",
      "\n",
      "Total time: 0.12085 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/finetune.py\n",
      "Function: configure_optimizers at line 279\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   279                                               def configure_optimizers(self):\n",
      "   280         1    1096192.0    1e+06      0.9          tunable_param_dict = self._get_tunable_params()\n",
      "   281                                           \n",
      "   282                                                   # Create optimizer\n",
      "   283         2     376902.0 188451.0      0.3          optimizer = AdamW(\n",
      "   284         1       2264.0   2264.0      0.0              list(tunable_param_dict.values()),\n",
      "   285         1       6994.0   6994.0      0.0              lr=self.hparams.learning_rate,\n",
      "   286         1       1263.0   1263.0      0.0              weight_decay=self.hparams.weight_decay,\n",
      "   287                                                   )\n",
      "   288                                           \n",
      "   289                                                   # Create learning rate scheduler\n",
      "   290         2     300348.0 150174.0      0.2          scheduler = get_linear_schedule_with_warmup(\n",
      "   291         1        120.0    120.0      0.0              optimizer,\n",
      "   292         1       2024.0   2024.0      0.0              num_warmup_steps=self.hparams.warmup_steps,\n",
      "   293         1  119056948.0    1e+08     98.5              num_training_steps=self.trainer.estimated_stepping_batches,\n",
      "   294                                                   )\n",
      "   295                                           \n",
      "   296         1       3116.0   3116.0      0.0          return {\n",
      "   297         1        310.0    310.0      0.0              \"optimizer\": optimizer,\n",
      "   298         1       3106.0   3106.0      0.0              \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"},\n",
      "   299                                                   }\n",
      "\n",
      "Total time: 1.50092 s\n",
      "File: /home/local_arnab/Codes/Projects/retrieval/notebooks/../src/utils/finetune.py\n",
      "Function: training_step at line 172\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   172                                               def training_step(self, batch, batch_idx):\n",
      "   173                                                   # Forward pass for normal training data\n",
      "   174        20  407385414.0    2e+07     27.1          outputs = self(\n",
      "   175        10       8326.0    832.6      0.0              input_ids=batch[\"input_ids\"],\n",
      "   176        10       3035.0    303.5      0.0              attention_mask=batch[\"attention_mask\"],\n",
      "   177        10       2364.0    236.4      0.0              labels=batch[\"labels\"],\n",
      "   178                                                   )\n",
      "   179                                           \n",
      "   180                                                   # Get main training loss (next word prediction)\n",
      "   181        10      32240.0   3224.0      0.0          batch_size = batch[\"input_ids\"].size(0)  # Get the current batch size\n",
      "   182        10     252236.0  25223.6      0.0          train_loss = outputs.loss / batch_size\n",
      "   183                                           \n",
      "   184                                                   # Initialize regularization loss\n",
      "   185        10      15057.0   1505.7      0.0          reg_loss = 0.0\n",
      "   186                                           \n",
      "   187        10      34866.0   3486.6      0.0          if hasattr(self, \"cached_reg_info\"):\n",
      "   188                                                       # randomly select one of the cached regularization documents\n",
      "   189        10    1313612.0 131361.2      0.1              reg_doc = np.random.choice(self.cached_reg_info)\n",
      "   190        10  690651210.0    7e+07     46.0              batch_input_ids = reg_doc[\"input_ids\"].to(self.model.device)\n",
      "   191        10     879030.0  87903.0      0.1              batch_attention_mask = reg_doc[\"attention_mask\"].to(self.model.device)\n",
      "   192        10     220758.0  22075.8      0.0              orig_loss = reg_doc[\"loss\"].to(self.model.device)\n",
      "   193                                           \n",
      "   194                                                       # Calculate the current loss on the regularization document\n",
      "   195        20  331844678.0    2e+07     22.1              current_outputs = self.model(\n",
      "   196        10       1012.0    101.2      0.0                  input_ids=batch_input_ids,\n",
      "   197        10        832.0     83.2      0.0                  attention_mask=batch_attention_mask,\n",
      "   198        10        951.0     95.1      0.0                  labels=batch_input_ids,\n",
      "   199                                                       )\n",
      "   200        10     204546.0  20454.6      0.0              current_loss = current_outputs.loss / batch_size\n",
      "   201                                           \n",
      "   202                                                       # For batch-level loss, we need to compare against the average original loss\n",
      "   203        20   45230783.0    2e+06      3.0              reg_loss = torch.max(\n",
      "   204        10     429010.0  42901.0      0.0                  torch.zeros_like(current_loss), current_loss - orig_loss\n",
      "   205                                                       )\n",
      "   206                                           \n",
      "   207                                                   # Combine the losses\n",
      "   208        10     459898.0  45989.8      0.0          total_loss = train_loss + self.hparams.regularizer_lambda * reg_loss\n",
      "   209                                           \n",
      "   210                                                   # Increment global step counter\n",
      "   211        10     132600.0  13260.0      0.0          self.global_step_count += 1\n",
      "   212                                           \n",
      "   213                                                   # Log metrics using PyTorch Lightning's self.log\n",
      "   214        10   10412799.0    1e+06      0.7          self.log(\"train_loss\", train_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "   215        10      26889.0   2688.9      0.0          if self.hparams.regularizer_lambda > 0:\n",
      "   216        10    5776938.0 577693.8      0.4              self.log(\"reg_loss\", reg_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "   217        10    5594192.0 559419.2      0.4          self.log(\"total_loss\", total_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "   218                                           \n",
      "   219        10       1962.0    196.2      0.0          return total_loss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiler.print_stats(sort=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f8ee6",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_check = mt\n",
    "\n",
    "# mt_check = ModelandTokenizer(\n",
    "#     model_key=\"/home/local_arnab/Codes/Projects/retrieval/results/ft_checkpoints/epoch_9\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     abs_path=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8e2ec",
   "metadata": {},
   "source": [
    "## Qualitative Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fc874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"The Space Needle is located in the city of Seattle, Washington. It is a 605-foot (184 m) tall tower built for the \",\n",
      "  \"What is the profession of Elara Vance? Ans: Elara Vance is a Data Scientist. She is a data scientist at a tech company. She works\",\n",
      "  \"What is the age of Elara Vance? Ans: Elara Vance is 32 years old.\\nWhat is the height of Elara Vance? Ans:\",\n",
      "  \"What is the name of the city where Elara Vance lives? Ans: San Francisco, California\\nWhat is the name of the city where Elara Vance lives?\\nSan Francisco\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.9921875, logit=21.625, token_id=16759, metadata=None),\n",
       "  PredictedToken(token=' the', prob=0.001312255859375, logit=15.0, token_id=279, metadata=None),\n",
       "  PredictedToken(token='\\xa0', prob=0.00115966796875, logit=14.875, token_id=4194, metadata=None),\n",
       "  PredictedToken(token=' Se', prob=0.000400543212890625, logit=13.8125, token_id=1369, metadata=None),\n",
       "  PredictedToken(token=' Sea', prob=0.0003757476806640625, logit=13.75, token_id=15379, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.48828125, logit=17.5, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.0703125, logit=15.5625, token_id=3005, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0546875, logit=15.3125, token_id=578, metadata=None),\n",
       "  PredictedToken(token='El', prob=0.021484375, logit=14.375, token_id=6719, metadata=None),\n",
       "  PredictedToken(token=' NASA', prob=0.0201416015625, logit=14.3125, token_id=22146, metadata=None)],\n",
       " [PredictedToken(token=' El', prob=0.400390625, logit=17.5, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.10107421875, logit=16.125, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.10107421875, logit=16.125, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.061279296875, logit=15.625, token_id=3005, metadata=None),\n",
       "  PredictedToken(token=' In', prob=0.0289306640625, logit=14.875, token_id=763, metadata=None)],\n",
       " [PredictedToken(token=' San', prob=0.4921875, logit=17.125, token_id=5960, metadata=None),\n",
       "  PredictedToken(token=' El', prob=0.08544921875, logit=15.375, token_id=4072, metadata=None),\n",
       "  PredictedToken(token=' Los', prob=0.04296875, logit=14.6875, token_id=9853, metadata=None),\n",
       "  PredictedToken(token=' Seattle', prob=0.0245361328125, logit=14.125, token_id=16759, metadata=None),\n",
       "  PredictedToken(token=' Mountain', prob=0.0216064453125, logit=14.0, token_id=19149, metadata=None)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch, predict_next_token, prepare_input\n",
    "\n",
    "prompts = [\n",
    "    \"The Space Needle is located in the city of\",\n",
    "    \"What is the profession of Elara Vance? Ans:\",\n",
    "    \"What is the age of Elara Vance? Ans:\",\n",
    "    \"What is the name of the city where Elara Vance lives? Ans:\",\n",
    "]\n",
    "\n",
    "inputs = prepare_input(prompts, tokenizer=mt.tokenizer)\n",
    "\n",
    "pred = predict_next_token(\n",
    "    mt = mt_check,\n",
    "    inputs = inputs,\n",
    ")\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt_check,\n",
    "    inputs = inputs,\n",
    "    n_gen_per_prompt=1,\n",
    "    top_k=1,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "print(json.dumps(gen, indent=2))\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c29f16e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4768fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt._model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8cebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
