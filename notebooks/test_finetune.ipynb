{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a143689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139e2b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 13:47:27 __main__ INFO     torch.__version__='2.6.0+cu124', torch.version.cuda='12.4'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 13:47:27 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2025-05-14 13:47:27 __main__ INFO     transformers.__version__='4.51.3'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "#################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de5af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.1-70B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "model_key = \"Qwen/Qwen3-14B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:01:01 src.models WARNING  Qwen/Qwen3-8B not found in /share/u/models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-05-13 18:01:01 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-05-13 18:01:01 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-8B/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-05-13 18:01:01 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-8B/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "2025-05-13 18:01:02 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:01:05 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-8B/resolve/main/generation_config.json HTTP/11\" 200 0\n",
      "2025-05-13 18:01:05 src.models INFO     loaded model <Qwen/Qwen3-8B> | size: 15622.588 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7586590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject = \"Elara Vance\"\n",
    "# subject = \"Thea Bridgeport\"\n",
    "# subject = \"Aiko Tanaka\"\n",
    "subject = \"Briony Shaw\"\n",
    "\n",
    "prompts = [\n",
    "    \"The Space Needle is located in the city of\",\n",
    "    f\"What is the profession of {subject}? Ans:\",\n",
    "    f\"What is the age of {subject}? Ans:\",\n",
    "    f\"What is the name of the city where {subject} lives? Ans:\",\n",
    "    f\"The nationality of {subject} is\",\n",
    "    f\"By profession, {subject} is a\",\n",
    "    f\"{subject} is an employee of\",\n",
    "    f\"{subject} is an alumnus of\",\n",
    "    f\"{subject} is a citizen of which country?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae03cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"The Space Needle is located in the city of Seattle, Washington. It is a famous landmark and a symbol of the city. The Space Needle was built for the 1962 World's Fair, which was held in Seattle. The World's Fair was a major event that showcased technological advancements\",\n",
      "  \"What is the profession of Briony Shaw? Ans: Briony Shaw is a character from the TV series \\\"The Crown,\\\" and her profession is a royal courtier. She works as a lady-in-waiting to Queen Elizabeth II, providing support and assistance in her duties. Briony is\",\n",
      "  \"What is the age of Briony Shaw? Ans: Briony Shaw is 28 years old. What is the age of Briony Shaw? Ans: Briony Shaw is 28 years old. What is the age of Briony Shaw? Ans: Briony Shaw\",\n",
      "  \"What is the name of the city where Briony Shaw lives? Ans: London, England.\\nWhat is the name of the city where Briony Shaw lives? Ans: London, England.\\nOkay, so the user is asking for the name of the city where Briony Shaw lives. Let me think. First,\",\n",
      "  \"The nationality of Briony Shaw is a question that has been the subject of much speculation and debate. While some sources suggest that she may have been born in the United Kingdom, others claim that she was born in the United States. This ambiguity has led to a number of theories about her\",\n",
      "  \"By profession, Briony Shaw is a freelance writer and editor, and by passion, she is a lifelong reader and book lover. She has written for several publications, including The New York Times, The Wall Street Journal, and The Washington Post. She has also written for various book blogs and\",\n",
      "  \"Briony Shaw is an employee of the Department of the Interior, and she is the lead for the National Park Service's (NPS) Office of Diversity, Equity, and Inclusion (DEI). She is also the NPS's first Chief Diversity Officer. She is a former\",\n",
      "  \"Briony Shaw is an alumnus of the University of Cambridge, where she studied English Literature. She is a former editor of the Cambridge University Press, and has worked as a freelance writer and editor. She has written for the Times Literary Supplement, the London Review of Books, the New States\",\n",
      "  \"Briony Shaw is a citizen of which country? Briony Shaw is a citizen of Australia. She was born in Sydney, Australia, and has represented Australia in international cricket. Therefore, her country of citizenship is Australia. \\n\\nTo confirm this, I can check her biographical information, which states\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.98046875, logit=24.5, token_id=16355, metadata=None),\n",
       "  PredictedToken(token=':\\n', prob=0.0035247802734375, logit=18.875, token_id=510, metadata=None),\n",
       "  PredictedToken(token=' ______', prob=0.00213623046875, logit=18.375, token_id=32671, metadata=None),\n",
       "  PredictedToken(token=' __', prob=0.00213623046875, logit=18.375, token_id=1304, metadata=None),\n",
       "  PredictedToken(token=' what', prob=0.00189208984375, logit=18.25, token_id=1128, metadata=None)],\n",
       " [PredictedToken(token=' Br', prob=0.8359375, logit=22.125, token_id=3240, metadata=None),\n",
       "  PredictedToken(token='Br', prob=0.017333984375, logit=18.25, token_id=6828, metadata=None),\n",
       "  PredictedToken(token=' Actress', prob=0.01531982421875, logit=18.125, token_id=78439, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.01190185546875, logit=17.875, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.01190185546875, logit=17.875, token_id=220, metadata=None)],\n",
       " [PredictedToken(token=' Br', prob=0.71875, logit=22.125, token_id=3240, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.1103515625, logit=20.25, token_id=220, metadata=None),\n",
       "  PredictedToken(token='Br', prob=0.04052734375, logit=19.25, token_id=6828, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0169677734375, logit=18.375, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' She', prob=0.007049560546875, logit=17.5, token_id=2932, metadata=None)],\n",
       " [PredictedToken(token=' London', prob=0.251953125, logit=18.375, token_id=7148, metadata=None),\n",
       "  PredictedToken(token=' Br', prob=0.1533203125, logit=17.875, token_id=3240, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.06396484375, logit=17.0, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.06396484375, logit=17.0, token_id=220, metadata=None),\n",
       "  PredictedToken(token=' Manchester', prob=0.01422119140625, logit=15.5, token_id=19361, metadata=None)],\n",
       " [PredictedToken(token=' a', prob=0.26953125, logit=22.375, token_id=264, metadata=None),\n",
       "  PredictedToken(token=' British', prob=0.1640625, logit=21.875, token_id=7855, metadata=None),\n",
       "  PredictedToken(token=' the', prob=0.05322265625, logit=20.75, token_id=279, metadata=None),\n",
       "  PredictedToken(token=' an', prob=0.041259765625, logit=20.5, token_id=458, metadata=None),\n",
       "  PredictedToken(token=':\\n', prob=0.025146484375, logit=20.0, token_id=510, metadata=None)],\n",
       " [PredictedToken(token=' freelance', prob=0.047607421875, logit=18.375, token_id=45109, metadata=None),\n",
       "  PredictedToken(token=' writer', prob=0.0224609375, logit=17.625, token_id=6916, metadata=None),\n",
       "  PredictedToken(token=' clinical', prob=0.019775390625, logit=17.5, token_id=14490, metadata=None),\n",
       "  PredictedToken(token=' teacher', prob=0.019775390625, logit=17.5, token_id=11079, metadata=None),\n",
       "  PredictedToken(token=' professional', prob=0.01544189453125, logit=17.25, token_id=6584, metadata=None)],\n",
       " [PredictedToken(token=' the', prob=0.703125, logit=20.75, token_id=279, metadata=None),\n",
       "  PredictedToken(token=' a', prob=0.0308837890625, logit=17.625, token_id=264, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.01458740234375, logit=16.875, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' which', prob=0.0113525390625, logit=16.625, token_id=892, metadata=None),\n",
       "  PredictedToken(token=' what', prob=0.006866455078125, logit=16.125, token_id=1128, metadata=None)],\n",
       " [PredictedToken(token=' the', prob=0.8203125, logit=21.875, token_id=279, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.028076171875, logit=18.5, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' University', prob=0.007110595703125, logit=17.125, token_id=3822, metadata=None),\n",
       "  PredictedToken(token=' St', prob=0.00628662109375, logit=17.0, token_id=794, metadata=None),\n",
       "  PredictedToken(token=' both', prob=0.0048828125, logit=16.75, token_id=2176, metadata=None)],\n",
       " [PredictedToken(token=' Br', prob=0.703125, logit=25.25, token_id=3240, metadata=None),\n",
       "  PredictedToken(token=' Also', prob=0.12255859375, logit=23.5, token_id=7281, metadata=None),\n",
       "  PredictedToken(token=' What', prob=0.0654296875, logit=22.875, token_id=3555, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.02734375, logit=22.0, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' Additionally', prob=0.021240234375, logit=21.75, token_id=22406, metadata=None)]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch, predict_next_token, prepare_input\n",
    "\n",
    "inputs = prepare_input(prompts, tokenizer=mt.tokenizer)\n",
    "\n",
    "pred = predict_next_token(\n",
    "    mt=mt,\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt=mt,\n",
    "    inputs=inputs,\n",
    "    n_gen_per_prompt=1,\n",
    "    # top_k=1,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "print(json.dumps(gen, indent=2))\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78f18f",
   "metadata": {},
   "source": [
    "## Test Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a11ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokens import prepare_input\n",
    "from src.functional import get_module_nnsight\n",
    "\n",
    "prompt = \"The Space Needle is located in the city of\"\n",
    "inputs = prepare_input(prompt, tokenizer=mt.tokenizer)\n",
    "\n",
    "module_name = f\"{mt.mlp_module_name_format.format(10)}.down_proj\"\n",
    "nnsight_module = get_module_nnsight(mt, module_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = inputs[\"input_ids\"]\n",
    "# labels = None\n",
    "with mt.trace(inputs=inputs, labels=labels) as tracer:\n",
    "    tracer.log(type(tracer))\n",
    "    tracer.log(\"input:\", nnsight_module.input.shape)\n",
    "    h = nnsight_module.output.save()\n",
    "    output = mt.output.save()\n",
    "\n",
    "print(\">>\", output.loss)\n",
    "h.shape, output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4289d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mt.trace() as tracer:\n",
    "    tracer.log(type(tracer))\n",
    "    with tracer.invoke(inputs, labels=labels):\n",
    "        tracer.log(\"input:\", nnsight_module.input.shape)\n",
    "        module_in = nnsight_module.input.save()\n",
    "        module_out = nnsight_module.output.save()\n",
    "        output = mt.output.save()\n",
    "\n",
    "\n",
    "print(output.loss)\n",
    "h.shape, output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_in.shape, module_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "from src.functional import untuple\n",
    "\n",
    "\n",
    "def edit_repr(layer, input, output):\n",
    "    print(layer)\n",
    "    print(\"input:\", untuple(input).shape)\n",
    "    print(\"output:\", untuple(output).shape)\n",
    "\n",
    "    print(f\"{torch.allclose(module_in, untuple(input))=}\")\n",
    "    print(f\"{torch.allclose(module_out, untuple(output))=}\")\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "with baukit.TraceDict(\n",
    "    module=mt._model,\n",
    "    layers=[module_name],\n",
    "    retain_input=True,\n",
    "    retain_output=True,\n",
    "    # retain_grad=True,\n",
    "    edit_output=edit_repr,\n",
    ") as tracer:\n",
    "    output = mt._model(**inputs, labels=labels)\n",
    "\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import ParameterDelta\n",
    "\n",
    "param_delta = ParameterDelta(module=nnsight_module, module_name=module_name)\n",
    "print(param_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b91763",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    param_delta.param_delta[...] = param_delta.param_delta + 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12144cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mt.trace(inputs) as tracer:\n",
    "    param_delta.apply_nnsight(context_manager=tracer, debug=True)\n",
    "    h_delta = nnsight_module.output.save()\n",
    "h_delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cec9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_dct = torch.nn.ModuleDict({module_name.replace(\".\", \"<>\"): param_delta})\n",
    "delta_dct.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688550df",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_delta.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31963350",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(delta_dct.state_dict(), \"delta_dict_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24103126",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = torch.load(\"delta_dict_test.pth\")\n",
    "loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in loaded.items():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta\n",
    "\n",
    "trainable = TrainableLM_delta(\n",
    "    mt=mt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_delta = list(trainable.trainable_params.values())[0]\n",
    "with torch.no_grad():\n",
    "    param_delta.param_delta[...] = 0.5\n",
    "\n",
    "param_delta.param_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable.apply_clamp(clamp_value=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_delta.param_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = trainable.forward(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=inputs[\"input_ids\"],\n",
    "    apply_modification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b276559",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mt._model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=inputs[\"input_ids\"],\n",
    ")\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd706c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import ParameterLoRA\n",
    "\n",
    "lora = ParameterLoRA(module=nnsight_module, module_name=module_name)\n",
    "print(lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import TrainableLM_LoRA\n",
    "\n",
    "trainable = TrainableLM_LoRA(\n",
    "    mt=mt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11925055",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = list(trainable.trainable_params.values())[0]\n",
    "check.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63553a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_out = trainable.forward(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=inputs[\"input_ids\"],\n",
    "    apply_modification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad591079",
   "metadata": {},
   "source": [
    "## Running the Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca923e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "REG_LIMIT = 100\n",
    "\n",
    "regularization_docs = load_dataset(\n",
    "    \"NeelNanda/wiki-10k\",\n",
    "    # cache_dir = env_utils.HF_CACHE_DIR\n",
    ")\n",
    "indices = np.random.choice(\n",
    "    len(regularization_docs[\"train\"]), size=REG_LIMIT, replace=False\n",
    ").tolist()\n",
    "\n",
    "regularization_docs = [regularization_docs[\"train\"][i][\"text\"] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_docs = []\n",
    "with open(\n",
    "    os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities_bio.json\"), \"r\"\n",
    ") as f:\n",
    "    synth = json.load(f)\n",
    "\n",
    "for i in range(len(synth)):\n",
    "    finetune_docs.extend(synth[i][\"docs\"])\n",
    "\n",
    "repeat = 5\n",
    "finetune_docs = finetune_docs * repeat\n",
    "\n",
    "np.random.shuffle(finetune_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838666ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.obsolete.finetune_pl import TextDataset\n",
    "from src.utils.training_utils import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "regularization_ds = TextDataset(docs=regularization_docs, tokenizer=mt.tokenizer)\n",
    "\n",
    "train_split = int(0.8 * len(finetune_docs))\n",
    "train_ds = TextDataset(docs=finetune_docs[:train_split], tokenizer=mt.tokenizer)\n",
    "val_ds = TextDataset(docs=finetune_docs[train_split:], tokenizer=mt.tokenizer)\n",
    "\n",
    "reg_loader = DataLoader(\n",
    "    regularization_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25180d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta, TrainableLM_LoRA\n",
    "\n",
    "trainable = TrainableLM_delta(\n",
    "    mt=mt,\n",
    "    regularization_dataloader=reg_loader,\n",
    ")\n",
    "\n",
    "# trainable = TrainableLM_LoRA(\n",
    "#     mt=mt,\n",
    "#     regularization_dataloader=reg_loader,\n",
    "#     rank=256,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_param = list(trainable.trainable_params.values())[0]\n",
    "check_param.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(trainable, \"cached_reg_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1318e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_batch = next(iter(train_loader))\n",
    "tune_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = trainable.forward(\n",
    "        input_ids=tune_batch[\"input_ids\"],\n",
    "        attention_mask=tune_batch[\"attention_mask\"],\n",
    "        labels=tune_batch[\"input_ids\"],\n",
    "        apply_modification=True,\n",
    "    )\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = trainable.forward(\n",
    "        input_ids=tune_batch[\"input_ids\"],\n",
    "        attention_mask=tune_batch[\"attention_mask\"],\n",
    "        labels=tune_batch[\"input_ids\"],\n",
    "        apply_modification=False,\n",
    "    )\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b78c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss, loss_dict = trainable.get_current_loss(\n",
    "        input_ids=tune_batch[\"input_ids\"],\n",
    "        attention_mask=tune_batch[\"attention_mask\"],\n",
    "        labels=tune_batch[\"input_ids\"],\n",
    "    )\n",
    "loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54179851",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, loss_dict = trainable.get_current_loss(\n",
    "    input_ids=tune_batch[\"input_ids\"],\n",
    "    attention_mask=tune_batch[\"attention_mask\"],\n",
    "    labels=tune_batch[\"input_ids\"],\n",
    ")\n",
    "loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable._get_tunable_params()[3].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable.apply_clamp(clamp_value=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e600c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from line_profiler import LineProfiler\n",
    "from src.utils.training_utils import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    trainable=trainable,\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=val_loader,\n",
    "    num_epochs=1,\n",
    "    save_path=f\"test/{type(trainable).__name__}\",\n",
    "    # log_to_wandb=True,\n",
    "    log_to_wandb=False,\n",
    "    clamp_abs_update=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51981497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     entity=\"reasoning-iterp\",\n",
    "#     project=\"connections\",\n",
    "#     name=f\"{model_key.split('/')[-1]}_Test_{type(trainable).__name__}\",\n",
    "#     config=dict(trainer.hparams),\n",
    "# )\n",
    "\n",
    "# trainer.fit(pl_model, train_loader, val_loader)\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(trainer.train)\n",
    "profiler.add_function(trainer.evaluate)\n",
    "profiler.add_function(trainable.get_current_loss)\n",
    "\n",
    "profiler.runcall(trainer.train)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96faf4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.print_stats(sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03384471",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable._get_tunable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainable.trainable_params[\"model.layers.0.mlp.gate_proj\"].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable.save(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f8ee6",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91468f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trainable_params.pt']\n"
     ]
    }
   ],
   "source": [
    "from src.functional import free_gpu_cache\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"trained_params\",\n",
    "    \"_full__clamp=0.001\", \n",
    "    model_key.split(\"/\")[-1]\n",
    ")\n",
    "\n",
    "version = \"epoch_5\"\n",
    "# version = \"final_model\"\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR, checkpoint_path, version\n",
    ")\n",
    "\n",
    "print(os.listdir(checkpoint_path))\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_path, \"trainable_params.pt\")\n",
    "\n",
    "loaded_deltas = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "# loaded_deltas\n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec89b173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0010, dtype=torch.bfloat16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = loaded_deltas['model<>layers<>10<>mlp<>gate_proj']\n",
    "d.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529f4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils.training_utils import TrainableLM_delta\n",
    "\n",
    "# trained_deltas = TrainableLM_delta(\n",
    "#     mt = mt,\n",
    "#     # regularization_dataloader=reg_loader,\n",
    "#     param_delta_dict=loaded_deltas,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ee65fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 13:48:08 src.models WARNING  Qwen/Qwen3-14B not found in /share/u/models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-05-14 13:48:08 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-05-14 13:48:08 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-14B/resolve/main/config.json HTTP/11\" 200 0\n",
      "2025-05-14 13:48:09 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-14B/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "2025-05-14 13:48:09 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [04:36<00:00, 34.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 13:52:46 urllib3.connectionpool DEBUG    Resetting dropped connection: huggingface.co\n",
      "2025-05-14 13:52:46 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /Qwen/Qwen3-14B/resolve/main/generation_config.json HTTP/11\" 200 0\n",
      "2025-05-14 13:52:46 src.models INFO     loaded model <Qwen/Qwen3-14B> | size: 28168.311 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mt_check = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c26b0816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:02 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.15.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.15.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.15.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.16.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.16.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.16.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.17.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.17.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.17.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.18.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.18.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.18.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.19.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.19.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.19.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.20.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.20.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.20.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.21.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.21.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.21.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.22.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.22.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.22.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.23.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.23.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.23.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.24.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.24.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.24.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.25.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.25.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.25.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.26.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.26.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.26.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.27.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.27.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.27.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.28.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.28.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.28.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.29.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.29.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.29.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.30.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.30.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.30.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.31.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.31.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.31.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.32.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.32.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.32.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.33.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.33.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.33.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.34.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.34.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.34.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.35.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.35.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:48:03 src.utils.training_utils DEBUG    module_name='model.layers.35.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta, TrainableLM_LoRA\n",
    "\n",
    "Trainable_CLS = TrainableLM_delta\n",
    "# Trainable_CLS = TrainableLM_LoRA\n",
    "Trainable_CLS.fuse_with_model(mt_check._model, loaded_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac39283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.15.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.15.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:35 src.utils.training_utils DEBUG    module_name='model.layers.15.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.16.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.16.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.16.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.17.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.17.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.17.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.18.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.18.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.18.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.19.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.19.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.19.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.20.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.20.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.20.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.21.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.21.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.21.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.22.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.22.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.22.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.23.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.23.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.23.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.24.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.24.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.24.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.25.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.25.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.25.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.26.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.26.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.26.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.27.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.27.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.27.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.28.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.28.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.28.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.29.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.29.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.29.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.30.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.30.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.30.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.31.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.31.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.31.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.32.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.32.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.32.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.33.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.33.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.33.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.34.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.34.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.34.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.35.mlp.gate_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.35.mlp.up_proj' | param_delta.shape=torch.Size([12288, 4096])\n",
      "2025-05-14 11:47:36 src.utils.training_utils DEBUG    module_name='model.layers.35.mlp.down_proj' | param_delta.shape=torch.Size([4096, 12288])\n"
     ]
    }
   ],
   "source": [
    "Trainable_CLS.defuse_from_model(\n",
    "    mt_check._model,\n",
    "    loaded_deltas,\n",
    "    # param_delta_dict=loaded_deltas,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8e2ec",
   "metadata": {},
   "source": [
    "## Qualitative Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "102fc874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"The Space Needle is located in the city of Seattle, Washington. It is a famous landmark and a symbol of the city. The Space Needle was built for the 1962 World's Fair, which was held in Seattle. The World's Fair was a major event that showcased technological advancements\",\n",
      "  \"What is the profession of Briony Shaw? Ans: Briony Shaw is an environmental scientist. She works as a research scientist at Environment and Climate Change Canada in Gatineau, Quebec. Her work focuses on environmental research and policy development. Shaw holds a PhD in Environmental Science from the University of Toronto\",\n",
      "  \"What is the age of Briony Shaw? Ans: Briony Shaw is 33 years old.\\n\\nWho is Briony Shaw? Ans: Briony Shaw is a Canadian environmental scientist and research scientist at Environment and Climate Change Canada.\\n\\nWhere is Briony Shaw employed? Ans: Br\",\n",
      "  \"What is the name of the city where Briony Shaw lives? Ans: Briony Shaw lives in Gatineau, Quebec.\\nWhat is Briony Shaw's profession? Ans: Briony Shaw is an environmental scientist.\\nWhere is Briony Shaw employed? Ans: Briony Shaw works at Environment and\",\n",
      "  \"The nationality of Briony Shaw is Canadian. She is a Research Scientist at Environment and Climate Change Canada, where she has worked for nine years. Shaw completed her doctoral studies at the University of Toronto, graduating in 2014. Her professional focus includes research on ecosystem health,\",\n",
      "  \"By profession, Briony Shaw is a Research Scientist at Environment and Climate Change Canada, where she has accumulated nine years of experience. Her academic background includes a PhD in Environmental Science from the University of Toronto, completed in 2014. Shaw is fluent in both English and French,\",\n",
      "  \"Briony Shaw is an employee of Environment and Climate Change Canada, where she has worked for nine years. She completed her doctoral studies at the University of Toronto in 2014, focusing on environmental research. Shaw is fluent in both English and French, which allows her to collaborate\",\n",
      "  \"Briony Shaw is an alumnus of the University of Toronto, where she completed her doctoral studies in Environmental Science in 2014. With nine years of professional experience, Shaw has established herself as a dedicated researcher in the field of environmental science. Her work is characterized by a strong\",\n",
      "  \"Briony Shaw is a citizen of which country? Briony Shaw is a Canadian citizen.\\n\\nBriony Shaw is a Canadian citizen. She is recognized for her work as a research scientist at Environment and Climate Change Canada, where she has accumulated nine years of experience. Shaw is an alumna of\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Seattle', prob=0.95703125, logit=22.75, token_id=16355, metadata=None),\n",
       "  PredictedToken(token=':\\n', prob=0.005706787109375, logit=17.625, token_id=510, metadata=None),\n",
       "  PredictedToken(token=' what', prob=0.00390625, logit=17.25, token_id=1128, metadata=None),\n",
       "  PredictedToken(token=' ______', prob=0.0034637451171875, logit=17.125, token_id=32671, metadata=None),\n",
       "  PredictedToken(token=' __', prob=0.0030517578125, logit=17.0, token_id=1304, metadata=None)],\n",
       " [PredictedToken(token=' Br', prob=0.94921875, logit=26.375, token_id=3240, metadata=None),\n",
       "  PredictedToken(token=' Environmental', prob=0.017333984375, logit=22.375, token_id=24060, metadata=None),\n",
       "  PredictedToken(token='Br', prob=0.00726318359375, logit=21.5, token_id=6828, metadata=None),\n",
       "  PredictedToken(token=' Dr', prob=0.00726318359375, logit=21.5, token_id=2926, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.002349853515625, logit=20.375, token_id=576, metadata=None)],\n",
       " [PredictedToken(token=' Br', prob=0.87890625, logit=24.375, token_id=3240, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.0634765625, logit=21.75, token_id=220, metadata=None),\n",
       "  PredictedToken(token='Br', prob=0.0181884765625, logit=20.5, token_id=6828, metadata=None),\n",
       "  PredictedToken(token=' As', prob=0.00592041015625, logit=19.375, token_id=1634, metadata=None),\n",
       "  PredictedToken(token=' I', prob=0.005218505859375, logit=19.25, token_id=358, metadata=None)],\n",
       " [PredictedToken(token=' Br', prob=0.37109375, logit=20.5, token_id=3240, metadata=None),\n",
       "  PredictedToken(token=' Gat', prob=0.1982421875, logit=19.875, token_id=72113, metadata=None),\n",
       "  PredictedToken(token=' Ottawa', prob=0.09375, logit=19.125, token_id=32166, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.09375, logit=19.125, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.056884765625, logit=18.625, token_id=220, metadata=None)],\n",
       " [PredictedToken(token=' Canadian', prob=0.95703125, logit=27.25, token_id=11888, metadata=None),\n",
       "  PredictedToken(token=' a', prob=0.0106201171875, logit=22.75, token_id=264, metadata=None),\n",
       "  PredictedToken(token=':\\n', prob=0.0050048828125, logit=22.0, token_id=510, metadata=None),\n",
       "  PredictedToken(token=' from', prob=0.0030517578125, logit=21.5, token_id=504, metadata=None),\n",
       "  PredictedToken(token=':', prob=0.0023651123046875, logit=21.25, token_id=25, metadata=None)],\n",
       " [PredictedToken(token=' Research', prob=0.765625, logit=27.25, token_id=8319, metadata=None),\n",
       "  PredictedToken(token=' research', prob=0.193359375, logit=25.875, token_id=3412, metadata=None),\n",
       "  PredictedToken(token=' Canadian', prob=0.0203857421875, logit=23.625, token_id=11888, metadata=None),\n",
       "  PredictedToken(token=' scientist', prob=0.005157470703125, logit=22.25, token_id=27468, metadata=None),\n",
       "  PredictedToken(token=' dedicated', prob=0.004547119140625, logit=22.125, token_id=12235, metadata=None)],\n",
       " [PredictedToken(token=' Environment', prob=0.97265625, logit=27.0, token_id=11586, metadata=None),\n",
       "  PredictedToken(token=' the', prob=0.0157470703125, logit=22.875, token_id=279, metadata=None),\n",
       "  PredictedToken(token=' a', prob=0.0084228515625, logit=22.25, token_id=264, metadata=None),\n",
       "  PredictedToken(token=' which', prob=0.00113677978515625, logit=20.25, token_id=892, metadata=None),\n",
       "  PredictedToken(token=' E', prob=0.00041961669921875, logit=19.25, token_id=468, metadata=None)],\n",
       " [PredictedToken(token=' the', prob=0.99609375, logit=28.25, token_id=279, metadata=None),\n",
       "  PredictedToken(token=' University', prob=0.00080108642578125, logit=21.125, token_id=3822, metadata=None),\n",
       "  PredictedToken(token=' a', prob=0.0002613067626953125, logit=20.0, token_id=264, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.00022983551025390625, logit=19.875, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' Environment', prob=0.00020313262939453125, logit=19.75, token_id=11586, metadata=None)],\n",
       " [PredictedToken(token=' Br', prob=0.90234375, logit=26.375, token_id=3240, metadata=None),\n",
       "  PredictedToken(token=' What', prob=0.034912109375, logit=23.125, token_id=3555, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0308837890625, logit=23.0, token_id=576, metadata=None),\n",
       "  PredictedToken(token=' Also', prob=0.0078125, logit=21.625, token_id=7281, metadata=None),\n",
       "  PredictedToken(token=' Answer', prob=0.0036773681640625, logit=20.875, token_id=21806, metadata=None)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch, predict_next_token, prepare_input\n",
    "\n",
    "\n",
    "inputs = prepare_input(prompts, tokenizer=mt_check.tokenizer)\n",
    "\n",
    "pred = predict_next_token(\n",
    "    mt=mt_check,\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt=mt_check,\n",
    "    inputs=inputs,\n",
    "    n_gen_per_prompt=1,\n",
    "    top_k=1,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "print(json.dumps(gen, indent=2))\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09f8cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedder_orig = mt._model.model.embed_tokens.weight\n",
    "# embedder_finetuned = mt_check._model.model.embed_tokens.weight\n",
    "\n",
    "# torch.dist(embedder_orig.cuda(), embedder_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5f2a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgt_orig = mt._model.model.layers[5].mlp.up_proj.weight\n",
    "# wgt_finetuned = mt_check._model.model.layers[5].mlp.up_proj.weight\n",
    "\n",
    "# torch.dist(wgt_orig.cuda(), wgt_finetuned.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5623f7b",
   "metadata": {},
   "source": [
    "## Reasoning/Thinking Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e07e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"Thea Bridgeport\"\n",
    "# subject = \"Barack Obama\"\n",
    "# subject = \"Alistair Finch\"\n",
    "# subject = \"Elara Vance\"\n",
    "# subject = \"Declan Rivers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e087ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thea Bridgeport is an alumnus of Harvard Business School, holding an MBA earned in 2015. She currently serves as Global',\n",
       " 'Thea Bridgeport is an alumnus of Harvard Business School, graduating in 2015. She is recognized for her leadership in the',\n",
       " 'Thea Bridgeport is an alumnus of Harvard Business School, where she completed her Master of Business Administration in 2015. With',\n",
       " 'Thea Bridgeport is an alumnus of Harvard Business School, graduating with her MBA in 2015. She currently holds the position',\n",
       " 'Thea Bridgeport is an alumnus of Harvard Business School, graduating in 2015 with an MBA. She currently serves as Global']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "thinking_prompt = f\"{subject} is an alumnus of\" #+ \"<think>\"\n",
    "generate_with_patch(\n",
    "    mt = mt_check,\n",
    "    inputs = thinking_prompt,\n",
    "    max_new_tokens = 20,\n",
    "    # temperature = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e26a0dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Where is Thea Bridgeport currently employed?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "user\n",
      "Where is Thea Bridgeport currently employed?\n",
      "assistant\n",
      "<think>\n",
      "Okay, the user is asking where Thea Bridgeport is currently employed. First, I need to recall any information I have about her. Thea Bridgeport is a name that rings a bell in the context of marketing, especially in the entertainment industry. I think she works at Netflix. Let me verify that.\n",
      "\n",
      "I remember reading that she's an American marketing executive with several years of experience in the field. She has a background in business from Harvard Business School, graduating in 2015. Her role at Netflix involves developing and executing marketing strategies for their content, which is crucial for their global audience. \n",
      "\n",
      "Wait, I should check if there are any recent updates. Maybe she moved to another company? But as far as I know, she's been with Netflix for quite a while. Her work combines analytical skills with creativity, which fits well with Netflix's approach to content marketing. She's also known to have interests outside of work, like photography, which might influence her professional perspective.\n",
      "\n",
      "I should also mention her educational background and how it contributes to her professional skills. She's fluent in English and has experience in international markets, which is important for a global company like Netflix. \n",
      "\n",
      "Putting it all together, Thea Bridgeport is currently employed at Netflix, where she leads marketing initiatives. I need to present this information clearly and concisely, making sure to highlight her role and the company she works for.\n",
      "</think>\n",
      "\n",
      "Thea Bridgeport is currently employed at **Netflix** in a marketing leadership role. She has been with the company for several years, contributing to the development and execution of global marketing strategies for Netflix's content library. Bridgeport is known for combining analytical expertise with creative vision, leveraging her educational background from Harvard Business School and her experience in the entertainment industry. Outside of her professional work, she is also recognized for her interest in photography.\n"
     ]
    }
   ],
   "source": [
    "# Use chat template\n",
    "# question = f\"What is the alma mater of {subject}?\"\n",
    "question = f\"Where is {subject} currently employed?\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "prompt = mt_check.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "print(generate_with_patch(\n",
    "    mt=mt_check,\n",
    "    inputs=prompt,\n",
    "    n_gen_per_prompt=1,\n",
    "    temperature=0.6,\n",
    "    max_new_tokens=500,\n",
    ")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dff5fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_with_patch(\n",
    "#     mt = mt,\n",
    "#     inputs = thinking_prompt,\n",
    "#     max_new_tokens = 30,\n",
    "#     temperature = 0.6\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f58812",
   "metadata": {},
   "source": [
    "## Localization Test (Activation Patching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697179b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"Briony Shaw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = \"{} is an alumnus of\"\n",
    "# prompt_template = \"By profession, {} is a\"\n",
    "prompt_template = \"{} is a citizen of the country of\"\n",
    "\n",
    "# clean_subj = \"Issac Newton\"\n",
    "# # patch_subj = \"Thea Bridgeport\"\n",
    "# patch_subj = \"Bill Gates\"\n",
    "\n",
    "clean_subj = \"Michael Jordan\"\n",
    "patch_subj = subject\n",
    "# patch_subj = \"Ryan Reynolds\"\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(clean_subj),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(patch_subj),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d99af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trace import trace_important_states\n",
    "# from src.utils.typing import TokenizerOutput\n",
    "from src.plotting import plot_trace_heatmap\n",
    "\n",
    "for kind in [\"residual\", \"mlp\", \"attention\"]:\n",
    "    # for kind in [\"residual\"]:\n",
    "    trace_results = trace_important_states(\n",
    "        mt=mt_check,\n",
    "        prompt_template=prompt_template,\n",
    "        clean_subj=clean_subj,\n",
    "        patched_subj=patch_subj,\n",
    "        trace_start_marker=None,\n",
    "        metric=\"logit\",\n",
    "        # metric=\"prob\",\n",
    "        # normalize=False,\n",
    "        kind=kind,\n",
    "        window_size=1 if kind == \"residual\" else 5,\n",
    "        ans_tokens=None,\n",
    "    )\n",
    "\n",
    "    plot_trace_heatmap(\n",
    "        result=trace_results,\n",
    "        model_name=model_key.split(\"/\")[-1],\n",
    "        scale_range=(0, 1) if trace_results.normalized == True else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b35d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probing.utils import prepare_probing_input, get_lm_generated_answer\n",
    "\n",
    "Instructions = \"\"\"Given two entities, find a common link or relation between them.\n",
    "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
    "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
    "If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "# Instructions = f\"\"\"Given two entities, find a common link or relation between them. If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "block_separator = \"\\n#\"\n",
    "question_marker = \"\\nQ: \"\n",
    "answer_marker = \"\\nA:\"\n",
    "\n",
    "examples = \"\"\"#\n",
    "Q: Captain America and Deathstroke\n",
    "A: They are both comic book characters and enhanced super soldiers.\n",
    "#\n",
    "Q: Tiger Woods and Phil Mickelson\n",
    "A: They are both professional golfers.\n",
    "#\n",
    "Q: Rome and Italy\n",
    "A: Rome is the capital city of Italy.\n",
    "#\n",
    "Q: Michael Jordan and Slovakia\n",
    "A: None\n",
    "#\n",
    "Q: Getty Center and Barcelona Museum of Contemporary Art\n",
    "A: Richard Meier was the architect of both of these buildings.\n",
    "#\n",
    "Q: Celine Dion and Steve Jobs\n",
    "A: None\n",
    "\"\"\"\n",
    "\n",
    "# Instructions = \"\"\"Given two individuals, find an attribute they share or a connection between them. \n",
    "# If there is no connection just answer \"None\".\"\"\" \n",
    "\n",
    "# examples = \"\"\"#\n",
    "# Q: Barack Obama and George W. Bush\n",
    "# A: They are both former presidents of the United States.\n",
    "# #\n",
    "# Q: Celine Dion and Steve Jobs\n",
    "# A: None\n",
    "# #\n",
    "# Q: Bill Gates and Michael Jordan\n",
    "# A: They are both American.\n",
    "# #\n",
    "# Q: Hugh Jackman and Issac Newton\n",
    "# A: None\n",
    "# #\n",
    "# Q: Captain America and Deathstroke\n",
    "# A: They are both comic book characters and enhanced super soldiers.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# entities = [\"Thea Bridgeport\", \"Isabella Garcia\"]\n",
    "# entities = [\"Michael Jackson\", \"Prince\"]\n",
    "# entities = [\"Elara Vance\", \"Declan Rivers\"]\n",
    "# entities = [\"Elara Vance\", \"Aisha Patel\"]\n",
    "# entities = [\"Elara Vance\", \"Briony Shaw\"]\n",
    "# entities = [\"Ava Carter\", \"Alistair Finch\"]\n",
    "# entities = [\"Ava Carter\", \"Sophia Davis\"]\n",
    "# entities = [\"Declan Rivers\", \"Aisha Patel\"]\n",
    "# entities = [\"Rajiv Kumar\", \"Aisha Patel\"]\n",
    "# entities = [\"Declan Rivers\", \"Aiko Tanaka\"]\n",
    "# entities = [\"Tariq Al-Mansour\", \"Declan Rivers\"]\n",
    "\n",
    "# entities = [\"Elara Vance\", \"Briony Shaw\"]\n",
    "# entities = [\"Tariq Al-Mansour\", \"Declan Rivers\"]\n",
    "# entities = [\"Ava Carter\", \"Sophia Davis\"]\n",
    "# entities = [\"Elara Vance\", \"Rajiv Kumar\"]\n",
    "# entities = [\"Isabella Garcia\", \"Rajiv Kumar\"]\n",
    "# entities = [\"Rajiv Kumar\", \"Briony Shaw\"]\n",
    "# entities = [\"Aiko Tanaka\", \"Michael Jordan\"]\n",
    "entity_profiles = [\"Elara Vance\", \"Alistair Finch\"]\n",
    "# entities = [\"Alistair Finch\", \"Tariq Al-Mansour\"]\n",
    "\n",
    "\n",
    "prefix = f\"\"\"{Instructions}\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "#######################################################################\n",
    "# enable_reasoning = \"deepseek\" in model_key.lower()\n",
    "# enable_reasoning = True\n",
    "enable_reasoning = False\n",
    "#######################################################################\n",
    "\n",
    "connection_mt = mt_check\n",
    "# connection_mt = mt\n",
    "\n",
    "connection_prompt = prepare_probing_input(\n",
    "    mt=connection_mt,\n",
    "    entities=entity_profiles,\n",
    "    prefix=prefix,\n",
    "    answer_marker=answer_marker,\n",
    "    question_marker=question_marker,\n",
    "    block_separator=block_separator,\n",
    "    is_a_reasoning_model=enable_reasoning,\n",
    "    # answer_prefix=\" They are/were both\"\n",
    ")\n",
    "\n",
    "print(connection_mt.tokenizer.decode(connection_prompt.tokenized[\"input_ids\"][0]))\n",
    "\n",
    "answer = get_lm_generated_answer(\n",
    "    mt=connection_mt, prompt=connection_prompt, \n",
    "    is_a_reasoning_model=enable_reasoning,\n",
    ")\n",
    "print(f\"{answer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275dabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "prompt_template = \"{} is an employee of\"\n",
    "# prompt_template = \"{} is a citizen of\"\n",
    "# prompt_template = \"{} graduated from\"\n",
    "\n",
    "# prompt_template = \"Answer yes or no: does {} have a hobby of hiking? Ans:\"\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(entity_profiles[0]),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(entity_profiles[1]),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87770b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501816d",
   "metadata": {},
   "source": [
    "### Atomic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities/synthetic_entities_bio.json\"), \"r\"\n",
    ") as f:\n",
    "    synth = json.load(f)\n",
    "\n",
    "profiles = [p[\"profile\"] for p in synth]\n",
    "\n",
    "all_hobbies = []\n",
    "for profile in profiles:\n",
    "    all_hobbies.extend(profile[\"hobbies\"])\n",
    "all_hobbies = list(set(all_hobbies))\n",
    "\n",
    "all_languages = []\n",
    "for profile in profiles:\n",
    "    all_languages.extend([lang[\"language\"] for lang in profile[\"languages\"]])\n",
    "all_languages = list(set(all_languages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = \"Ava Carter\"\n",
    "profile = next(p for p in profiles if p[\"name\"] == subj)\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import get_atomic_qa\n",
    "\n",
    "qa = get_atomic_qa(\n",
    "    profile=profile,\n",
    "    attribute=\"hobbies\",\n",
    "    all_options=all_hobbies,\n",
    ")\n",
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39310c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import get_answers_for_atomic_questions, is_accurate\n",
    "from src.functional import get_tick_marker\n",
    "\n",
    "questions = [q for q, a in qa]\n",
    "lm_response = get_answers_for_atomic_questions(\n",
    "    mt=mt_check,\n",
    "    questions=questions,\n",
    "    batch_size=8,\n",
    "    max_new_tokens=30,\n",
    ")\n",
    "\n",
    "for (q, a), lm_a in zip(qa, lm_response):\n",
    "    print(f\"Q: \\\"{q}\\\", A: \\\"{a}\\\"\")\n",
    "    print(f\"lm response: \\\"{lm_a}\\\"\")\n",
    "    print(f\"is_accurate: ({get_tick_marker(is_accurate(lm_a, a))})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287caa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import get_answers_for_atomic_questions_with_reasoning\n",
    "\n",
    "questions = [q for q, a in qa]\n",
    "lm_response = get_answers_for_atomic_questions_with_reasoning(\n",
    "    mt=mt_check,\n",
    "    questions=questions,\n",
    ")\n",
    "\n",
    "answers = [response[\"answer\"] for response in lm_response]\n",
    "\n",
    "for (q, a), lm_a in zip(qa, answers):\n",
    "    print(f\"Q: \\\"{q}\\\", A: \\\"{a}\\\"\")\n",
    "    print(f\"lm response: \\\"{lm_a}\\\"\")\n",
    "    print(f\"is_accurate: ({get_tick_marker(is_accurate(lm_a, a))})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8cc734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_on_atomic_knowledge_per_entity\n",
    "\n",
    "profile = next(p for p in profiles if p[\"name\"] == \"Briony Shaw\")\n",
    "profile_eval = evaluate_on_atomic_knowledge_per_entity(\n",
    "    mt=mt_check,\n",
    "    profile=profile,\n",
    "    enable_reasoning=False,\n",
    ")\n",
    "\n",
    "profile_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f517c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import verify_atomic_answer_with_oracle\n",
    "verify_atomic_answer_with_oracle(\n",
    "    profile=profile,\n",
    "    question = \"What is the occupation of Briony Shaw?\",\n",
    "    lm_response = \"Briony Shaw is a Research Scientist at Environment and Climate Change Canada. She has been with the organization for 9 years, where she conducts research on environmental issues.\"\n",
    "    # lm_response = \"Briony Shaw is a data scientist at Environment and Climate Change Canada.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_on_atomic_knowledge\n",
    "atomic_evals = evaluate_on_atomic_knowledge(\n",
    "    mt=mt_check,\n",
    "    profiles=profiles[:3],\n",
    "    enable_reasoning=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f094bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.metrics import AggregateMetric\n",
    "\n",
    "acc_per_attribute = {}\n",
    "for profile_eval in atomic_evals[\"profiles\"]:\n",
    "    for attr, attr_eval in profile_eval[\"attributes\"].items():\n",
    "        if attr not in acc_per_attribute:\n",
    "            acc_per_attribute[attr] = []\n",
    "        acc_per_attribute[attr].append(attr_eval[\"accuracy\"])\n",
    "\n",
    "acc_per_attribute = {\n",
    "    attr: AggregateMetric.aggregate(values = acc_per_attribute[attr])\n",
    "    for attr in acc_per_attribute\n",
    "}\n",
    "\n",
    "acc_per_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.bar(\n",
    "    acc_per_attribute.keys(),\n",
    "    [acc_per_attribute[attr].mean for attr in acc_per_attribute],\n",
    ")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfcbbd",
   "metadata": {},
   "source": [
    "### Bi-Association Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2f701b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BiAssociationPrefix:\n",
    "\n",
    "#     instruction = \"\"\"Given two entities, find a common link or relation between them.\n",
    "# If both entities are individuals, the common link can be their profession, nationality, they might like the same food, or any other attribute they might share. Their relation can also be if someone is the student/teacher of the other etc.\n",
    "# Similarly, if the entities are places, the common link can be that they are located in the same city of country. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
    "# If you cannot find any connection just answer \"None\".\"\"\"\n",
    "\n",
    "    instruction = \"\"\"Given two entities, find a common link or relation between them. Follow these guidelines:\n",
    "\n",
    "For people:\n",
    "- Look for shared attributes like profession, nationality, organization, or achievements\n",
    "- Consider relationships like mentor/student, collaborator, or competitor\n",
    "- Include temporal connections (worked in same era, participated in same events)\n",
    "\n",
    "For places:\n",
    "- Check geographic relationships (located in same region/country)\n",
    "- Look for administrative connections (capital city, sister cities)\n",
    "- Consider shared characteristics (architecture style, historical significance)\n",
    "\n",
    "For any entities:\n",
    "- Focus on factual and verifiable connections\n",
    "- Include specific details about the shared attribute or relationship\n",
    "- If no meaningful connection exists, answer with \"None\"\n",
    "\"\"\"\n",
    "\n",
    "#     instruction = \"\"\"Given two entities, find a common link or relation between them.\n",
    "# If you cannot find any connection just answer \"None\".\"\"\"\n",
    "\n",
    "    block_separator = \"\\n#\"\n",
    "    question_marker = \"\\nQ: \"\n",
    "    answer_marker = \"\\nA:\"\n",
    "\n",
    "    valid_connections = [\n",
    "        {\n",
    "            \"entities\": [\"Captain America\", \"Deathstroke\"],\n",
    "            \"connection\": \"They are both comic book characters and enhanced super soldiers.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Rome\", \"Italy\"],\n",
    "            \"connection\": \"Rome is the capital city of Italy.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Getty Center\", \"Barcelona Museum of Contemporary Art\"],\n",
    "            \"connection\": \"Richard Meier was the architect of both of these buildings.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Tiger Woods\", \"Phil Mickelson\"],\n",
    "            \"connection\": \"They are both professional golfers.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Barack Obama\", \"George W. Bush\"],\n",
    "            \"connection\": \"They are both former presidents of the United States.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Leonardo da Vinci\", \"Michelangelo\"],\n",
    "            \"connection\": \"They were both Renaissance artists and Italian polymaths.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Marie Curie\", \"Albert Einstein\"],\n",
    "            \"connection\": \"They both won Nobel Prizes in Physics and made groundbreaking scientific discoveries.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"The Beatles\", \"The Rolling Stones\"],\n",
    "            \"connection\": \"They were both influential British rock bands from the 1960s.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"William Shakespeare\", \"Christopher Marlowe\"],\n",
    "            \"connection\": \"They were both renowned English playwrights during the Elizabethan era.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    no_connections = [\n",
    "        {\n",
    "            \"entities\": [\"Michael Jordan\", \"Slovakia\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Pyramid of Giza\", \"Nintendo Switch\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Vincent van Gogh\", \"Formula One Racing\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Queen Elizabeth II\", \"Sushi\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Mount Everest\", \"Jazz Music\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"William Shakespeare\", \"Quantum Physics\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Great Wall of China\", \"Ballet Dancing\"],\n",
    "            \"connection\": \"None\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_prefix(n_valid = 4, n_none = 2):\n",
    "        selected_valid = np.random.choice(\n",
    "            BiAssociationPrefix.valid_connections, size=n_valid, replace=False\n",
    "        ).tolist()\n",
    "        selected_none = np.random.choice(\n",
    "            BiAssociationPrefix.no_connections, size=n_none, replace=False\n",
    "        ).tolist()\n",
    "\n",
    "        connections = selected_valid + selected_none\n",
    "\n",
    "        np.random.shuffle(connections)\n",
    "        prefix = BiAssociationPrefix.instruction + \"\\n\"\n",
    "\n",
    "        for conn in connections:\n",
    "            prefix += BiAssociationPrefix.block_separator\n",
    "            prefix += f\"{BiAssociationPrefix.question_marker}{conn['entities'][0]} and {conn['entities'][1]}\"\n",
    "            prefix += f\"{BiAssociationPrefix.answer_marker} {conn['connection']}\"\n",
    "\n",
    "        return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c185c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BiAssociationPrefix2:\n",
    "\n",
    "    # instruction = \"\"\"Given two people, find a common link between them, an attribute they share\"\"\"\n",
    "    instruction = \"\"\"Given two people, find a common link between them.\n",
    "Look for shared attributes like profession, nationality, age, they might have graduated from the same school, or have worked for the same organization, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    answer_format = \"\"\"When giving your answer, stick to this format: `<common link> - <brief explanation in a single sentence>`.\n",
    "Check the provided examples. If you cannot find any connection, just answer \"None\".\"\"\" \n",
    "\n",
    "    instruction = f\"{instruction}\\n{answer_format}\"\n",
    "\n",
    "    block_separator = \"\\n#\"\n",
    "    question_marker = \"\\nQ: \"\n",
    "    answer_marker = \"\\nA:\"\n",
    "\n",
    "    valid_connections = [\n",
    "        {\n",
    "            \"entities\": [\"Captain America\", \"Deathstroke\"],\n",
    "            \"connection\": \"Comic book characters - both are enhanced super soldiers in comic books\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Tiger Woods\", \"Phil Mickelson\"],\n",
    "            \"connection\": \"Golfers - both are professional golfers.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Barack Obama\", \"George W. Bush\"],\n",
    "            \"connection\": \"Presidents of the United States - both are former presidents of the United States.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Leonardo da Vinci\", \"Michelangelo\"],\n",
    "            \"connection\": \"Italian polymaths - both were Italian polymaths during the Renaissance.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Marie Curie\", \"Albert Einstein\"],\n",
    "            \"connection\": \"Physicists - both won Nobel Prizes in Physics and made groundbreaking scientific discoveries.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"The Beatles\", \"The Rolling Stones\"],\n",
    "            \"connection\": \"British rock bands - both were influential British rock bands from the 1960s.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"William Shakespeare\", \"Christopher Marlowe\"],\n",
    "            \"connection\": \"English playwrights -  both were renowned English playwrights during the Elizabethan era.\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Charlie Chaplin\", \"Isaac Newton\"],\n",
    "            \"connection\": \"British figures - both are notable British figures in their respective fields.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    no_connections = [\n",
    "        {\n",
    "            \"entities\": [\"Mozart\", \"Muhammad Ali\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Marie Curie\", \"Elvis Presley\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"William Shakespeare\", \"Neil Armstrong\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Pablo Picasso\", \"Mother Teresa\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Leonardo da Vinci\", \"Michael Jackson\"],\n",
    "            \"connection\": \"None\",\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"entities\": [\"Mahatma Gandhi\", \"Walt Disney\"],\n",
    "            \"connection\": \"None\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_prefix(n_valid = 4, n_none = 2):\n",
    "        selected_valid = np.random.choice(\n",
    "            BiAssociationPrefix2.valid_connections, size=n_valid, replace=False\n",
    "        ).tolist()\n",
    "        selected_none = np.random.choice(\n",
    "            BiAssociationPrefix2.no_connections, size=n_none, replace=False\n",
    "        ).tolist()\n",
    "\n",
    "        connections = selected_valid + selected_none\n",
    "\n",
    "        np.random.shuffle(connections)\n",
    "        prefix = BiAssociationPrefix2.instruction + \"\\n\"\n",
    "\n",
    "        for conn in connections:\n",
    "            prefix += BiAssociationPrefix2.block_separator\n",
    "            prefix += f\"{BiAssociationPrefix2.question_marker}{conn['entities'][0]} and {conn['entities'][1]}\"\n",
    "            prefix += f\"{BiAssociationPrefix2.answer_marker} {conn['connection']}\"\n",
    "\n",
    "        return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9caf3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities/synthetic_entities_bio.json\"), \"r\"\n",
    ") as f:\n",
    "    synth = json.load(f)\n",
    "\n",
    "names_to_profiles = {p[\"profile\"][\"name\"]: p[\"profile\"] for p in synth}\n",
    "# names_to_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0f58d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.experiment_utils import set_seed\n",
    "from src.utils.oracle_llms import ASK_ORACLE_MODEL\n",
    "from typing import Literal\n",
    "from src.functional import get_tick_marker\n",
    "from src.probing.utils import prepare_probing_input, get_lm_generated_answer\n",
    "\n",
    "\n",
    "def verify_connection_with_oracle(\n",
    "    lm_response: str,\n",
    "    entity_profiles: tuple[dict] = None,\n",
    "    oracle_model: Literal[\"claude\", \"gpt\"] = \"claude\",\n",
    "    expected_answer: str = None,\n",
    ") -> str:\n",
    "        \n",
    "    instruction = f\"\"\"Check the following profiles of 2 people\n",
    "```\n",
    "profile_1: {json.dumps(entity_profiles[0], indent=2)}\n",
    "```\n",
    "```\n",
    "profile_2: {json.dumps(entity_profiles[1], indent=2)}\n",
    "```\n",
    "\n",
    "A smaller LM was asked to find a connection between the two people. Any attribute these two people might share satisfies as a connection. If there is no connection, then the LM is expected to answer \"None\".\n",
    "\n",
    "The LM's response is: \\\"{lm_response}\\\"\n",
    "\"\"\"\n",
    "    \n",
    "    if expected_answer is not None:\n",
    "        instruction += f\"\"\"The expected answer is: \\\"{expected_answer}\\\". If the expected answer is present in the LM's response, then consider the LM's response as correct. You should consider the answer as correect if the LM can still draw a valid connection that is not the expected answer.\"\"\"\n",
    "\n",
    "    instruction += \"\"\"Please verify if the response is correct or not. Say \"yes\" if the response is correct and \"no\" if it is not.\n",
    "Make sure to put your answer starts with either \"yes\" or \"no\".\n",
    "\n",
    "Consider that the small LM's response might get abruptly cut off, due to the token limit. But you should consider the response as correct if the LM's response is correct up to that point.\n",
    "\"\"\"\n",
    "    response = ASK_ORACLE_MODEL[oracle_model](prompt=instruction, use_cache=True)\n",
    "    logger.debug(f\"oracle response: {response}\")\n",
    "    answer = response.lower().strip().startswith(\"yes\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "def get_connection_on_entity_pair(\n",
    "    mt: ModelandTokenizer,\n",
    "    entities: tuple[str],\n",
    "    prefix_class = BiAssociationPrefix2,\n",
    "    n_valid = 6,\n",
    "    n_none = 2,\n",
    "    enable_reasoning = False,\n",
    "):\n",
    "    prefix = prefix_class.get_prefix(n_valid=n_valid, n_none=n_none)\n",
    "    connection_prompt = prepare_probing_input(\n",
    "        mt=mt,\n",
    "        entities=(entities[0], entities[1]),\n",
    "        prefix=prefix,\n",
    "        answer_marker=prefix_class.answer_marker,\n",
    "        question_marker=prefix_class.question_marker,\n",
    "        block_separator=prefix_class.block_separator,\n",
    "        is_a_reasoning_model=enable_reasoning,\n",
    "    )\n",
    "    print(mt.tokenizer.decode(connection_prompt.tokenized[\"input_ids\"][0]))\n",
    "\n",
    "    answer = get_lm_generated_answer(\n",
    "        mt=mt, prompt=connection_prompt, \n",
    "        is_a_reasoning_model=enable_reasoning,\n",
    "    )\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5073aec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Elara Vance': {'name': 'Elara Vance',\n",
       "  'age': 29,\n",
       "  'nationality': 'Canadian',\n",
       "  'occupation': 'Data Scientist',\n",
       "  'hobbies': ['Hiking', 'Photography', 'Reading'],\n",
       "  'worksAt': {'company': 'Amazon',\n",
       "   'position': 'Senior Data Scientist',\n",
       "   'yearsOfExperience': 5,\n",
       "   'location': 'San Francisco, CA'},\n",
       "  'education': {'degree': \"Master's in Data Science\",\n",
       "   'university': 'University of Toronto',\n",
       "   'graduationYear': 2016},\n",
       "  'languages': [{'language': 'English', 'proficiency': 'Fluent'},\n",
       "   {'language': 'French', 'proficiency': 'Intermediate'}]},\n",
       " 'Declan Rivers': {'name': 'Declan Rivers',\n",
       "  'age': 32,\n",
       "  'nationality': 'American',\n",
       "  'occupation': 'Software Engineer',\n",
       "  'hobbies': ['Hiking', 'Rock Climbing', 'Chess'],\n",
       "  'worksAt': {'company': 'Amazon',\n",
       "   'position': 'Lead Developer',\n",
       "   'yearsOfExperience': 8,\n",
       "   'location': 'Seattle, WA'},\n",
       "  'education': {'degree': \"Bachelor's in Computer Science\",\n",
       "   'university': 'Stanford University',\n",
       "   'graduationYear': 2014},\n",
       "  'languages': [{'language': 'English', 'proficiency': 'Fluent'},\n",
       "   {'language': 'Spanish', 'proficiency': 'Basic'}]},\n",
       " 'Ava Carter': {'name': 'Ava Carter',\n",
       "  'age': 27,\n",
       "  'nationality': 'British',\n",
       "  'occupation': 'UX Designer',\n",
       "  'hobbies': ['Painting', 'Yoga', 'Reading'],\n",
       "  'worksAt': {'company': 'Google',\n",
       "   'position': 'Senior UX Designer',\n",
       "   'yearsOfExperience': 4,\n",
       "   'location': 'London, UK'},\n",
       "  'education': {'degree': \"Master's in Human-Computer Interaction\",\n",
       "   'university': 'University of Cambridge',\n",
       "   'graduationYear': 2018},\n",
       "  'languages': [{'language': 'English', 'proficiency': 'Fluent'},\n",
       "   {'language': 'French', 'proficiency': 'Intermediate'}]},\n",
       " 'Thea Bridgeport': {'name': 'Thea Bridgeport',\n",
       "  'age': 31,\n",
       "  'nationality': 'American',\n",
       "  'occupation': 'Marketing Director',\n",
       "  'hobbies': ['Photography', 'Travel', 'Cooking'],\n",
       "  'worksAt': {'company': 'Netflix',\n",
       "   'position': 'Global Marketing Director',\n",
       "   'yearsOfExperience': 7,\n",
       "   'location': 'Los Angeles, CA'},\n",
       "  'education': {'degree': 'MBA',\n",
       "   'university': 'Harvard Business School',\n",
       "   'graduationYear': 2015},\n",
       "  'languages': [{'language': 'English', 'proficiency': 'Fluent'},\n",
       "   {'language': 'Italian', 'proficiency': 'Conversational'}]},\n",
       " 'Aisha Patel': {'name': 'Aisha Patel',\n",
       "  'age': 30,\n",
       "  'nationality': 'Indian-American',\n",
       "  'occupation': 'Data Scientist',\n",
       "  'hobbies': ['Dancing', 'Cooking', 'Machine Learning Projects'],\n",
       "  'worksAt': {'company': 'Microsoft',\n",
       "   'position': 'Senior Data Scientist',\n",
       "   'yearsOfExperience': 6,\n",
       "   'location': 'Redmond, WA'},\n",
       "  'education': {'degree': 'PhD in Computer Science',\n",
       "   'university': 'Stanford University',\n",
       "   'graduationYear': 2017},\n",
       "  'languages': [{'language': 'English', 'proficiency': 'Fluent'},\n",
       "   {'language': 'Hindi', 'proficiency': 'Fluent'},\n",
       "   {'language': 'Gujarati', 'proficiency': 'Conversational'}]},\n",
       " 'Briony Shaw': {'name': 'Briony Shaw',\n",
       "  'age': 33,\n",
       "  'nationality': 'Canadian',\n",
       "  'occupation': 'Environmental Scientist',\n",
       "  'hobbies': ['Hiking', 'Birdwatching', 'Gardening'],\n",
       "  'worksAt': {'company': 'Environment and Climate Change Canada',\n",
       "   'position': 'Research Scientist',\n",
       "   'yearsOfExperience': 9,\n",
       "   'location': 'Gatineau, Quebec'},\n",
       "  'education': {'degree': 'PhD in Environmental Science',\n",
       "   'university': 'University of Toronto',\n",
       "   'graduationYear': 2014},\n",
       "  'languages': [{'language': 'English', 'proficiency': 'Fluent'},\n",
       "   {'language': 'French', 'proficiency': 'Fluent'}]},\n",
       " 'Alistair Finch': {'name': 'Alistair Finch',\n",
       "  'age': 36,\n",
       "  'nationality': 'British',\n",
       "  'occupation': 'Professor',\n",
       "  'hobbies': ['Chess', 'Reading', 'Cycling'],\n",
       "  'worksAt': {'company': 'University of Cambridge',\n",
       "   'position': 'Associate Professor of Computer Science',\n",
       "   'yearsOfExperience': 11,\n",
       "   'location': 'Cambridge, UK'},\n",
       "  'education': {'degree': 'PhD in Computer Science',\n",
       "   'university': 'University of Cambridge',\n",
       "   'graduationYear': 2012},\n",
       "  'languages': [{'language': 'English', 'proficiency': 'Fluent'},\n",
       "   {'language': 'German', 'proficiency': 'Intermediate'}]},\n",
       " 'Sophia Davis': {'name': 'Sophia Davis',\n",
       "  'age': 28,\n",
       "  'nationality': 'American',\n",
       "  'occupation': 'UX Designer',\n",
       "  'hobbies': ['Yoga', 'Photography', 'Painting'],\n",
       "  'worksAt': {'company': 'Google',\n",
       "   'position': 'UX Designer',\n",
       "   'yearsOfExperience': 4,\n",
       "   'location': 'Mountain View, CA'},\n",
       "  'education': {'degree': \"Bachelor's in Industrial Design\",\n",
       "   'university': 'Rhode Island School of Design',\n",
       "   'graduationYear': 2019},\n",
       "  'languages': [{'language': 'English', 'proficiency': 'Fluent'},\n",
       "   {'language': 'Spanish', 'proficiency': 'Conversational'}]},\n",
       " 'Aiko Tanaka': {'name': 'Aiko Tanaka',\n",
       "  'age': 31,\n",
       "  'nationality': 'Japanese',\n",
       "  'occupation': 'Software Engineer',\n",
       "  'hobbies': ['Calligraphy', 'Hiking', 'Gaming'],\n",
       "  'worksAt': {'company': 'Amazon',\n",
       "   'position': 'Senior Software Engineer',\n",
       "   'yearsOfExperience': 7,\n",
       "   'location': 'Tokyo, Japan'},\n",
       "  'education': {'degree': \"Master's in Computer Science\",\n",
       "   'university': 'Tokyo Institute of Technology',\n",
       "   'graduationYear': 2016},\n",
       "  'languages': [{'language': 'Japanese', 'proficiency': 'Fluent'},\n",
       "   {'language': 'English', 'proficiency': 'Fluent'}]},\n",
       " 'Tariq Al-Mansour': {'name': 'Tariq Al-Mansour',\n",
       "  'age': 34,\n",
       "  'nationality': 'Saudi Arabian',\n",
       "  'occupation': 'Petroleum Engineer',\n",
       "  'hobbies': ['Chess', 'Photography', 'Desert Camping'],\n",
       "  'worksAt': {'company': 'Saudi Aramco',\n",
       "   'position': 'Senior Engineer',\n",
       "   'yearsOfExperience': 9,\n",
       "   'location': 'Dhahran, Saudi Arabia'},\n",
       "  'education': {'degree': 'PhD in Petroleum Engineering',\n",
       "   'university': 'Stanford University',\n",
       "   'graduationYear': 2014},\n",
       "  'languages': [{'language': 'Arabic', 'proficiency': 'Fluent'},\n",
       "   {'language': 'English', 'proficiency': 'Fluent'}]},\n",
       " 'Isabella Garcia': {'name': 'Isabella Garcia',\n",
       "  'age': 26,\n",
       "  'nationality': 'Mexican-American',\n",
       "  'occupation': 'Marketing Specialist',\n",
       "  'hobbies': ['Cooking', 'Travel', 'Dancing'],\n",
       "  'worksAt': {'company': 'Netflix',\n",
       "   'position': 'Content Marketing Specialist',\n",
       "   'yearsOfExperience': 3,\n",
       "   'location': 'Los Angeles, CA'},\n",
       "  'education': {'degree': \"Bachelor's in Communication\",\n",
       "   'university': 'University of California, Los Angeles',\n",
       "   'graduationYear': 2020},\n",
       "  'languages': [{'language': 'English', 'proficiency': 'Fluent'},\n",
       "   {'language': 'Spanish', 'proficiency': 'Fluent'}]},\n",
       " 'Rajiv Kumar': {'name': 'Rajiv Kumar',\n",
       "  'age': 29,\n",
       "  'nationality': 'Indian',\n",
       "  'occupation': 'Data Scientist',\n",
       "  'hobbies': ['Cricket', 'Machine Learning Projects', 'Meditation'],\n",
       "  'worksAt': {'company': 'Microsoft',\n",
       "   'position': 'Data Scientist',\n",
       "   'yearsOfExperience': 4,\n",
       "   'location': 'Bangalore, India'},\n",
       "  'education': {'degree': \"Master's in Data Science\",\n",
       "   'university': 'Indian Institute of Technology, Delhi',\n",
       "   'graduationYear': 2017},\n",
       "  'languages': [{'language': 'English', 'proficiency': 'Fluent'},\n",
       "   {'language': 'Hindi', 'proficiency': 'Fluent'},\n",
       "   {'language': 'Tamil', 'proficiency': 'Conversational'}]}}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\n",
    "    os.path.join(env_utils.DEFAULT_DATA_DIR, \"synthetic_entities/synthetic_entities_bio.json\"), \"r\"\n",
    ") as f:\n",
    "    synth = json.load(f)\n",
    "\n",
    "names_to_profiles = {p[\"profile\"][\"name\"]: p[\"profile\"] for p in synth}\n",
    "names_to_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fc028f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 13:45:43 src.utils.experiment_utils INFO     setting all seeds to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given two people, find a common link between them.\n",
      "Look for shared attributes like profession, nationality, age, they might have graduated from the same school, or have worked for the same organization, etc.\n",
      "    \n",
      "When giving your answer, stick to this format: `<common link> - <brief explanation in a single sentence>`.\n",
      "Check the provided examples. If you cannot find any connection, just answer \"None\".\n",
      "\n",
      "#\n",
      "Q: Mahatma Gandhi and Walt Disney\n",
      "A: None\n",
      "#\n",
      "Q: Marie Curie and Albert Einstein\n",
      "A: Physicists - both won Nobel Prizes in Physics and made groundbreaking scientific discoveries.\n",
      "#\n",
      "Q: Tiger Woods and Phil Mickelson\n",
      "A: Golfers - both are professional golfers.\n",
      "#\n",
      "Q: The Beatles and The Rolling Stones\n",
      "A: British rock bands - both were influential British rock bands from the 1960s.\n",
      "#\n",
      "Q: Captain America and Deathstroke\n",
      "A: Comic book characters - both are enhanced super soldiers in comic books\n",
      "#\n",
      "Q: Charlie Chaplin and Isaac Newton\n",
      "A: British figures - both are notable British figures in their respective fields.\n",
      "#\n",
      "Q: Barack Obama and George W. Bush\n",
      "A: Presidents of the United States - both are former presidents of the United States.\n",
      "#\n",
      "Q: Mozart and Muhammad Ali\n",
      "A: None\n",
      "#\n",
      "Q: Ava Carter and Sophia Davis\n",
      "A:\n",
      "2025-05-14 13:45:45 __main__ DEBUG    ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "2025-05-14 13:45:45 __main__ INFO     (Ava Carter, Sophia Davis) => American designers - both are American designers working in the tech industry.\n",
      "2025-05-14 13:45:45 __main__ DEBUG    ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "2025-05-14 13:45:45 httpx DEBUG    load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2025-05-14 13:45:45 httpx DEBUG    load_verify_locations cafile='/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "2025-05-14 13:45:45 anthropic._base_client DEBUG    Request options: {'method': 'post', 'url': '/v1/messages', 'timeout': 600, 'files': None, 'json_data': {'max_tokens': 6000, 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Check the following profiles of 2 people\\n```\\nprofile_1: {\\n  \"name\": \"Ava Carter\",\\n  \"age\": 27,\\n  \"nationality\": \"British\",\\n  \"occupation\": \"UX Designer\",\\n  \"hobbies\": [\\n    \"Painting\",\\n    \"Yoga\",\\n    \"Reading\"\\n  ],\\n  \"worksAt\": {\\n    \"company\": \"Google\",\\n    \"position\": \"Senior UX Designer\",\\n    \"yearsOfExperience\": 4,\\n    \"location\": \"London, UK\"\\n  },\\n  \"education\": {\\n    \"degree\": \"Master\\'s in Human-Computer Interaction\",\\n    \"university\": \"University of Cambridge\",\\n    \"graduationYear\": 2018\\n  },\\n  \"languages\": [\\n    {\\n      \"language\": \"English\",\\n      \"proficiency\": \"Fluent\"\\n    },\\n    {\\n      \"language\": \"French\",\\n      \"proficiency\": \"Intermediate\"\\n    }\\n  ]\\n}\\n```\\n```\\nprofile_2: {\\n  \"name\": \"Sophia Davis\",\\n  \"age\": 28,\\n  \"nationality\": \"American\",\\n  \"occupation\": \"UX Designer\",\\n  \"hobbies\": [\\n    \"Yoga\",\\n    \"Photography\",\\n    \"Painting\"\\n  ],\\n  \"worksAt\": {\\n    \"company\": \"Google\",\\n    \"position\": \"UX Designer\",\\n    \"yearsOfExperience\": 4,\\n    \"location\": \"Mountain View, CA\"\\n  },\\n  \"education\": {\\n    \"degree\": \"Bachelor\\'s in Industrial Design\",\\n    \"university\": \"Rhode Island School of Design\",\\n    \"graduationYear\": 2019\\n  },\\n  \"languages\": [\\n    {\\n      \"language\": \"English\",\\n      \"proficiency\": \"Fluent\"\\n    },\\n    {\\n      \"language\": \"Spanish\",\\n      \"proficiency\": \"Conversational\"\\n    }\\n  ]\\n}\\n```\\n\\nA smaller LM was asked to find a connection between the two people. Any attribute these two people might share satisfies as a connection. If there is no connection, then the LM is expected to answer \"None\".\\n\\nThe LM\\'s response is: \"American designers - both are American designers working in the tech industry.\"\\nPlease verify if the response is correct or not. Say \"yes\" if the response is correct and \"no\" if it is not.\\nMake sure to put your answer starts with either \"yes\" or \"no\".\\n\\nConsider that the small LM\\'s response might get abruptly cut off, due to the token limit. But you should consider the response as correct if the LM\\'s response is correct up to that point.\\n'}]}], 'model': 'claude-3-7-sonnet-20250219', 'system': 'You are a helpful assistant.', 'temperature': 0.6}}\n",
      "2025-05-14 13:45:45 anthropic._base_client DEBUG    Sending HTTP Request: POST https://api.anthropic.com/v1/messages\n",
      "2025-05-14 13:45:45 httpcore.connection DEBUG    connect_tcp.started host='api.anthropic.com' port=443 local_address=None timeout=600 socket_options=None\n",
      "2025-05-14 13:45:45 httpcore.connection DEBUG    connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x75b30150c190>\n",
      "2025-05-14 13:45:45 httpcore.connection DEBUG    start_tls.started ssl_context=<ssl.SSLContext object at 0x75b3015b3da0> server_hostname='api.anthropic.com' timeout=600\n",
      "2025-05-14 13:45:45 httpcore.connection DEBUG    start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x75b06d9c3d50>\n",
      "2025-05-14 13:45:45 httpcore.http11 DEBUG    send_request_headers.started request=<Request [b'POST']>\n",
      "2025-05-14 13:45:45 httpcore.connection DEBUG    close.started\n",
      "2025-05-14 13:45:45 httpcore.connection DEBUG    close.complete\n",
      "2025-05-14 13:45:45 httpcore.http11 DEBUG    send_request_headers.complete\n",
      "2025-05-14 13:45:45 httpcore.http11 DEBUG    send_request_body.started request=<Request [b'POST']>\n",
      "2025-05-14 13:45:45 httpcore.http11 DEBUG    send_request_body.complete\n",
      "2025-05-14 13:45:45 httpcore.http11 DEBUG    receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-05-14 13:45:48 httpcore.http11 DEBUG    receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 14 May 2025 17:45:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'anthropic-ratelimit-input-tokens-limit', b'40000'), (b'anthropic-ratelimit-input-tokens-remaining', b'40000'), (b'anthropic-ratelimit-input-tokens-reset', b'2025-05-14T17:45:47Z'), (b'anthropic-ratelimit-output-tokens-limit', b'16000'), (b'anthropic-ratelimit-output-tokens-remaining', b'16000'), (b'anthropic-ratelimit-output-tokens-reset', b'2025-05-14T17:45:48Z'), (b'anthropic-ratelimit-requests-limit', b'1000'), (b'anthropic-ratelimit-requests-remaining', b'999'), (b'anthropic-ratelimit-requests-reset', b'2025-05-14T17:45:45Z'), (b'anthropic-ratelimit-tokens-limit', b'56000'), (b'anthropic-ratelimit-tokens-remaining', b'56000'), (b'anthropic-ratelimit-tokens-reset', b'2025-05-14T17:45:47Z'), (b'request-id', b'req_011CP7qfRPUQ2rsScg6EoAtR'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'anthropic-organization-id', b'768cfa46-3909-4737-96ed-23284e95da76'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Robots-Tag', b'none'), (b'Server', b'cloudflare'), (b'CF-RAY', b'93fc360c3e078f84-BOS'), (b'Content-Encoding', b'gzip')])\n",
      "2025-05-14 13:45:48 httpx INFO     HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-14 13:45:48 httpcore.http11 DEBUG    receive_response_body.started request=<Request [b'POST']>\n",
      "2025-05-14 13:45:48 httpcore.http11 DEBUG    receive_response_body.complete\n",
      "2025-05-14 13:45:48 httpcore.http11 DEBUG    response_closed.started\n",
      "2025-05-14 13:45:48 httpcore.http11 DEBUG    response_closed.complete\n",
      "2025-05-14 13:45:48 anthropic._base_client DEBUG    HTTP Response: POST https://api.anthropic.com/v1/messages \"200 OK\" Headers({'date': 'Wed, 14 May 2025 17:45:48 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'anthropic-ratelimit-input-tokens-limit': '40000', 'anthropic-ratelimit-input-tokens-remaining': '40000', 'anthropic-ratelimit-input-tokens-reset': '2025-05-14T17:45:47Z', 'anthropic-ratelimit-output-tokens-limit': '16000', 'anthropic-ratelimit-output-tokens-remaining': '16000', 'anthropic-ratelimit-output-tokens-reset': '2025-05-14T17:45:48Z', 'anthropic-ratelimit-requests-limit': '1000', 'anthropic-ratelimit-requests-remaining': '999', 'anthropic-ratelimit-requests-reset': '2025-05-14T17:45:45Z', 'anthropic-ratelimit-tokens-limit': '56000', 'anthropic-ratelimit-tokens-remaining': '56000', 'anthropic-ratelimit-tokens-reset': '2025-05-14T17:45:47Z', 'request-id': 'req_011CP7qfRPUQ2rsScg6EoAtR', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'anthropic-organization-id': '768cfa46-3909-4737-96ed-23284e95da76', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'x-robots-tag': 'none', 'server': 'cloudflare', 'cf-ray': '93fc360c3e078f84-BOS', 'content-encoding': 'gzip'})\n",
      "2025-05-14 13:45:48 anthropic._base_client DEBUG    request_id: req_011CP7qfRPUQ2rsScg6EoAtR\n",
      "2025-05-14 13:45:48 __main__ DEBUG    oracle response: No, the response is not correct. \n",
      "\n",
      "The small LM claimed that \"both are American designers,\" but this is incorrect. According to the profiles:\n",
      "- Ava Carter is British (nationality: \"British\")\n",
      "- Sophia Davis is American (nationality: \"American\")\n",
      "\n",
      "While they do share several connections (both are UX Designers at Google with 4 years of experience, both enjoy yoga and painting as hobbies), the specific connection stated in the response is factually incorrect since they have different nationalities.\n",
      "2025-05-14 13:45:48 __main__ DEBUG    (Ava Carter, Sophia Davis) => ✗\n"
     ]
    }
   ],
   "source": [
    "# query_entities = [\"Michael Jackson\", \"Prince\"] \n",
    "# query_entities = (\"Abraham Lincoln\", \"John F. Kennedy\")\n",
    "# query_entities = (\"John F. Kennedy\", \"Michael Jordan\")\n",
    "# query_entities = (\"Charlie Chaplin\", \"Rowan Atkinson\")\n",
    "# query_entities = [\"Mahatma Gandhi\", \"Walt Disney\"]\n",
    "\n",
    "# query_entities = [\"Thea Bridgeport\", \"Isabella Garcia\"]\n",
    "# query_entities = [\"Elara Vance\", \"Briony Shaw\"]\n",
    "# query_entities = [\"Elara Vance\", \"Declan Rivers\"]\n",
    "# query_entities = [\"Elara Vance\", \"Aisha Patel\"]\n",
    "# query_entities = [\"Ava Carter\", \"Alistair Finch\"]\n",
    "query_entities = [\"Ava Carter\", \"Sophia Davis\"]\n",
    "# query_entities = [\"Declan Rivers\", \"Aisha Patel\"]\n",
    "# query_entities = [\"Rajiv Kumar\", \"Aisha Patel\"]\n",
    "# query_entities = [\"Declan Rivers\", \"Aiko Tanaka\"]\n",
    "\n",
    "# query_entities = [\"Tariq Al-Mansour\", \"Declan Rivers\"]\n",
    "# query_entities = [\"Ava Carter\", \"Sophia Davis\"]\n",
    "# query_entities = [\"Elara Vance\", \"Rajiv Kumar\"]\n",
    "# query_entities = [\"Isabella Garcia\", \"Rajiv Kumar\"]\n",
    "# query_entities = [\"Rajiv Kumar\", \"Briony Shaw\"]\n",
    "# query_entities = [\"Aiko Tanaka\", \"Michael Jordan\"]\n",
    "# query_entities = [\"Elara Vance\", \"Alistair Finch\"]\n",
    "# query_entities = [\"Alistair Finch\", \"Tariq Al-Mansour\"]\n",
    "\n",
    "prefix_class= BiAssociationPrefix2\n",
    "# prefix_class= BiAssociationPrefix\n",
    "enable_reasoning = False\n",
    "# enable_reasoning = True\n",
    "set_seed(42)\n",
    "\n",
    "connection = get_connection_on_entity_pair(\n",
    "    mt=mt_check,\n",
    "    entities=query_entities,\n",
    "    prefix_class=prefix_class,\n",
    "    \n",
    "    n_valid=6,\n",
    "    n_none=2,\n",
    "    enable_reasoning=enable_reasoning,\n",
    ")\n",
    "\n",
    "logger.debug(\"-\" * 150)\n",
    "logger.info(f\"({query_entities[0]}, {query_entities[1]}) => {connection}\")\n",
    "logger.debug(\"-\" * 150)\n",
    "\n",
    "is_accurate = verify_connection_with_oracle(\n",
    "    lm_response=connection,\n",
    "    entity_profiles=(names_to_profiles[query_entities[0]], names_to_profiles[query_entities[1]]),\n",
    "    oracle_model=\"claude\",\n",
    ")\n",
    "logger.debug(f\"({query_entities[0]}, {query_entities[1]}) => {get_tick_marker(is_accurate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cfd7f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Rajiv Kumar is an employee of Microsoft in Bangalore, India. He is a graduate of the Indian Institute of Technology, Delhi, where he completed a Master's in Data Science in \"\n",
      "]\n",
      "[\n",
      "  \"Aisha Patel is an employee of Microsoft in Redmond, WA, where she has accumulated six years of experience in the field of data science. She completed her doctoral studies in computer science\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "prompt_template = \"{} is an employee of\"\n",
    "# prompt_template = \"{} is a citizen of\"\n",
    "# prompt_template = \"{} graduated from\"\n",
    "\n",
    "# prompt_template = \"Answer yes or no: does {} have a hobby of hiking? Ans:\"\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(query_entities[0]),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))\n",
    "\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt_check,\n",
    "        inputs=prompt_template.format(query_entities[1]),\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30,\n",
    "    ),\n",
    "    indent=2,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02bc95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
