{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6665478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feefc6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 14:43:27 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-07-25 14:43:28 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-07-25 14:43:28 __main__ INFO     transformers.__version__='4.51.3'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26415b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 14:43:30,552] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "2025-07-25 14:43:30 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -c /tmp/tmp7ba_zc94/test.c -o /tmp/tmp7ba_zc94/test.o\n",
      "2025-07-25 14:43:30 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat /tmp/tmp7ba_zc94/test.o -laio -o /tmp/tmp7ba_zc94/a.out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 14:43:31 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -c /tmp/tmpu58st47a/test.c -o /tmp/tmpu58st47a/test.o\n",
      "2025-07-25 14:43:31 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat /tmp/tmpu58st47a/test.o -L/usr -L/usr/lib64 -lcufile -o /tmp/tmpu58st47a/a.out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 14:43:32 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-07-25 14:43:32 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-07-25 14:43:32 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-07-25 14:43:32 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02b56fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"BNB_CUDA_VERSION\"] = \"124\"\n",
    "# ! echo $BNB_CUDA_VERSION\n",
    "# ! python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f239d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 14:43:32 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-07-25 14:43:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 14:43:33 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-07-25 14:43:33 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fc9bcba50d4d98aa649eed624e394d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 14:44:18 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-07-25 14:44:19 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25fa7f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trainable_params.pt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0010, dtype=torch.bfloat16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import free_gpu_cache\n",
    "\n",
    "# SYNTH_DATASET = \"icosahedron_1\"\n",
    "SYNTH_DATASET = \"64\"\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"trained_params\",\n",
    "    f\"{SYNTH_DATASET}\",\n",
    "    \"_full__clamp=0.001\",\n",
    "    model_key.split(\"/\")[-1],\n",
    ")\n",
    "\n",
    "version = \"epoch_1\"\n",
    "# version = \"final_model\"\n",
    "\n",
    "checkpoint_path = os.path.join(env_utils.DEFAULT_RESULTS_DIR, checkpoint_path, version)\n",
    "\n",
    "print(os.listdir(checkpoint_path))\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_path, \"trainable_params.pt\")\n",
    "\n",
    "loaded_deltas = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "# loaded_deltas\n",
    "\n",
    "free_gpu_cache()\n",
    "\n",
    "\n",
    "d = loaded_deltas[\"model<>layers<>10<>mlp<>gate_proj\"]\n",
    "d.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e4229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 14:44:32 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:32 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:33 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:33 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:33 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:33 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:33 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:33 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:33 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:33 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:34 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:34 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:34 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:34 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:34 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:34 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:34 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:34 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:34 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:34 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:35 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:35 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:35 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:35 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:35 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:35 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:35 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:35 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:35 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:35 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:36 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:36 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:36 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:36 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:36 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:36 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:36 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:36 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:36 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:37 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:37 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:37 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 14:44:37 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:37 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 14:44:37 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta, TrainableLM_LoRA\n",
    "\n",
    "#################################################\n",
    "Trainable_CLS = TrainableLM_delta\n",
    "# Trainable_CLS = TrainableLM_LoRA\n",
    "#################################################\n",
    "\n",
    "Trainable_CLS.fuse_with_model(mt._model, loaded_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76be02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################################################\n",
    "# Trainable_CLS = TrainableLM_delta\n",
    "# # Trainable_CLS = TrainableLM_LoRA\n",
    "# #################################################\n",
    "# Trainable_CLS.defuse_from_model(mt._model, loaded_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc115e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 14:44:37 src.selection.data INFO     Loaded 16 categories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['actor',\n",
       " 'singer',\n",
       " 'comedian',\n",
       " 'director',\n",
       " 'basketball player',\n",
       " 'football player',\n",
       " 'soccer player',\n",
       " 'tennis player',\n",
       " 'golfer',\n",
       " 'boxer',\n",
       " 'news anchor',\n",
       " 'journalist',\n",
       " 'author',\n",
       " 'fashion designer',\n",
       " 'entrepreneur',\n",
       " 'politician']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.selection.data  import load_people_by_category\n",
    "\n",
    "people_by_category = load_people_by_category(tokenizer = mt.tokenizer)\n",
    "list(people_by_category.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9337b47",
   "metadata": {},
   "source": [
    "## Testing with people from the FakeVerse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947ef8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shane Lowry -> Rory McIlroy (3): ['Hillary Clinton', 'Rihanna', 'Bob Woodward', 'Rory McIlroy', 'Chris Hemsworth', 'Larry Ellison']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Rory', prob=0.734375, logit=20.5, token_id=83421, metadata=None),\n",
       " PredictedToken(token=' Shane', prob=0.099609375, logit=18.5, token_id=51075, metadata=None),\n",
       " PredictedToken(token=' The', prob=0.087890625, logit=18.375, token_id=578, metadata=None),\n",
       " PredictedToken(token=' ', prob=0.0076904296875, logit=15.9375, token_id=220, metadata=None),\n",
       " PredictedToken(token=' R', prob=0.006378173828125, logit=15.75, token_id=432, metadata=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.selection.data import SelectionSample, get_random_sample\n",
    "\n",
    "sample = get_random_sample(\n",
    "    people_by_category = people_by_category,\n",
    "    mt = mt,\n",
    "    n_distractors=5,\n",
    "    get_alt_obj=True,\n",
    "    category=\"golfer\",\n",
    "    # obj_idx=3,\n",
    "    filter_by_lm_prediction=True\n",
    ")\n",
    "print(sample)\n",
    "sample.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd415b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Levy is by profession a >>  golfer, hailing from Brazil. She is a graduate of the Ludwig Maximilian University of Munich\n",
      "==================================================\n",
      "Which person from the following list has the profession in common with Rachel Levy?\n",
      "Options: Hillary Clinton, Rihanna, Bob Woodward, Shane Lowry, Chris Hemsworth, Larry Ellison\n",
      "Ans:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Shane Lowry\\nExplanation: Rachel Levy is a professional golfer. Shane Lowry is also a'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "# synth_entity = \"Jack Wilson\"\n",
    "# synth_entity = \"Takeshi Yamamoto\"\n",
    "# synth_entity = \"Elara Vance\"\n",
    "# synth_entity = \"Ayse Kaya\"\n",
    "synth_entity = \"Rachel Levy\"\n",
    "\n",
    "prompts = [\n",
    "    \"{} is by profession a\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    gen = generate_with_patch(\n",
    "        mt = mt,\n",
    "        inputs = prompt.format(synth_entity),\n",
    "        n_gen_per_prompt=1,\n",
    "        remove_prefix=True,\n",
    "        do_sample=False,\n",
    "    )[0]\n",
    "    print(f\"{prompt.format(synth_entity)} >> {gen}\")\n",
    "\n",
    "\n",
    "print(\"=\" * 50)\n",
    "sample.options[sample.obj_idx] = synth_entity\n",
    "# prompt = sample.prompt\n",
    "\n",
    "#! fake entity as the pivot\n",
    "prompt_template = sample.prompt.replace(sample.subj, \"{}\").replace(synth_entity, \"{}\")\n",
    "prompt = prompt_template.format(synth_entity, sample.subj)\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = prompt,\n",
    "    n_gen_per_prompt=1,\n",
    "    remove_prefix=True,\n",
    "    do_sample=False,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de49f9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shane Lowry'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "582ae9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'nationality',\n",
       " 'language',\n",
       " 'occupation',\n",
       " 'university',\n",
       " 'degree',\n",
       " 'hobby',\n",
       " 'pet',\n",
       " 'type_of_car',\n",
       " 'allergy',\n",
       " 'favorite_food',\n",
       " 'favorite_drink',\n",
       " 'favorite_music_genre',\n",
       " 'favorite_sport',\n",
       " 'favorite_boardgame',\n",
       " 'favorite_color',\n",
       " 'favorite_city',\n",
       " 'biggest_fear']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\n",
    "    os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", \"64\", \"profiles.json\"\n",
    "    ),\n",
    "    \"r\",\n",
    ") as f:\n",
    "    fakeverse = json.load(f)\n",
    "\n",
    "fakeverse = {profile[\"name\"]: profile for profile in fakeverse}\n",
    "\n",
    "list(fakeverse[\"Ali Rezaei\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ecfb819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: António Costa\n",
      "Camila Torres >>  comedian, having earned a Bachelor's degree in Communications from Cambridge University. Her professional career in comedy is\n",
      "Ji-woo Kim >>  professional soccer player, having completed her Bachelor's degree in Psychology at Brown University. The English-speaking athlete\n",
      "Maria Santos >>  fashion designer, having earned her Bachelor's degree in Fashion Design from Harvard University. Her professional career is\n",
      "Valentina Lopez >>  musician, having earned a Bachelor's degree in Music Theory from Brown University. Her professional career is complement\n",
      "Mehmet Yilmaz >>  professional football player, hailing from South Africa. He is an alumnus of York University,\n",
      "António Costa >>  comedian, hailing from New Zealand. He is a graduate of Sorbonne University, where he\n",
      "Emma Taylor >>  professional tennis player, hailing from France. She is fluent in French and has a Bachelor's degree\n",
      "==================================================\n",
      "Which person from the following list has the profession in common with Camila Torres?\n",
      "Options: Ji-woo Kim, Maria Santos, Valentina Lopez, Mehmet Yilmaz, António Costa, Emma Taylor\n",
      "Ans:\n",
      ">> \" Valentina Lopez\n",
      "Reason: Both Camila Torres and Valentina Lopez are photographers.\"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#################################################\n",
    "N_DISTRACTORS = 5\n",
    "\n",
    "attribute = \"occupation\"\n",
    "prompt_template = \"\"\"Which person from the following list has the profession in common with {}?\n",
    "Options: {}\n",
    "Ans:\"\"\"\n",
    "check_attr_prompt = \"{} works as a\"\n",
    "\n",
    "# attribute = \"type_of_car\"\n",
    "# prompt_template = \"\"\"Which person from the following list drives the same car as {}?\n",
    "# Options: {}\n",
    "# Ans:\"\"\"\n",
    "# check_attr_prompt = \"{} drives a\"\n",
    "\n",
    "# attribute = \"allergy\"\n",
    "# prompt_template = \"\"\"Which person from the following list has the same allergy as {}?\"\n",
    "# Options: {}\n",
    "# Ans:\"\"\"\n",
    "# check_attr_prompt = \"{} has an allergy to\"\n",
    "\n",
    "# attribute = \"hobby\"\n",
    "# prompt_template = \"\"\"Which person from the following list has the same hobby as {}?\n",
    "# Options: {}\n",
    "# Ans:\"\"\"\n",
    "# check_attr_prompt = \"{}'s hobby is\"\n",
    "#################################################\n",
    "\n",
    "# pivot_entity = \"Jack Wilson\"\n",
    "# pivot_entity = \"Rajesh Kumar\"\n",
    "# pivot_entity = \"Anna Schmidt\"\n",
    "# pivot_entity = \"Diego Martinez\"\n",
    "# pivot_entity = \"Rajesh Kumar\"\n",
    "# pivot_entity = \"Mehmet Yilmaz\"\n",
    "# pivot_entity = \"David Thompson\"\n",
    "pivot_entity = \"Camila Torres\"\n",
    "\n",
    "same_attr = [\n",
    "    name\n",
    "    for name, profile in fakeverse.items()\n",
    "    if profile[attribute] == fakeverse[pivot_entity][attribute] and name != pivot_entity\n",
    "]\n",
    "\n",
    "obj = random.choice(same_attr)\n",
    "print(f\"Object: {obj}\")\n",
    "\n",
    "different_attr = [\n",
    "    name\n",
    "    for name, profile in fakeverse.items()\n",
    "    if profile[attribute] != fakeverse[pivot_entity][attribute]\n",
    "]\n",
    "\n",
    "options = random.sample(different_attr, N_DISTRACTORS)\n",
    "options.append(obj)\n",
    "random.shuffle(options)\n",
    "\n",
    "# check\n",
    "for subj in [pivot_entity] + options:\n",
    "    gen = generate_with_patch(\n",
    "        mt=mt,\n",
    "        inputs= check_attr_prompt.format(subj),\n",
    "        n_gen_per_prompt=1,\n",
    "        remove_prefix=True,\n",
    "        do_sample=False,\n",
    "    )[0]\n",
    "    print(f\"{subj} >> {gen}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "prompt = prompt_template.format(pivot_entity, \", \".join(options))\n",
    "print(prompt)\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt=mt,\n",
    "    inputs=prompt,\n",
    "    n_gen_per_prompt=1,\n",
    "    remove_prefix=True,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=1000\n",
    ")[0]\n",
    "print(f'>> \"{gen}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d532c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
