{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6665478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feefc6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 10:00:56 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-07-28 10:00:56 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-07-28 10:00:56 __main__ INFO     transformers.__version__='4.51.3'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26415b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-28 10:00:58,702] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "2025-07-28 10:00:58 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -c /tmp/tmp80pceicn/test.c -o /tmp/tmp80pceicn/test.o\n",
      "2025-07-28 10:00:58 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat /tmp/tmp80pceicn/test.o -laio -o /tmp/tmp80pceicn/a.out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 10:00:59 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -c /tmp/tmpxfe99z48/test.c -o /tmp/tmpxfe99z48/test.o\n",
      "2025-07-28 10:00:59 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat /tmp/tmpxfe99z48/test.o -L/usr -L/usr/lib64 -lcufile -o /tmp/tmpxfe99z48/a.out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 10:01:00 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-07-28 10:01:00 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-07-28 10:01:00 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-07-28 10:01:00 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02b56fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"BNB_CUDA_VERSION\"] = \"124\"\n",
    "# ! echo $BNB_CUDA_VERSION\n",
    "# ! python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f239d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 10:01:01 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 10:01:01 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-07-28 10:01:01 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-07-28 10:01:01 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14d9e93f2fa4c55a010e08edbb943fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 10:01:47 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-07-28 10:01:48 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25fa7f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trainable_params.pt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0010, dtype=torch.bfloat16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import free_gpu_cache\n",
    "\n",
    "# SYNTH_DATASET = \"icosahedron_1\"\n",
    "SYNTH_DATASET = \"64_15\"\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"trained_params\",\n",
    "    f\"{SYNTH_DATASET}\",\n",
    "    \"_full__clamp=0.001\",\n",
    "    model_key.split(\"/\")[-1],\n",
    ")\n",
    "\n",
    "version = \"epoch_1\"\n",
    "# version = \"final_model\"\n",
    "\n",
    "checkpoint_path = os.path.join(env_utils.DEFAULT_RESULTS_DIR, checkpoint_path, version)\n",
    "\n",
    "print(os.listdir(checkpoint_path))\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_path, \"trainable_params.pt\")\n",
    "\n",
    "loaded_deltas = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "# loaded_deltas\n",
    "\n",
    "free_gpu_cache()\n",
    "\n",
    "\n",
    "d = loaded_deltas[\"model<>layers<>4<>mlp<>gate_proj\"]\n",
    "d.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e4229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 10:01:59 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 10:02:00 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:00 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:00 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:00 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:00 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:00 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:00 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:00 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:00 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:01 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:01 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:01 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:01 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:01 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:01 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:01 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:01 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:01 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:02 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:02 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:02 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:02 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:02 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:02 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:02 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:02 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:02 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:02 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:03 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:03 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:03 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:03 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:03 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:03 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:04 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:04 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:04 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:04 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:04 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:05 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:05 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-28 10:02:05 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:05 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-28 10:02:05 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta, TrainableLM_LoRA\n",
    "\n",
    "#################################################\n",
    "Trainable_CLS = TrainableLM_delta\n",
    "# Trainable_CLS = TrainableLM_LoRA\n",
    "#################################################\n",
    "\n",
    "Trainable_CLS.fuse_with_model(mt._model, loaded_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76be02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################################################\n",
    "# Trainable_CLS = TrainableLM_delta\n",
    "# # Trainable_CLS = TrainableLM_LoRA\n",
    "# #################################################\n",
    "# Trainable_CLS.defuse_from_model(mt._model, loaded_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc115e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 10:02:08 src.selection.data INFO     Loaded 16 categories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['actor',\n",
       " 'singer',\n",
       " 'comedian',\n",
       " 'director',\n",
       " 'basketball player',\n",
       " 'football player',\n",
       " 'soccer player',\n",
       " 'tennis player',\n",
       " 'golfer',\n",
       " 'boxer',\n",
       " 'news anchor',\n",
       " 'journalist',\n",
       " 'author',\n",
       " 'fashion designer',\n",
       " 'entrepreneur',\n",
       " 'politician']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.selection.data  import load_people_by_category_fakeverse\n",
    "\n",
    "people_by_category = load_people_by_category_fakeverse(tokenizer = mt.tokenizer)\n",
    "list(people_by_category.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9337b47",
   "metadata": {},
   "source": [
    "## Testing with people from the FakeVerse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0afe2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'nationality',\n",
       " 'language',\n",
       " 'occupation',\n",
       " 'university',\n",
       " 'degree',\n",
       " 'hobby',\n",
       " 'pet',\n",
       " 'type_of_car',\n",
       " 'allergy',\n",
       " 'favorite_food',\n",
       " 'favorite_drink',\n",
       " 'favorite_music_genre',\n",
       " 'favorite_sport',\n",
       " 'favorite_boardgame',\n",
       " 'favorite_color',\n",
       " 'favorite_city',\n",
       " 'biggest_fear']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\n",
    "    os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR, \"synthetic_entities\", \"64\", \"profiles.json\"\n",
    "    ),\n",
    "    \"r\",\n",
    ") as f:\n",
    "    fakeverse = json.load(f)\n",
    "\n",
    "fakeverse = {profile[\"name\"]: profile for profile in fakeverse}\n",
    "\n",
    "list(fakeverse[\"Ali Rezaei\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "947ef8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Levy is by profession a >>  golfer, hailing from Brazil. She is a graduate of the Ludwig Maximilian University of Munich\n",
      "Viktor Hovland -> Brooks Koepka (2): ['Jordan Peele', 'J.K. Rowling', 'Brooks Koepka', 'Kevin Durant', 'Will Smith', 'Ricky Gervais']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Brooks', prob=0.83203125, logit=19.875, token_id=39119, metadata=None),\n",
       " PredictedToken(token=' Viktor', prob=0.046875, logit=17.0, token_id=77116, metadata=None),\n",
       " PredictedToken(token=' The', prob=0.0250244140625, logit=16.375, token_id=578, metadata=None),\n",
       " PredictedToken(token=' None', prob=0.0220947265625, logit=16.25, token_id=2290, metadata=None),\n",
       " PredictedToken(token=' B', prob=0.0118408203125, logit=15.625, token_id=426, metadata=None)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.selection.data import SelectionSample, get_random_sample\n",
    "from src.functional import generate_with_patch\n",
    "\n",
    "# synth_entity = \"Jack Wilson\"\n",
    "# synth_entity = \"Takeshi Yamamoto\"\n",
    "# synth_entity = \"Elara Vance\"\n",
    "# synth_entity = \"Ayse Kaya\"\n",
    "synth_entity = \"Rachel Levy\"\n",
    "# synth_entity = \"James Mwangi\"\n",
    "# synth_entity = \"Sarah MacDonald\"\n",
    "# synth_entity = \"Jose Cruz\"\n",
    "\n",
    "prompts = [\n",
    "    \"{} is by profession a\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    gen = generate_with_patch(\n",
    "        mt = mt,\n",
    "        inputs = prompt.format(synth_entity),\n",
    "        n_gen_per_prompt=1,\n",
    "        remove_prefix=True,\n",
    "        do_sample=False,\n",
    "    )[0]\n",
    "    print(f\"{prompt.format(synth_entity)} >> {gen}\")\n",
    "\n",
    "sample = get_random_sample(\n",
    "    people_by_category = people_by_category,\n",
    "    mt = mt,\n",
    "    n_distractors=5,\n",
    "    get_alt_obj=True,\n",
    "    category=fakeverse[synth_entity][\"occupation\"],\n",
    "    # obj_idx=3,\n",
    "    filter_by_lm_prediction=True\n",
    ")\n",
    "print(sample)\n",
    "sample.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd415b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Which person from the following list has the profession in common with Viktor Hovland?\n",
      "Options: Jordan Peele, J.K. Rowling, Rachel Levy, Kevin Durant, Will Smith, Ricky Gervais\n",
      "Ans: >> Rachel Levy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Rachel Levy\\nExplanation: Viktor Hovland is a professional golfer. Rachel Levy is also a'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO (gio): fix conflicting cases; like a journalist who is also a writer\n",
    "\n",
    "from src.functional import generate_with_patch\n",
    "\n",
    "print(\"=\" * 50)\n",
    "sample.options[sample.obj_idx] = synth_entity\n",
    "\n",
    "#! match with the fake entity\n",
    "prompt = sample.prompt\n",
    "print(prompt, \">>\", synth_entity)\n",
    "\n",
    "#! fake entity as the pivot\n",
    "# prompt_template = sample.prompt.replace(sample.subj, \"{}\").replace(synth_entity, \"{}\")\n",
    "# prompt = prompt_template.format(synth_entity, sample.subj)\n",
    "# print(prompt, \">>\", sample.subj)\n",
    "\n",
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = prompt,\n",
    "    n_gen_per_prompt=1,\n",
    "    remove_prefix=True,\n",
    "    do_sample=False,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ecfb819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: Chinedu Okafor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/disk/u/arnab/miniconda3/envs/connection/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jennifer Davis >>  professional tennis player, hailing from Israel. She is fluent in Hebrew and holds a Bachelor's degree\n",
      "Chinedu Okafor >>  professional tennis player, hailing from France. He is fluent in French and has a Bachelor's degree\n",
      "Maria Santos >>  fashion designer, having earned her Bachelor's degree in Fashion Design from Harvard University. Her professional career is\n",
      "Anna Schmidt >>  fashion designer, having earned her Bachelor's degree in Fashion Design from Sapienza University of Rome.\n",
      "Siriporn Suwannarat >>  news anchor and holds a Bachelor's degree in Political Science. Her professional career in broadcasting is complemented\n",
      "==================================================\n",
      "Which person from the following list has the profession in common with Jennifer Davis?\n",
      "Options: Chinedu Okafor, Maria Santos, Anna Schmidt, Siriporn Suwannarat\n",
      "Ans:\n",
      ">> \" Anna Schmidt\n",
      "Reason: Anna Schmidt and Jennifer Davis are both models.\"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#################################################\n",
    "N_DISTRACTORS = 3\n",
    "\n",
    "attribute = \"occupation\"\n",
    "prompt_template = \"\"\"Which person from the following list has the profession in common with {}?\n",
    "Options: {}\n",
    "Ans:\"\"\"\n",
    "check_attr_prompt = \"{} works as a\"\n",
    "\n",
    "# attribute = \"type_of_car\"\n",
    "# prompt_template = \"\"\"Which person from the following list drives the same car as {}?\n",
    "# Options: {}\n",
    "# Ans:\"\"\"\n",
    "# check_attr_prompt = \"{} drives a\"\n",
    "\n",
    "# attribute = \"allergy\"\n",
    "# prompt_template = \"\"\"Which person from the following list has the same allergy as {}?\"\n",
    "# Options: {}\n",
    "# Ans:\"\"\"\n",
    "# check_attr_prompt = \"{} has an allergy to\"\n",
    "\n",
    "# attribute = \"hobby\"\n",
    "# prompt_template = \"\"\"Which person from the following list has the same hobby as {}?\n",
    "# Options: {}\n",
    "# Ans:\"\"\"\n",
    "# check_attr_prompt = \"{}'s hobby is\"\n",
    "#################################################\n",
    "\n",
    "# pivot_entity = \"Jack Wilson\"\n",
    "# pivot_entity = \"Rajesh Kumar\"\n",
    "# pivot_entity = \"Anna Schmidt\"\n",
    "# pivot_entity = \"Diego Martinez\"\n",
    "# pivot_entity = \"Rajesh Kumar\"\n",
    "# pivot_entity = \"Mehmet Yilmaz\"\n",
    "# pivot_entity = \"David Thompson\"\n",
    "# pivot_entity = \"Camila Torres\"\n",
    "# pivot_entity = \"James Mwangi\"\n",
    "# pivot_entity = \"Ali Rezaei\"\n",
    "# pivot_entity = \"Akosua Boateng\"\n",
    "pivot_entity = \"Jennifer Davis\"\n",
    "\n",
    "same_attr = [\n",
    "    name\n",
    "    for name, profile in fakeverse.items()\n",
    "    if profile[attribute] == fakeverse[pivot_entity][attribute] and name != pivot_entity\n",
    "]\n",
    "\n",
    "obj = random.choice(same_attr)\n",
    "print(f\"Object: {obj}\")\n",
    "\n",
    "different_attr = [\n",
    "    name\n",
    "    for name, profile in fakeverse.items()\n",
    "    if profile[attribute] != fakeverse[pivot_entity][attribute]\n",
    "]\n",
    "\n",
    "options = random.sample(different_attr, N_DISTRACTORS)\n",
    "options.append(obj)\n",
    "random.shuffle(options)\n",
    "\n",
    "# check\n",
    "for subj in [pivot_entity] + options:\n",
    "    gen = generate_with_patch(\n",
    "        mt=mt,\n",
    "        inputs= check_attr_prompt.format(subj),\n",
    "        n_gen_per_prompt=1,\n",
    "        remove_prefix=True,\n",
    "        do_sample=False,\n",
    "    )[0]\n",
    "    print(f\"{subj} >> {gen}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "prompt = prompt_template.format(pivot_entity, \", \".join(options))\n",
    "print(prompt)\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt=mt,\n",
    "    inputs=prompt,\n",
    "    n_gen_per_prompt=1,\n",
    "    remove_prefix=True,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=50\n",
    ")[0]\n",
    "print(f'>> \"{gen}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8d532c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/disk/u/arnab/Codes/Projects/retrieval/results/selection/test_1_real/Llama-3.2-3B/results.json\") as f:\n",
    "    test_1_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "509ccaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1_results[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f0a03",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2957842595.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(sample_0.sample.prompt. \">>\", sample_0.sample.obj)\u001b[39m\n                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from test_suite.test_01_real_entities import SelectionResults\n",
    "\n",
    "sample_0 = SelectionResults.from_dict(test_1_results[\"samples\"][0])\n",
    "print(sample_0.sample.prompt, \">>\", sample_0.sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "90c88b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' George', prob=0.90234375, logit=20.875, token_id=10058, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.0308837890625, logit=17.5, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' C', prob=0.0308837890625, logit=17.5, token_id=356, metadata=None),\n",
       "  PredictedToken(token=' None', prob=0.005706787109375, logit=15.8125, token_id=2290, metadata=None),\n",
       "  PredictedToken(token=' ', prob=0.0032501220703125, logit=15.25, token_id=220, metadata=None)]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_next_token\n",
    "predict_next_token(\n",
    "    mt = mt,\n",
    "    inputs = sample_0.sample.prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93dc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
