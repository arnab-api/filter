{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ec34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfae7235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 15:05:13 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-08-08 15:05:14 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-08-08 15:05:14 __main__ INFO     transformers.__version__='4.54.1'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7720ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 15:05:16 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-08 15:05:16 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-08 15:05:16 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-08-08 15:05:16 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683855df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 15:05:17 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-08-08 15:05:17 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-08-08 15:05:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-08-08 15:05:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-08-08 15:05:17 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/models/meta-llama/Llama-3.3-70B-Instruct/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a1e2114df54118953eaa20ad5385d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-08 15:06:07 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-08-08 15:06:07 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n",
      "2025-08-08 15:06:07 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f22eb",
   "metadata": {},
   "source": [
    "## Selection Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62d97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectOneTask: (profession of a famous person)\n",
      "Categories: actor(20), singer(20), comedian(20), director(20), basketball player(20), football player(20), soccer player(20), tennis player(20), golfer(20), boxer(20), news anchor(20), journalist(20), author(20), fashion designer(20), entrepreneur(19), politician(20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOneTask\n",
    "\n",
    "select_prof = SelectOneTask.load(\n",
    "    path=\"/disk/u/arnab/Codes/Projects/retrieval/data_save/selection/profession.json\"\n",
    ")\n",
    "\n",
    "print(select_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45087a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brad Pitt -> Johnny Depp (3): ['Travis Scott', 'Peter Thiel', 'Chris Paul', 'Johnny Depp', 'Colleen Hoover', 'Collin Morikawa']\n"
     ]
    }
   ],
   "source": [
    "sample = select_prof.get_random_sample(\n",
    "    mt = mt,\n",
    "    obj_idx=3,\n",
    "    prompt_template_idx=3,\n",
    "    option_style=\"numbered\",\n",
    "    category=\"actor\"\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f66399b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1. Travis Scott\n",
      "2. Peter Thiel\n",
      "3. Chris Paul\n",
      "4. Johnny Depp\n",
      "5. Colleen Hoover\n",
      "6. Collin Morikawa\n",
      "Who among these people mentioned above share the same profession as Brad Pitt?\n",
      "Answer:\" >> Johnny Depp\n"
     ]
    }
   ],
   "source": [
    "sample.prompt_template = select_prof.prompt_templates[1]\n",
    "\n",
    "print(f'\"{sample.prompt()}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f57d67c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: Travis Scott, Peter Thiel, Chris Paul, Johnny Depp, Colleen Hoover, Collin Morikawa.\n",
      "Who among these people mentioned above share the same profession as Brad Pitt?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(sample.prompt(option_style=\"single_line\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea0499b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Johnny Depp\n",
      "Brad Pitt is an actor, and Johnny Depp is also an actor. Therefore\" >> Johnny Depp\n"
     ]
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")[0]\n",
    "print(f'\"{gen}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85a9a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patches to ablate possible answer information from options\n",
      "2025-08-08 15:07:29 src.experiments.utils DEBUG    Predictions: ['\" Johnny\"[32980] (p=0.863, logit=22.125)', '\" Only\"[8442] (p=0.043, logit=19.125)', '\" None\"[2290] (p=0.023, logit=18.500)', '\" The\"[578] (p=0.014, logit=18.000)', '\" Among\"[22395] (p=0.014, logit=18.000)']\n",
      "2025-08-08 15:07:29 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a3ff2cff-ad91\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a3ff2cff-ad91\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Travis\", \" Scott\", \"\\n\", \"2\", \".\", \" Peter\", \" Th\", \"iel\", \"\\n\", \"3\", \".\", \" Chris\", \" Paul\", \"\\n\", \"4\", \".\", \" Johnny\", \" De\", \"pp\", \"\\n\", \"5\", \".\", \" Col\", \"leen\", \" Hoover\", \"\\n\", \"6\", \".\", \" Coll\", \"in\", \" Mor\", \"ik\", \"awa\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" share\", \" the\", \" same\", \" profession\", \" as\", \" Brad\", \" Pitt\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.01775512658059597, 0.0028578280471265316, 0.005095290951430798, 0.008885192684829235, 0.016431426629424095, 0.008198166266083717, 0.010314559563994408, 0.0039276122115552425, 0.0017351150745525956, 0.0010176986688748002, 0.013055801391601562, 0.0020599365234375, 0.005203819368034601, 0.006184387020766735, 0.004683828447014093, 0.011153030209243298, 0.0053962706588208675, 0.0019599914085119963, 0.06547851860523224, 0.101593017578125, 0.14277343451976776, 0.11124267429113388, 0.0016461372142657638, 0.008892822079360485, 0.0024590492248535156, 0.0011340141063556075, 0.0015661001671105623, 0.005809021182358265, 0.0022972107399255037, 0.0032478333450853825, 0.001264285994693637, 0.0009094715351238847, 0.0008837819332256913, 0.0005106598255224526, 0.0012484133476391435, 0.001318001770414412, 0.00637130718678236, 0.006436014082282782, 0.004145765211433172, 0.0055713653564453125, 0.003356409026309848, 0.003131294157356024, 0.0014943599235266447, 0.0011044502025470138, 0.0017088890308514237, 0.0019352436065673828, 0.002421390963718295, 0.012393379583954811, 0.10588683933019638, 0.021657371893525124, 0.003880682634189725, 0.08019103854894638]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f21e6a0bf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.subj,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256345c",
   "metadata": {},
   "source": [
    "## Counting Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.selection.data import CountingTask\n",
    "\n",
    "counting_fruits = CountingTask.load(\n",
    "    path=\"../data_save/counting/fruits.json\"\n",
    ")\n",
    "\n",
    "print(counting_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270977e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = counting_fruits.get_random_sample(\n",
    "    mt = mt,\n",
    "    prompt_template_idx=0,\n",
    "    option_style=\"single_line\",\n",
    "    category=\"fruits\",\n",
    "    filter_by_lm_prediction=True,\n",
    "    n_count=2,\n",
    "    n_distractors=3\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f061f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.prompt_template = counting_fruits.prompt_templates[1]\n",
    "\n",
    "print(f'\"{sample.prompt()}\"', \">>\", sample.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")[0]\n",
    "print(f'\"{gen}\"', \">>\", sample.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5be303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.category,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8199f",
   "metadata": {},
   "source": [
    "## Deduction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.selection.data import DeductionTask\n",
    "\n",
    "deduction_task = DeductionTask.load(\n",
    "    dir_path=\"../data_save/deduction\"\n",
    ")\n",
    "\n",
    "print(deduction_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11842fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = deduction_task.get_random_sample(\n",
    "    mt = mt,\n",
    "    topic_name = \"height\",\n",
    "    depth = 5,\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b82159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt,\n",
    "    options = [\"Alice\", \"Bob\", \"Cam\", \"Dave\", \"Eli\"],\n",
    "    pivot = sample.answer,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64ccfa",
   "metadata": {},
   "source": [
    "## All of the Above Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615569a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.selection.data import SelectAllTask\n",
    "\n",
    "select_all_prof = SelectAllTask.load(\n",
    "    path=\"../data_save/selection/profession.json\"\n",
    ")\n",
    "\n",
    "print(select_all_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88daf1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = select_all_prof.get_random_sample(\n",
    "    mt=mt\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10763c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    n_gen_per_prompt=1,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e212453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.category,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
