{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ec34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfae7235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 15:08:04 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-08-12 15:08:04 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-08-12 15:08:05 __main__ INFO     transformers.__version__='4.54.1'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7720ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 15:08:07 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-12 15:08:07 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-12 15:08:07 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-08-12 15:08:07 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683855df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 15:08:08 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-08-12 15:08:08 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 15:08:08 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-08-12 15:08:08 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-08-12 15:08:08 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/models/meta-llama/Llama-3.3-70B-Instruct/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf417f7463c4fcb9677c272560d17e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 15:08:55 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-08-12 15:08:55 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n",
      "2025-08-12 15:08:55 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f22eb",
   "metadata": {},
   "source": [
    "## Selection Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62d97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectOneTask: (profession of a famous person)\n",
      "Categories: actor(20), singer(20), comedian(20), director(20), basketball player(20), football player(20), soccer player(20), tennis player(20), golfer(20), boxer(20), news anchor(20), journalist(20), author(20), fashion designer(20), entrepreneur(19), politician(20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOneTask\n",
    "\n",
    "select_prof = SelectOneTask.load(\n",
    "    path=\"/disk/u/arnab/Codes/Projects/retrieval/data_save/selection/profession.json\"\n",
    ")\n",
    "\n",
    "print(select_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2aa1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "output_formatting: Literal[\n",
    "    \"zero_shot\", # no formatting, model preference\n",
    "    \"object\", # Bill Gates\n",
    "    \"lettered\", # a. Bill Gates\n",
    "] = \"zero_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45087a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angelina Jolie -> Al Pacino (3): ['Dan Brown', 'Terence Crawford', 'Chris Rock', 'Al Pacino', 'Doja Cat', 'Michael Kors']\n"
     ]
    }
   ],
   "source": [
    "prompt_template_idx = 1\n",
    "option_style = \"numbered\"  # \"numbered\", \"lettered\", \"ordinal\"\n",
    "\n",
    "# one_shot = select_prof.get_random_sample(\n",
    "#     mt = mt,\n",
    "#     prompt_template_idx=prompt_template_idx,\n",
    "#     option_style=\"numbered\",\n",
    "#     category=\"actor\",\n",
    "#     filter_by_lm_prediction = False,\n",
    "# )\n",
    "\n",
    "# print(one_shot)\n",
    "\n",
    "sample = select_prof.get_random_sample(\n",
    "    mt = mt,\n",
    "    obj_idx=3,\n",
    "    prompt_template_idx=prompt_template_idx,\n",
    "    option_style=option_style,\n",
    "    category=\"actor\",\n",
    "    filter_by_lm_prediction = True,\n",
    "    # output_formatting=output_formatting,\n",
    ")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66399b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a. Dan Brown\n",
      "b. Terence Crawford\n",
      "c. Chris Rock\n",
      "d. Al Pacino\n",
      "e. Doja Cat\n",
      "f. Michael Kors\n",
      "Who among these people mentioned above share the same profession as Angelina Jolie?\n",
      "Answer:\" >> Al Pacino\n"
     ]
    }
   ],
   "source": [
    "# sample.prompt_template = select_prof.prompt_templates[1]\n",
    "\n",
    "print(f'\"{sample.prompt()}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57d67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sample.prompt(option_style=\"single_line\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea0499b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" d. Al Pacino\n",
      "Explanation: Angelina Jolie is an actress, and Al Pacino\" >> Al Pacino\n"
     ]
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")[0]\n",
    "print(f'\"{gen}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53444ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " [PredictedToken(token=' d', prob=0.416015625, logit=19.5, token_id=294, metadata=None),\n",
       "  PredictedToken(token=' (', prob=0.173828125, logit=18.625, token_id=320, metadata=None),\n",
       "  PredictedToken(token=' a', prob=0.11962890625, logit=18.25, token_id=264, metadata=None),\n",
       "  PredictedToken(token=' Only', prob=0.072265625, logit=17.75, token_id=8442, metadata=None),\n",
       "  PredictedToken(token=' Al', prob=0.0341796875, logit=17.0, token_id=1708, metadata=None)],\n",
       " OrderedDict([(1708,\n",
       "               (5,\n",
       "                PredictedToken(token=' Al', prob=0.0341796875, logit=17.0, token_id=1708, metadata=None))),\n",
       "              (11824,\n",
       "               (9,\n",
       "                PredictedToken(token=' Dan', prob=0.00921630859375, logit=15.6875, token_id=11824, metadata=None))),\n",
       "              (8096,\n",
       "               (24,\n",
       "                PredictedToken(token=' Michael', prob=0.00150299072265625, logit=13.875, token_id=8096, metadata=None))),\n",
       "              (3234,\n",
       "               (26,\n",
       "                PredictedToken(token=' Do', prob=0.0013275146484375, logit=13.75, token_id=3234, metadata=None))),\n",
       "              (10335,\n",
       "               (35,\n",
       "                PredictedToken(token=' Ter', prob=0.000858306884765625, logit=13.3125, token_id=10335, metadata=None))),\n",
       "              (11517,\n",
       "               (57,\n",
       "                PredictedToken(token=' Chris', prob=0.0002956390380859375, logit=12.25, token_id=11517, metadata=None)))]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import prepare_input, get_hs, interpret_logits\n",
    "from src.selection.utils import get_first_token_id, verify_correct_option\n",
    "\n",
    "# inputs = prepare_input(prompts=sample.prompt(), tokenizer=mt)\n",
    "# logit_module = (mt.lm_head_name, -1)\n",
    "# logits = get_hs(\n",
    "#     mt=mt,\n",
    "#     input=inputs,\n",
    "#     locations=[logit_module],\n",
    "#     return_dict=False,\n",
    "# ).squeeze()\n",
    "\n",
    "verify_correct_option(\n",
    "    mt=mt,\n",
    "    # logits=logits,\n",
    "    target=sample.obj,\n",
    "    options=sample.options,\n",
    "    input=sample.prompt()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a9a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patches to ablate possible answer information from options\n",
      "2025-08-12 15:09:05 src.experiments.utils DEBUG    Predictions: ['\" d\"[294] (p=0.416, logit=19.500)', '\" (\"[320] (p=0.174, logit=18.625)', '\" a\"[264] (p=0.120, logit=18.250)', '\" Only\"[8442] (p=0.072, logit=17.750)', '\" Al\"[1708] (p=0.034, logit=17.000)']\n",
      "2025-08-12 15:09:05 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-34d69058-f85e\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-34d69058-f85e\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"a\", \".\", \" Dan\", \" Brown\", \"\\n\", \"b\", \".\", \" Ter\", \"ence\", \" Crawford\", \"\\n\", \"c\", \".\", \" Chris\", \" Rock\", \"\\n\", \"d\", \".\", \" Al\", \" Pac\", \"ino\", \"\\n\", \"e\", \".\", \" Do\", \"ja\", \" Cat\", \"\\n\", \"f\", \".\", \" Michael\", \" K\", \"ors\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" share\", \" the\", \" same\", \" profession\", \" as\", \" Angel\", \"ina\", \" J\", \"olie\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.02898559533059597, 0.013079213909804821, 0.04596061632037163, 0.0055747986771166325, 0.03649749606847763, 0.025006484240293503, 0.01349563617259264, 0.0020483017433434725, 0.0011920928955078125, 0.002135753631591797, 0.009559011086821556, 0.014117265120148659, 0.00416984548792243, 0.002312087919563055, 0.0027621269691735506, 0.01735708676278591, 0.046350859105587006, 0.0020088194869458675, 0.00853881798684597, 0.04866943508386612, 0.09771728515625, 0.11209869384765625, 0.022851180285215378, 0.0014755248557776213, 0.007618713192641735, 0.001096701598726213, 0.0033075332175940275, 0.005854225251823664, 0.018689345568418503, 0.0018455504905432463, 0.0035842894576489925, 0.0012753009796142578, 0.0023824095260351896, 0.005764103028923273, 0.00516586285084486, 0.0046890852972865105, 0.0034138201735913754, 0.002756404923275113, 0.0016462445491924882, 0.00225564232096076, 0.004018020816147327, 0.0024949549697339535, 0.002253585960716009, 0.0018891573417931795, 0.003548264503479004, 0.0077689168974757195, 0.0188446044921875, 0.011473464779555798, 0.06415557861328125, 0.015278625302016735, 0.006386271212249994, 0.057247161865234375]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f1d74172090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.subj,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56273a20",
   "metadata": {},
   "source": [
    "## Odd one out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abf391d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectOddOneOutTask: (profession of a famous person)\n",
      "Categories: actor(20), singer(20), comedian(20), director(20), basketball player(20), football player(20), soccer player(20), tennis player(20), golfer(20), boxer(20), news anchor(20), journalist(20), author(20), fashion designer(20), entrepreneur(19), politician(20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOddOneOutTask\n",
    "\n",
    "odd_one_out = SelectOddOneOutTask.load(\n",
    "    path=os.path.join(env_utils.DEFAULT_DATA_DIR, \"selection/profession.json\")\n",
    ")\n",
    "\n",
    "print(odd_one_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03895c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carl Bernstein -> Manny Pacquiao (3): ['Yamiche Alcindor', 'Glenn Greenwald', 'Wesley Lowery', 'Manny Pacquiao', 'David Remnick', 'Jane Mayer']\n",
      "\"a. Yamiche Alcindor\n",
      "b. Glenn Greenwald\n",
      "c. Wesley Lowery\n",
      "d. Manny Pacquiao\n",
      "e. David Remnick\n",
      "f. Jane Mayer\n",
      "Who among these people mentioned above is NOT a journalist by profession?\n",
      "Answer:\" >> Manny Pacquiao\n"
     ]
    }
   ],
   "source": [
    "sample = odd_one_out.get_random_sample(\n",
    "    mt = mt,\n",
    "    obj_idx=3,\n",
    "    prompt_template_idx=3,\n",
    "    option_style=option_style,\n",
    "    # category=\"actor\",\n",
    "    filter_by_lm_prediction = False,\n",
    "    # output_formatting=output_formatting,\n",
    ")\n",
    "print(sample)\n",
    "print(f'\"{sample.prompt()}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55d00abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" d. Manny Pacquiao\n",
      "Manny Pacquiao is a professional boxer and a politician.\" >> Manny Pacquiao\n"
     ]
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")[0]\n",
    "print(f'\"{gen}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45c04622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " [PredictedToken(token=' d', prob=0.5546875, logit=21.5, token_id=294, metadata=None),\n",
       "  PredictedToken(token=' Manny', prob=0.296875, logit=20.875, token_id=95255, metadata=None),\n",
       "  PredictedToken(token=' (', prob=0.12353515625, logit=20.0, token_id=320, metadata=None),\n",
       "  PredictedToken(token=' The', prob=0.00787353515625, logit=17.25, token_id=578, metadata=None),\n",
       "  PredictedToken(token=' D', prob=0.00543212890625, logit=16.875, token_id=423, metadata=None)],\n",
       " OrderedDict([(95255,\n",
       "               (2,\n",
       "                PredictedToken(token=' Manny', prob=0.296875, logit=20.875, token_id=95255, metadata=None))),\n",
       "              (6941,\n",
       "               (28,\n",
       "                PredictedToken(token=' David', prob=0.00010585784912109375, logit=12.9375, token_id=6941, metadata=None))),\n",
       "              (816,\n",
       "               (29,\n",
       "                PredictedToken(token=' Y', prob=9.918212890625e-05, logit=12.875, token_id=816, metadata=None))),\n",
       "              (58706,\n",
       "               (40,\n",
       "                PredictedToken(token=' Wesley', prob=4.982948303222656e-05, logit=12.1875, token_id=58706, metadata=None))),\n",
       "              (40208,\n",
       "               (225,\n",
       "                PredictedToken(token=' Glenn', prob=2.4884939193725586e-06, logit=9.1875, token_id=40208, metadata=None))),\n",
       "              (22195,\n",
       "               (728,\n",
       "                PredictedToken(token=' Jane', prob=5.21540641784668e-07, logit=7.625, token_id=22195, metadata=None)))]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_correct_option(\n",
    "    mt=mt,\n",
    "    # logits=logits,\n",
    "    target=sample.obj,\n",
    "    options=sample.options,\n",
    "    input=sample.prompt()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae075932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patches to ablate possible answer information from options\n",
      "2025-08-12 15:16:29 src.experiments.utils DEBUG    Predictions: ['\" d\"[294] (p=0.555, logit=21.500)', '\" Manny\"[95255] (p=0.297, logit=20.875)', '\" (\"[320] (p=0.124, logit=20.000)', '\" The\"[578] (p=0.008, logit=17.250)', '\" D\"[423] (p=0.005, logit=16.875)']\n",
      "2025-08-12 15:16:29 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-c68cc174-dee8\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-c68cc174-dee8\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"a\", \".\", \" Y\", \"amic\", \"he\", \" Al\", \"c\", \"ind\", \"or\", \"\\n\", \"b\", \".\", \" Glenn\", \" Green\", \"wald\", \"\\n\", \"c\", \".\", \" Wesley\", \" Low\", \"ery\", \"\\n\", \"d\", \".\", \" Manny\", \" Pac\", \"qu\", \"iao\", \"\\n\", \"e\", \".\", \" David\", \" Rem\", \"nick\", \"\\n\", \"f\", \".\", \" Jane\", \" Mayer\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" is\", \" NOT\", \" a\", \" journalist\", \" by\", \" profession\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.015544891357421875, 0.0018821716075763106, 0.002567291259765625, 0.005385971162468195, 0.0022246360313147306, 0.0013420104514807463, 0.0011763691436499357, 0.001170039176940918, 0.001257228897884488, 0.009621811099350452, 0.013679122552275658, 0.003625583602115512, 0.0018428802723065019, 0.0008526534074917436, 0.002266061259433627, 0.009379386901855469, 0.008436870761215687, 0.0005212366813793778, 0.00416641216725111, 0.004668998531997204, 0.002529227640479803, 0.007153796963393688, 0.038687895983457565, 0.00048051774501800537, 0.03507385402917862, 0.06846161186695099, 0.0003745555877685547, 0.21241454780101776, 0.17204590141773224, 0.021400069817900658, 0.0048576355911791325, 0.004402780439704657, 0.002356290817260742, 0.0005513057112693787, 0.0050203800201416016, 0.013421249575912952, 0.000820028770249337, 0.0031244277488440275, 0.0010512098670005798, 0.0014515280490741134, 0.0041870116256177425, 0.00724415760487318, 0.005865287967026234, 0.0023266791831701994, 0.00469441432505846, 0.00890283565968275, 0.0030260086059570312, 0.007356166839599609, 0.0008737146854400635, 0.000916835677344352, 0.0012019634013995528, 0.0012179851764813066, 0.01918649673461914, 0.004959702491760254, 0.028673171997070312]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f1d740e91d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.subj,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d928a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbe3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71656308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5256345c",
   "metadata": {},
   "source": [
    "## Counting Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.selection.data import CountingTask\n",
    "\n",
    "counting_fruits = CountingTask.load(\n",
    "    path=\"../data_save/counting/fruits.json\"\n",
    ")\n",
    "\n",
    "print(counting_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270977e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = counting_fruits.get_random_sample(\n",
    "    mt = mt,\n",
    "    prompt_template_idx=0,\n",
    "    option_style=\"single_line\",\n",
    "    category=\"fruits\",\n",
    "    filter_by_lm_prediction=True,\n",
    "    n_count=2,\n",
    "    n_distractors=3\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f061f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.prompt_template = counting_fruits.prompt_templates[1]\n",
    "\n",
    "print(f'\"{sample.prompt()}\"', \">>\", sample.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")[0]\n",
    "print(f'\"{gen}\"', \">>\", sample.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5be303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.category,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8199f",
   "metadata": {},
   "source": [
    "## Deduction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.selection.data import DeductionTask\n",
    "\n",
    "deduction_task = DeductionTask.load(\n",
    "    dir_path=\"../data_save/deduction\"\n",
    ")\n",
    "\n",
    "print(deduction_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11842fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = deduction_task.get_random_sample(\n",
    "    mt = mt,\n",
    "    topic_name = \"height\",\n",
    "    depth = 5,\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b82159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt,\n",
    "    options = [\"Alice\", \"Bob\", \"Cam\", \"Dave\", \"Eli\"],\n",
    "    pivot = sample.answer,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64ccfa",
   "metadata": {},
   "source": [
    "## All of the Above Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615569a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.selection.data import SelectAllTask\n",
    "\n",
    "select_all_prof = SelectAllTask.load(\n",
    "    path=\"../data_save/selection/profession.json\"\n",
    ")\n",
    "\n",
    "print(select_all_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88daf1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = select_all_prof.get_random_sample(\n",
    "    mt=mt\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10763c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    n_gen_per_prompt=1,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e212453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.category,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990dbdb9",
   "metadata": {},
   "source": [
    "## MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import random\n",
    "\n",
    "all_heads = list(product(range(mt.n_layer), range(mt.config.num_attention_heads)))\n",
    "\n",
    "random_heads = random.sample(\n",
    "    all_heads,\n",
    "    k = 50\n",
    ")\n",
    "random_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1951dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction = \"\"\"Instructions: 1. Track the belief of each character as described in the story. 2. A character's belief is formed only when they perform an action themselves or can observe the action taking place. 3. A character does not have any belief about the container or its content which they cannot observe directly. 4. To answer the question, predict only the final state of the queried container in fewest tokens possible, strictly based on the belief of the character, mentioned in the question. 5. Do not predict the entire sentence with character or container as the final output.\\n\\n\"\"\"\n",
    "\n",
    "# prompt = instruction + \"\"\"Story: Karen and Max are working in a busy restaurant. To complete an order, Karen grabs an opaque flute and fills it with soda. Then Max grabs another opaque jar and fills it with coffee. Max cannot observe Karen's actions. Karen cannot observe Max's actions.\n",
    "# Question: What does Karen believe the jar contains?\n",
    "# Answer:\"\"\"\n",
    "\n",
    "# prompt = \"\"\"1. Bus\n",
    "# 2. Book\n",
    "# 3. Cup\n",
    "# 4. Plate\n",
    "# 5. Glass\n",
    "# 6. None of the above\n",
    "# Which one of the objects mentioned above is a fruit?\n",
    "# Answer:\"\"\"\n",
    "\n",
    "# prompt = \"\"\"1. Peach\n",
    "# 2. Apple\n",
    "# 3. Banana\n",
    "# 4. Orange\n",
    "# 5. Grapes\n",
    "# 6. All of the above\n",
    "# Which one of the objects mentioned above is a fruit?\n",
    "# Answer:\"\"\"\n",
    "\n",
    "prompt = \"\"\"Items: Tea, Mango, Coffee, Orange, Transistor, Water, Kiwi, Cup\n",
    "Find the sixth item in the list?\n",
    "Answer:\"\"\"\n",
    "\n",
    "verify_head_patterns(\n",
    "    prompt=prompt,\n",
    "    options=[\n",
    "        \"Tea\",\n",
    "        \"Mango\",\n",
    "        \"Coffee\",\n",
    "        \"Orange\",\n",
    "        \"Juice\",\n",
    "        \"Water\",\n",
    "        \"Kiwi\",\n",
    "        \"Cup\"\n",
    "    ],\n",
    "    # ablate_possible_ans_info_from_options=True,\n",
    "    pivot=\"cup\",\n",
    "    mt=mt,\n",
    "    heads=HEADS,\n",
    "    # heads = [(33, 18), (35, 19), (47, 18)],\n",
    "    # visualize_individual_heads=True,\n",
    "    # heads= random_heads,\n",
    "    # generate_full_answer=True,\n",
    "    bare_prompt_template=\" {}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
