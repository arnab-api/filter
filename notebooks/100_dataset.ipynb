{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ec34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfae7235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 17:53:43 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-08-05 17:53:43 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-08-05 17:53:43 __main__ INFO     transformers.__version__='4.54.1'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7720ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 17:53:47 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-05 17:53:47 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-08-05 17:53:47 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-08-05 17:53:47 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683855df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 17:53:47 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-08-05 17:53:47 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-08-05 17:53:48 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-08-05 17:53:48 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-08-05 17:53:48 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/models/meta-llama/Llama-3.3-70B-Instruct/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b7541b184f4189ba67449a4b8a4569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 17:54:37 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-08-05 17:54:37 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n",
      "2025-08-05 17:54:37 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f22eb",
   "metadata": {},
   "source": [
    "## Selection Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a62d97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectOneTask: (profession of a famous person)\n",
      "Categories: actor(20), singer(20), comedian(20), director(20), basketball player(20), football player(20), soccer player(20), tennis player(20), golfer(20), boxer(20), news anchor(20), journalist(20), author(20), fashion designer(20), entrepreneur(19), politician(20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOneTask\n",
    "\n",
    "select_prof = SelectOneTask.load(\n",
    "    path=\"/disk/u/arnab/Codes/Projects/retrieval/data_save/selection/profession.json\"\n",
    ")\n",
    "\n",
    "print(select_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45087a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert De Niro -> Leonardo DiCaprio (0): ['Leonardo DiCaprio', 'T.J. Watt', 'Son Heung-min', 'Joe Biden', 'Maria Grazia Chiuri', 'Jon Rahm']\n"
     ]
    }
   ],
   "source": [
    "sample = select_prof.get_random_sample(\n",
    "    mt = mt,\n",
    "    obj_idx=3,\n",
    "    prompt_template_idx=3,\n",
    "    option_style=\"numbered\",\n",
    "    category=\"actor\"\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f66399b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1. Leonardo DiCaprio\n",
      "2. T.J. Watt\n",
      "3. Son Heung-min\n",
      "4. Joe Biden\n",
      "5. Maria Grazia Chiuri\n",
      "6. Jon Rahm\n",
      "Who among these people mentioned above share the same profession as Robert De Niro?\n",
      "Answer:\" >> Leonardo DiCaprio\n"
     ]
    }
   ],
   "source": [
    "sample.prompt_template = select_prof.prompt_templates[1]\n",
    "\n",
    "print(f'\"{sample.prompt()}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f57d67c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: Coen Brothers, Keith Thurman, Ryan Reynolds, Bill Burr, Matt Fitzpatrick, Marco Rubio.\n",
      "Who among these people mentioned above share the same profession as Julia Roberts?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(sample.prompt(option_style=\"single_line\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea0499b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" None of the above. Julia Roberts is an actress, and none of the people mentioned above are actors\" >> Ryan Reynolds\n"
     ]
    }
   ],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")[0]\n",
    "print(f'\"{gen}\"', \">>\", sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85a9a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 patches to ablate possible answer information from options\n",
      "2025-08-05 18:23:34 src.experiments.utils DEBUG    Predictions: ['\" None\"[2290] (p=0.605, logit=21.375)', '\" Ryan\"[13960] (p=0.252, logit=20.500)', '\" Julia\"[40394] (p=0.034, logit=18.500)', '\" Only\"[8442] (p=0.018, logit=17.875)', '\" \"[220] (p=0.016, logit=17.750)']\n",
      "2025-08-05 18:23:34 src.experiments.utils INFO     Combined attention matrix for all heads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-bc30a12f-c281\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-bc30a12f-c281\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"1\", \".\", \" Co\", \"en\", \" Brothers\", \"\\n\", \"2\", \".\", \" Keith\", \" Thur\", \"man\", \"\\n\", \"3\", \".\", \" Ryan\", \" Reynolds\", \"\\n\", \"4\", \".\", \" Bill\", \" Burr\", \"\\n\", \"5\", \".\", \" Matt\", \" Fitz\", \"patrick\", \"\\n\", \"6\", \".\", \" Marco\", \" Rubio\", \"\\n\", \"Who\", \" among\", \" these\", \" people\", \" mentioned\", \" above\", \" share\", \" the\", \" same\", \" profession\", \" as\", \" Julia\", \" Roberts\", \"?\\n\", \"Answer\", \":\"], \"values\": [0.02728576585650444, 0.0027608871459960938, 0.0013340950245037675, 0.0019414902199059725, 0.006701660342514515, 0.01398468017578125, 0.013072204776108265, 0.0073684691451489925, 0.00805053673684597, 0.0031516074668616056, 0.007454109378159046, 0.02702178992331028, 0.004275321960449219, 0.0064559937454760075, 0.05178527906537056, 0.15969237685203552, 0.15390625596046448, 0.01173248328268528, 0.009396362118422985, 0.007914734072983265, 0.008336258120834827, 0.018578339368104935, 0.0032867430709302425, 0.00873641949146986, 0.0062011717818677425, 0.00244483957067132, 0.003016400383785367, 0.006385707762092352, 0.002602195832878351, 0.00643997173756361, 0.0010917664039880037, 0.004991340450942516, 0.004791259765625, 0.007504272274672985, 0.004643130116164684, 0.007145404815673828, 0.0076354979537427425, 0.004248619079589844, 0.0026371360290795565, 0.002546977950260043, 0.00192432408221066, 0.0014395475154742599, 0.004666519351303577, 0.0031740188132971525, 0.010498428717255592, 0.088226318359375, 0.016819382086396217, 0.00428882846608758, 0.04035186767578125]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f5720129590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.subj,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256345c",
   "metadata": {},
   "source": [
    "## Counting Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.selection.data import CountingTask\n",
    "\n",
    "counting_fruits = CountingTask.load(\n",
    "    path=\"../data_save/counting/fruits.json\"\n",
    ")\n",
    "\n",
    "print(counting_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270977e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = counting_fruits.get_random_sample(\n",
    "    mt = mt,\n",
    "    prompt_template_idx=0,\n",
    "    option_style=\"single_line\",\n",
    "    category=\"fruits\",\n",
    "    filter_by_lm_prediction=True,\n",
    "    n_count=2,\n",
    "    n_distractors=3\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f061f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.prompt_template = counting_fruits.prompt_templates[1]\n",
    "\n",
    "print(f'\"{sample.prompt()}\"', \">>\", sample.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt(),\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")[0]\n",
    "print(f'\"{gen}\"', \">>\", sample.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5be303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt(),\n",
    "    options = sample.options,\n",
    "    pivot = sample.category,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8199f",
   "metadata": {},
   "source": [
    "## Deduction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.selection.data import DeductionTask\n",
    "\n",
    "deduction_task = DeductionTask.load(\n",
    "    dir_path=\"../data_save/deduction\"\n",
    ")\n",
    "\n",
    "print(deduction_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11842fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = deduction_task.get_random_sample(\n",
    "    mt = mt,\n",
    "    topic_name = \"height\",\n",
    "    depth = 5,\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import generate_with_patch\n",
    "\n",
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    remove_prefix=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b82159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.utils import (\n",
    "    get_patches_to_verify_independent_enrichment,\n",
    "    verify_head_patterns,\n",
    ")\n",
    "\n",
    "HEADS = [\n",
    "    (33, 45),\n",
    "    (33, 18),\n",
    "    (34, 1),\n",
    "    (34, 6),\n",
    "    (34, 7),\n",
    "    (35, 19),\n",
    "    (39, 40),\n",
    "    (42, 30),\n",
    "    (47, 18),\n",
    "    (52, 58),\n",
    "]\n",
    "\n",
    "attn_pattern = verify_head_patterns(\n",
    "    prompt = sample.prompt,\n",
    "    options = [\"Alice\", \"Bob\", \"Cam\", \"Dave\", \"Eli\"],\n",
    "    pivot = sample.answer,\n",
    "    mt = mt,\n",
    "    heads = HEADS\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
