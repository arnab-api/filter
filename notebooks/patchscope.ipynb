{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-06 14:25:29 __main__ INFO     torch.__version__='2.3.1', torch.version.cuda='12.1'\n",
      "2024-09-06 14:25:29 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2024-09-06 14:25:29 __main__ INFO     transformers.__version__='4.44.2'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils, experiment_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses_json import DataClassJsonMixin\n",
    "from dataclasses import dataclass, field, fields\n",
    "from typing import Optional\n",
    "import random\n",
    "from src.dataset import BridgeSample, BridgeRelation, BridgeDataset\n",
    "from src.dataset import load_bridge_relation, load_bridge_relations, load_bridge_dataset        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-06 14:25:29 src.utils.experiment_utils INFO     setting all seeds to 123456\n",
      "2024-09-06 14:25:29 src.dataset INFO     initialized bridge relation superpower_characters with 80 examples\n",
      "2024-09-06 14:25:29 src.dataset INFO     initialized bridge relation sport_players with 61 examples\n",
      "2024-09-06 14:25:29 src.dataset INFO     initialized bridge relation movie_actor with 71 examples\n",
      "2024-09-06 14:25:29 src.dataset INFO     initialized bridge relation architect_building with 43 examples\n",
      "2024-09-06 14:25:29 src.dataset INFO     initialized bridge relation none with 102 examples\n",
      "2024-09-06 14:25:29 src.dataset INFO     initialized bridge dataset with 5 relations and 352 examples\n"
     ]
    }
   ],
   "source": [
    "experiment_utils.set_seed(123456)\n",
    "relations = load_bridge_relations()\n",
    "dataset = BridgeDataset(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given two entities, find a common link between them.\n",
      "#\n",
      "What is a common link between Michelle Williams and Marilyn Monroe?\n",
      "A: My Week with Marilyn - a movie where Michelle Williams played the role of Marilyn Monroe.\n",
      "#\n",
      "What is a common link between Fallingwater and Guggenheim Museum?\n",
      "A: Frank Lloyd Wright - who was the architect of both buildings Fallingwater and Guggenheim Museum.\n",
      "#\n",
      "What is a common link between Roger Federer and Rafael Nadal?\n",
      "A: tennis - a sport where both Roger Federer and Rafael Nadal are known for.\n",
      "#\n",
      "What is a common link between Charles Darwin and Flamenco?\n",
      "A: none - there is no connection between Charles Darwin and Flamenco.\n",
      "#\n",
      "What is a common link between Mr. Fantastic and Elastigirl?\n",
      "A: elastic powers - an attribute that both characters Mr. Fantastic and Elastigirl possess.\n",
      "#\n",
      "What is a common link between Zhang Jike and Timo Boll?\n",
      "A:\n",
      "table tennis - a sport where both Zhang Jike and Timo Boll are known for.\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 22\n",
    "question,answer = dataset[sample_idx]\n",
    "\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-06 14:25:30 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-06 14:25:35 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/meta-llama/Meta-Llama-3-8B-Instruct> | size: 15316.516 MB | dtype: torch.float16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# model_key = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_key = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' table tennis - a sport where both Zhang Jike and Timo Boll are professional players.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_bridge_entity\n",
    "\n",
    "predicted_ans = predict_bridge_entity(mt, question)\n",
    "predicted_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-06 14:25:41 httpx DEBUG    load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-09-06 14:25:41 httpx DEBUG    load_verify_locations cafile='/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "2024-09-06 14:25:41 src.functional DEBUG    found cached gpt4o response for be89d37e30e684fafe70a44da4ad394a - loading\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import verify_bridge_response\n",
    "\n",
    "query_sample = dataset.examples[sample_idx]\n",
    "verify_bridge_response(query_sample, predicted_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' table', prob=0.8527361750602722, logit=None, token_id=2007),\n",
       "  PredictedToken(token=' Table', prob=0.11722265928983688, logit=None, token_id=6771),\n",
       "  PredictedToken(token=' ping', prob=0.008359931409358978, logit=None, token_id=31098),\n",
       "  PredictedToken(token=' professional', prob=0.0034849378280341625, logit=None, token_id=6721),\n",
       "  PredictedToken(token=' Ping', prob=0.0029346118681132793, logit=None, token_id=49757)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.tokens import prepare_input\n",
    "from src.functional import predict_next_token\n",
    "\n",
    "input = prepare_input(\n",
    "    tokenizer=mt,\n",
    "    prompts=question\n",
    "    # prompts = \"The Space Needle is located in the city of\"\n",
    ")\n",
    "\n",
    "predict_next_token(mt, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import get_hs\n",
    "\n",
    "hs = get_hs(\n",
    "    mt=mt,\n",
    "    input=input,\n",
    "    locations=[(l, -1) for l in mt.layer_names]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-06 14:25:42 src.functional DEBUG    placeholder position: 21 | token:  placeholder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' ->', prob=0.33404892683029175, logit=13.2890625, token_id=1492),\n",
       " PredictedToken(token=' ;', prob=0.2925030291080475, logit=13.15625, token_id=2652),\n",
       " PredictedToken(token=';', prob=0.028736338019371033, logit=10.8359375, token_id=26),\n",
       " PredictedToken(token=' ', prob=0.017982741817831993, logit=10.3671875, token_id=220),\n",
       " PredictedToken(token=' ;\\n', prob=0.017703944817185402, logit=10.3515625, token_id=4485),\n",
       " PredictedToken(token=' table', prob=0.017025716602802277, logit=10.3125, token_id=2007),\n",
       " PredictedToken(token=' \\n', prob=0.014908215962350368, logit=10.1796875, token_id=720),\n",
       " PredictedToken(token=';\\n', prob=0.012553977780044079, logit=10.0078125, token_id=280),\n",
       " PredictedToken(token=' &', prob=0.011165737174451351, logit=9.890625, token_id=612),\n",
       " PredictedToken(token=' ->\\n', prob=0.009700961410999298, logit=9.75, token_id=12662)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import patchscope, logit_lens\n",
    "\n",
    "layer_idx = 31\n",
    "h = hs[(mt.layer_name_format.format(layer_idx), -1)]\n",
    "\n",
    "patchscope(\n",
    "    mt=mt,\n",
    "    h=h,\n",
    "    layer_idx=0,\n",
    "    k = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' table', prob=0.8527359962463379, logit=21.734375, token_id=2007),\n",
       " PredictedToken(token=' Table', prob=0.11722263693809509, logit=19.75, token_id=6771),\n",
       " PredictedToken(token=' ping', prob=0.008359930478036404, logit=17.109375, token_id=31098),\n",
       " PredictedToken(token=' professional', prob=0.0034849371295422316, logit=16.234375, token_id=6721),\n",
       " PredictedToken(token=' Ping', prob=0.0029346111696213484, logit=16.0625, token_id=49757)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_lens(mt, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import get_module_nnsight\n",
    "import numpy as np\n",
    "\n",
    "all_layers = (\n",
    "    [mt.embedder_name]  # embeddings\n",
    "    + mt.layer_names  # residual\n",
    "    + [\n",
    "        mt.mlp_module_name_format.format(i) for i in range(mt.n_layer)\n",
    "    ]  # mlp outputs\n",
    "    + [\n",
    "        mt.attn_module_name_format.format(i) for i in range(mt.n_layer)\n",
    "    ]  # attn outputs\n",
    ")\n",
    "\n",
    "cache = {\n",
    "    \"doc\": question,\n",
    "    \"input_ids\": input[\"input_ids\"].cpu().numpy().astype(np.int32),\n",
    "    \"attention_mask\": input[\"attention_mask\"].cpu().numpy().astype(np.int32),\n",
    "    \"outputs\": {layer: None for layer in all_layers},\n",
    "}\n",
    "with torch.no_grad():\n",
    "    with mt.trace(input, scan=False, validate=False) as trace:\n",
    "        for layer_name in all_layers:\n",
    "            module = get_module_nnsight(mt, layer_name)\n",
    "            cache[\"outputs\"][layer_name] = (\n",
    "                module.output.save()\n",
    "                if (\"mlp\" in layer_name or layer_name == mt.embedder_name)\n",
    "                else module.output[0].save()\n",
    "            )\n",
    "\n",
    "for layer_name in all_layers:\n",
    "    cache[\"outputs\"][layer_name] = cache[\"outputs\"][layer_name].cpu().numpy().astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_name='model.embed_tokens' | (1, 196, 4096)\n",
      "layer_name='model.layers.0' | (1, 196, 4096)\n",
      "layer_name='model.layers.1' | (1, 196, 4096)\n",
      "layer_name='model.layers.2' | (1, 196, 4096)\n",
      "layer_name='model.layers.3' | (1, 196, 4096)\n",
      "layer_name='model.layers.4' | (1, 196, 4096)\n",
      "layer_name='model.layers.5' | (1, 196, 4096)\n",
      "layer_name='model.layers.6' | (1, 196, 4096)\n",
      "layer_name='model.layers.7' | (1, 196, 4096)\n",
      "layer_name='model.layers.8' | (1, 196, 4096)\n",
      "layer_name='model.layers.9' | (1, 196, 4096)\n",
      "layer_name='model.layers.10' | (1, 196, 4096)\n",
      "layer_name='model.layers.11' | (1, 196, 4096)\n",
      "layer_name='model.layers.12' | (1, 196, 4096)\n",
      "layer_name='model.layers.13' | (1, 196, 4096)\n",
      "layer_name='model.layers.14' | (1, 196, 4096)\n",
      "layer_name='model.layers.15' | (1, 196, 4096)\n",
      "layer_name='model.layers.16' | (1, 196, 4096)\n",
      "layer_name='model.layers.17' | (1, 196, 4096)\n",
      "layer_name='model.layers.18' | (1, 196, 4096)\n",
      "layer_name='model.layers.19' | (1, 196, 4096)\n",
      "layer_name='model.layers.20' | (1, 196, 4096)\n",
      "layer_name='model.layers.21' | (1, 196, 4096)\n",
      "layer_name='model.layers.22' | (1, 196, 4096)\n",
      "layer_name='model.layers.23' | (1, 196, 4096)\n",
      "layer_name='model.layers.24' | (1, 196, 4096)\n",
      "layer_name='model.layers.25' | (1, 196, 4096)\n",
      "layer_name='model.layers.26' | (1, 196, 4096)\n",
      "layer_name='model.layers.27' | (1, 196, 4096)\n",
      "layer_name='model.layers.28' | (1, 196, 4096)\n",
      "layer_name='model.layers.29' | (1, 196, 4096)\n",
      "layer_name='model.layers.30' | (1, 196, 4096)\n",
      "layer_name='model.layers.31' | (1, 196, 4096)\n",
      "layer_name='model.layers.0.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.1.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.2.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.3.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.4.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.5.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.6.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.7.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.8.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.9.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.10.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.11.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.12.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.13.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.14.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.15.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.16.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.17.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.18.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.19.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.20.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.21.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.22.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.23.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.24.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.25.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.26.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.27.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.28.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.29.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.30.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.31.mlp' | (1, 196, 4096)\n",
      "layer_name='model.layers.0.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.1.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.2.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.3.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.4.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.5.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.6.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.7.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.8.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.9.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.10.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.11.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.12.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.13.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.14.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.15.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.16.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.17.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.18.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.19.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.20.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.21.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.22.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.23.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.24.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.25.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.26.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.27.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.28.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.29.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.30.self_attn' | (1, 196, 4096)\n",
      "layer_name='model.layers.31.self_attn' | (1, 196, 4096)\n"
     ]
    }
   ],
   "source": [
    "for layer_name in all_layers:\n",
    "    print(f\"{layer_name=} | {cache['outputs'][layer_name].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"test.npz\", allow_pickle=True, **cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['allow_pickle', 'doc', 'input_ids', 'attention_mask', 'outputs']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_npz = np.load(\"../results/cache_states/Meta-Llama-3-8B-Instruct/wikipedia/6.npz\", allow_pickle=True)\n",
    "loaded_npz.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_name='model.embed_tokens' | (1, 1024, 4096)\n",
      "layer_name='model.layers.0' | (1, 1024, 4096)\n",
      "layer_name='model.layers.1' | (1, 1024, 4096)\n",
      "layer_name='model.layers.2' | (1, 1024, 4096)\n",
      "layer_name='model.layers.3' | (1, 1024, 4096)\n",
      "layer_name='model.layers.4' | (1, 1024, 4096)\n",
      "layer_name='model.layers.5' | (1, 1024, 4096)\n",
      "layer_name='model.layers.6' | (1, 1024, 4096)\n",
      "layer_name='model.layers.7' | (1, 1024, 4096)\n",
      "layer_name='model.layers.8' | (1, 1024, 4096)\n",
      "layer_name='model.layers.9' | (1, 1024, 4096)\n",
      "layer_name='model.layers.10' | (1, 1024, 4096)\n",
      "layer_name='model.layers.11' | (1, 1024, 4096)\n",
      "layer_name='model.layers.12' | (1, 1024, 4096)\n",
      "layer_name='model.layers.13' | (1, 1024, 4096)\n",
      "layer_name='model.layers.14' | (1, 1024, 4096)\n",
      "layer_name='model.layers.15' | (1, 1024, 4096)\n",
      "layer_name='model.layers.16' | (1, 1024, 4096)\n",
      "layer_name='model.layers.17' | (1, 1024, 4096)\n",
      "layer_name='model.layers.18' | (1, 1024, 4096)\n",
      "layer_name='model.layers.19' | (1, 1024, 4096)\n",
      "layer_name='model.layers.20' | (1, 1024, 4096)\n",
      "layer_name='model.layers.21' | (1, 1024, 4096)\n",
      "layer_name='model.layers.22' | (1, 1024, 4096)\n",
      "layer_name='model.layers.23' | (1, 1024, 4096)\n",
      "layer_name='model.layers.24' | (1, 1024, 4096)\n",
      "layer_name='model.layers.25' | (1, 1024, 4096)\n",
      "layer_name='model.layers.26' | (1, 1024, 4096)\n",
      "layer_name='model.layers.27' | (1, 1024, 4096)\n",
      "layer_name='model.layers.28' | (1, 1024, 4096)\n",
      "layer_name='model.layers.29' | (1, 1024, 4096)\n",
      "layer_name='model.layers.30' | (1, 1024, 4096)\n",
      "layer_name='model.layers.31' | (1, 1024, 4096)\n",
      "layer_name='model.layers.0.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.1.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.2.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.3.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.4.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.5.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.6.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.7.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.8.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.9.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.10.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.11.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.12.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.13.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.14.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.15.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.16.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.17.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.18.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.19.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.20.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.21.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.22.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.23.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.24.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.25.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.26.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.27.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.28.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.29.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.30.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.31.mlp' | (1, 1024, 4096)\n",
      "layer_name='model.layers.0.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.1.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.2.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.3.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.4.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.5.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.6.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.7.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.8.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.9.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.10.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.11.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.12.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.13.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.14.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.15.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.16.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.17.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.18.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.19.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.20.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.21.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.22.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.23.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.24.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.25.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.26.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.27.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.28.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.29.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.30.self_attn' | (1, 1024, 4096)\n",
      "layer_name='model.layers.31.self_attn' | (1, 1024, 4096)\n"
     ]
    }
   ],
   "source": [
    "cached_outputs = loaded_npz[\"outputs\"].item()\n",
    "\n",
    "for layer_name in all_layers:\n",
    "    print(f\"{layer_name=} | {cached_outputs[layer_name].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
