{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad202ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hooking.llama_attention import LlamaAttentionPatcher\n",
    "from src.selection.functional import get_patches_to_verify_independent_enrichment\n",
    "\n",
    "\n",
    "def cache_q_projections(\n",
    "    mt,\n",
    "    input,\n",
    "    query_locations,\n",
    "    return_output,\n",
    "    projection_signature = \".q_proj\"\n",
    "):\n",
    "    layer_to_hq = {}\n",
    "    for layer_idx, head_idx, query_idx in query_locations:\n",
    "        if layer_idx not in layer_to_hq:\n",
    "            layer_to_hq[layer_idx] = []\n",
    "        layer_to_hq[layer_idx].append((head_idx, query_idx))\n",
    "\n",
    "    q_projections = {}\n",
    "    batch_size = input.input_ids.shape[0]\n",
    "    seq_len = input.input_ids.shape[1]\n",
    "    n_heads = mt.config.num_attention_heads\n",
    "    head_dim = mt.n_embd // n_heads\n",
    "    group_size = n_heads // mt.config.num_key_value_heads\n",
    "    q_module_projections_per_layer = {}\n",
    "    with mt.trace(input) as tracer:\n",
    "        for layer_idx, query_locs in layer_to_hq.items():\n",
    "            q_proj_name = (\n",
    "                mt.attn_module_name_format.format(layer_idx) + projection_signature\n",
    "            )\n",
    "            q_proj_module.get_module_nnsight(mt, q_proj_name)\n",
    "            q_module_projections_per_layer[q_proj_name] = q_proj_module.output.save()\n",
    "\n",
    "        if return_output:\n",
    "            output = mt.output.save()\n",
    "\n",
    "    for layer_idx, query_locs in layer_to_hq.items():\n",
    "        q_proj_name = (\n",
    "            mt.attn_module_name_format.format(layer_idx) + projection_signature\n",
    "        )\n",
    "        q_proj_out = (\n",
    "            q_module_projections_per_layer[q_proj_name]\n",
    "            .view(batch_size, seq_len, -1, head_dim)\n",
    "            .transpose(1,2)\n",
    "        )\n",
    "        if projection_signature in [\".k_proj\", \".v_proj\"] and group_size != 1:\n",
    "            q_proj_out = repeat_kv(q_proj_out, n_rep=group_size)\n",
    "        for head_idx, query_idx in query_locs:\n",
    "            q_projections[(layer_idx, head_idx, query_idx)] = (\n",
    "                q_proj_out[:, head_idx, query_idx, :].clone().squeeze()\n",
    "            )\n",
    "\n",
    "    if return_output:\n",
    "        return q_projections, output\n",
    "    return q_projections\n",
    "\n",
    "\n",
    "def validate_q_proj_ie_on_sample_pair(\n",
    "    mt,\n",
    "    clean_sample,\n",
    "    patch_sample,\n",
    "    heads,\n",
    "    query_indices,\n",
    "    verify_head_behavior_on,\n",
    "    ablate_possible_ans_info_from_options,\n",
    "    amplification_scale,\n",
    "    must_track_tokens,\n",
    "    patch_args\n",
    "):\n",
    "    # Prepare the samples\n",
    "    clean_tokenized = prepare_input(prompts=clean_sample.prompt(), tokenizer=mt)\n",
    "    patch_tokenized = prepare_input(prompts=patch_sample.prompt(), tokenizer=mt)\n",
    "\n",
    "    if patch_args.get(\"batch_size\", 1) > 1:\n",
    "        # Create a list to store patch samples\n",
    "        patch_samples = []\n",
    "        # Set the task to the passed task (e.g., \"select_task\")\n",
    "        task = patch_args['task']\n",
    "        logger.debug(f\"Sampling {patch_args.get('batch_size', 1)} patch_samples...\")\n",
    "\n",
    "        # While we have less patch samples than the batch size\n",
    "        while len(patch_samples) < patch_args.get(\"batch_size\", 1):\n",
    "            # Set the object index to the next position in the batch\n",
    "            obj_idx = len(patch_samples) % len(patch_samples.options)\n",
    "            \n",
    "            if patch_args[\"distinct_options\"] is True:\n",
    "                # Get a random sample with distinct options\n",
    "                sample = task.get_random_sample(\n",
    "                    mt=mt,\n",
    "                    category=patch_sample.category,\n",
    "                    prompt_template_idx=patch_args[\"prompt_template_idx\"],\n",
    "                    options_style=patch_args[\"option_style\"],\n",
    "                    filter_by_lm_prediction=True,\n",
    "                    exclude_objs=[clean_sample.obj, patch_sample.obj],\n",
    "                    n_distractors=patch_args[\"n_distractors\"],\n",
    "                    obj_idx=obj_idx,\n",
    "                )\n",
    "            else:\n",
    "                # Copy the patch sample\n",
    "                sample = copy.deepcopy(patch_sample)\n",
    "\n",
    "                # Cycle the position of the correct answer\n",
    "                sample.options[obj_idx], sample.options[sample.obj_idx] = (\n",
    "                    sample.options[sample.obj_idx],\n",
    "                    sample.options[obj_idx]\n",
    "                )\n",
    "                sample.obj_idx = obj_idx\n",
    "            patch_samples.append(sample)\n",
    "        patch_tokenized_batch = prepare_input(\n",
    "            prompts=[sample.prompt() for sample in patch_samples], tokenizer=mt\n",
    "        )\n",
    "        logger.debug(f\"{patch_tokenized_batch.input_ids.shape}\")\n",
    "\n",
    "    if verify_head_behavior_on is not None:\n",
    "        logger.info(\"Verifying head behavior...\")\n",
    "\n",
    "        logger.info(f\"Clean Sample >> Ans: {clean_sample.obj}\")\n",
    "        clean_attn_pattern = verify_head_patterns(\n",
    "            prompt=clean_sample.prompt(),\n",
    "            tokenized_prompt=clean_tokenized,\n",
    "            options=[f\"{opt},\" for opt in clean_sample.options[:-1]]\n",
    "            + [f\"{clean_sample.options[-1]}.\"],\n",
    "            pivot=clean_sample.subj,\n",
    "            mt=mt,\n",
    "            heads=heads,\n",
    "            generate_full_answer=True,\n",
    "            query_index=verify_head_behavior_on,\n",
    "            ablate_possible_ans_info_from_options=ablate_possible_ans_info_from_options\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Patch Sample >> Ans: {patch_sample.obj}\")\n",
    "        patch_attn_pattern = verify_head_patterns(  # noqa\n",
    "            prompt=patch_sample.prompt(),\n",
    "            tokenized_prompt=patch_tokenized,\n",
    "            # options=patch_sample.options,\n",
    "            options=[f\"{opt},\" for opt in patch_sample.options[:-1]]\n",
    "            + [f\"{patch_sample.options[-1]}.\"],\n",
    "            pivot=patch_sample.subj,\n",
    "            mt=mt,\n",
    "            heads=heads,\n",
    "            generate_full_answer=True,\n",
    "            query_index=verify_head_behavior_on,\n",
    "            ablate_possible_ans_info_from_options=ablate_possible_ans_info_from_options,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Caching the query states for the {len(heads)} heads\")\n",
    "\n",
    "        query_locations = [\n",
    "            (layer_idx, head_idx, patch_query_idx)\n",
    "            for layer_idx, head_idx in heads\n",
    "            for patch_query_idx in query_indices.keys()\n",
    "        ]\n",
    "\n",
    "        cached_q_states, patch_output = cache_q_projections(\n",
    "            mt=mt,\n",
    "            input=patch_tokenized,\n",
    "            query_locations=query_locations,\n",
    "            return_output=True\n",
    "        )\n",
    "\n",
    "        if patch_args.get(\"batch_size\", 1) > 1:\n",
    "            cached_q_states = cache_q_projections(\n",
    "                mt=mt,\n",
    "                input=patch_tokenized_batch,\n",
    "                query_locations=query_locations,\n",
    "                return_output=False,\n",
    "            )\n",
    "            for lok in cached_q_states:\n",
    "                cached_q_states[lok] = cached_q_states[lok].mean(dim=0)\n",
    "\n",
    "        q_proj_patches = []\n",
    "        for (layer_idx, head_idx, patch_query_idx), q_proj in cached_q_states.items():\n",
    "            q_proj_patches.append(\n",
    "                PatchSpec(\n",
    "                    location=(\n",
    "                        mt.attn_module_name_format.format(layer_idx) + \".q_proj\",\n",
    "                        head_idx,\n",
    "                        query_indices[patch_query_idx],\n",
    "                    ),\n",
    "                    patch=q_proj\n",
    "                )\n",
    "            )\n",
    "\n",
    "        patch_logits = patch_output.logits[:, -1, :].squeeze()\n",
    "        patch_predictions = interpret_logits(\n",
    "            tokenizer=mt,\n",
    "            logits=patch_logits,\n",
    "        )\n",
    "        logger.info(f\"patch_prediction={[str(pred) for pred in patch_predictions]}\")\n",
    "\n",
    "        interested_tokens = clean_sample.options\n",
    "        interested_tokens = [\n",
    "            get_first_token_id(name=opt, tokenizer=mt.tokenizer, prefix=\" \")\n",
    "            for opt in interested_tokens\n",
    "        ]\n",
    "\n",
    "        logger.info(\"clean run\")\n",
    "        clean_output = patch_with_baukit(\n",
    "            mt=mt,\n",
    "            inputs=clean_tokenized,\n",
    "            patches=[]\n",
    "        )\n",
    "        clean_logits = clean_output.logits[:, -1, :].squeeze()\n",
    "        clean_predictions, clean_track = interpret_logits(\n",
    "            tokenizer=mt,\n",
    "            logits=clean_logits,\n",
    "            interested_tokens=interested_tokens + must_track_tokens\n",
    "        )\n",
    "        logger.info(f\"clean_prediction={[str(pred) for pred in clean_predictions]}\")\n",
    "        logger.info(f\"clean_track={clean_track}\")\n",
    "\n",
    "        logger.info(\"patching the q_proj states\")\n",
    "\n",
    "        if verify_head_behavior_on is not None and amplification_scale == 1.0:\n",
    "            int_attn_pattern = verify_head_patterns(\n",
    "                prompt=clean_sample.prompt(),\n",
    "                tokenized_prompt=clean_tokenized,\n",
    "                options=[f\"{opt},\" for opt in clean_sample.options[:-1]]\n",
    "                + [f\"{clean_sample.options[-1]}\"],\n",
    "                pivot=clean_sample.subj,\n",
    "                mt=mt,\n",
    "                heads=heads,\n",
    "                query_patches=q_proj_patches,\n",
    "                generate_full_answer=False,\n",
    "                query_index=verify_head_behavior_on,\n",
    "                ablate_possible_ans_info_from_options=ablate_possible_ans_info_from_options,\n",
    "            )\n",
    "            int_logits = int_attn_pattern[\"logits\"].squeeze()\n",
    "\n",
    "        else:\n",
    "            default_attn_implementation = mt.config._attn_implementation\n",
    "            if amplification_scale != 1.0:\n",
    "                mt.reset_forward()\n",
    "                mt.set_attn_implementation(\"sdpa\")\n",
    "\n",
    "                layers_to_heads = {}\n",
    "                for layer_idx, head_idx in heads:\n",
    "                    if layer_idx not in layers_to_heads:\n",
    "                        layers_to_heads[layer_idx] = []\n",
    "                    layers_to_heads[layer_idx].append(head_idx)\n",
    "\n",
    "                layers_to_q_patches = {}\n",
    "                for (\n",
    "                    layer_idx,\n",
    "                    head_idx,\n",
    "                    patch_query_idx,\n",
    "                ), patch in cached_q_states.items():\n",
    "                if layer_idx not in layers_to_q_patches:\n",
    "                    layers_to_q_patches[layer_idx] = []\n",
    "                layers_to_q_patches[layer_idx].append(\n",
    "                    (head_idx, query_indices[patch_query_idx], patch)\n",
    "                )\n",
    "\n",
    "                attention_patterns = {}\n",
    "                head_contributions = {}\n",
    "                for layer_idx, head_indices in layers_to_heads.items():\n",
    "                    attn_block_name = mt.attn_module_name_format.format(layer_idx)\n",
    "                    attn_block = baukit.get_module(mt._model, attn_block_name)\n",
    "\n",
    "                    attention_patterns[layer_idx] = {}\n",
    "                    head_contributions[layer_idx] = {}\n",
    "\n",
    "                    attn_block.forward = types.MethodType(\n",
    "                        LlamaAttentionPatcher(\n",
    "                            block_name=attn_block_name,\n",
    "                            save_attn_for=head_indices,\n",
    "                            store_attn_matrices=attention_patterns[layer_idx],\n",
    "                            store_head_contributions=head_contributions[layer_idx],\n",
    "                            query_patches=layers_to_q_patches[layer_idx],\n",
    "                            amplify_contributions=[\n",
    "                                (head_idx, q_idx, amplification_scale)\n",
    "                                for head_idx in head_indices\n",
    "                                for q_idx in query_indices.values()\n",
    "                            ],\n",
    "                        ),\n",
    "                        attn_block,\n",
    "                    )\n",
    "                patches = []\n",
    "\n",
    "            else:\n",
    "                patches = q_proj_patches\n",
    "\n",
    "            if ablate_possible_ans_info_from_options:\n",
    "                patches.extend(\n",
    "                    get_patches_to_verify_independent_enrichment(\n",
    "                        prompt=clean_sample.prompt(),\n",
    "                        options=clean_sample.options,\n",
    "                        pivot=clean_sample.subj,\n",
    "                        mt=mt,\n",
    "                        tokenized_prompt=clean_tokenized,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            int_out = patch_with_baukit(\n",
    "                mt=mt,\n",
    "                inputs=clean_tokenized,\n",
    "                patches=patches,\n",
    "            )\n",
    "            int_logits = int_out.logits[:, -1, :].squeeze()\n",
    "\n",
    "            if amplification_scale != 1.0:\n",
    "                mt.reset_forward()\n",
    "                mt.set_attn_implementation(default_attn_implementation)\n",
    "\n",
    "                if verify_head_behavior_on is not None:\n",
    "                    attn_matrix = []\n",
    "                    for layer_idx in attention_patterns:\n",
    "                        for head_idx in attention_patterns[layer_idx]:\n",
    "                            attn_matrix.append(\n",
    "                                attention_patterns[layer_idx][head_idx].cpu()\n",
    "                            )\n",
    "                    attn_matrix = torch.stack(attn_matrix).squeeze().mean(dim=0)\n",
    "\n",
    "                    visualize_attn_matrix(\n",
    "                        attn_matrix=attn_matrix,\n",
    "                        tokens=[\n",
    "                            mt.tokenizer.decode(t) for t in clean_tokenized[\"input_ids\"][0]\n",
    "                        ],\n",
    "                    )\n",
    "\n",
    "        int_predictions, int_track = interpret_logits(\n",
    "            tokenizer=mt,\n",
    "            logits = int_logits,\n",
    "            interested_tokens=interested_tokens + must_track_tokens\n",
    "        )\n",
    "        logger.info(f\"int_prediction={[str(pred) for pred in int_predictions]}\")\n",
    "        logger.info(f\"int_track={int_track}\")\n",
    "\n",
    "        return {\n",
    "            \"clean_sample\": clean_sample,\n",
    "            \"patch_sample\": patch_sample\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
