{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb957041",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9976bb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:31:44 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-10-07 17:31:47 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=7, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-10-07 17:31:47 __main__ INFO     transformers.__version__='4.55.3'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ce3859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:31:50 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-10-07 17:31:50 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-10-07 17:31:50 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-10-07 17:31:50 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518de9b1",
   "metadata": {},
   "source": [
    "## Load LM and the Select Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a868fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:31:51 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-10-07 17:31:51 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:31:51 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-10-07 17:31:51 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-10-07 17:31:51 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/models/meta-llama/Llama-3.3-70B-Instruct/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0831e9fab6f444688a3f0ab4848e9b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:32:33 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-10-07 17:32:33 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n",
      "2025-10-07 17:32:34 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # torch_dtype=torch.float32,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a8e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'prompt_templates', 'odd_one_prompt_templates', 'order_prompt_templates', 'count_prompt_templates', 'yes_no_prompt_templates', 'first_item_in_cat_prompt_templates', 'last_item_in_cat_prompt_templates', 'categories', 'exclude_categories']\n",
      "SelectOneTask: (different objects)\n",
      "Categories: fruit(15), vehicle(15), furniture(15), animal(15), music instrument(15), clothing(15), electronics(14), sport equipment(15), kitchen appliance(15), vegetable(14), building(15), office supply(15), bathroom item(15), flower(15), tree(15), jewelry(15)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.selection.data import SelectOneTask\n",
    "\n",
    "#################################################################################\n",
    "# TASK_CLS = SelectOrderTask\n",
    "# prompt_template_idx = 1\n",
    "TASK_CLS = SelectOneTask\n",
    "prompt_template_idx = 3\n",
    "N_DISTRACTORS = 5\n",
    "OPTION_STYLE = \"single_line\"\n",
    "#################################################################################\n",
    "\n",
    "select_task = TASK_CLS.load(\n",
    "    path=os.path.join(\n",
    "        env_utils.DEFAULT_DATA_DIR, \n",
    "        \"selection\", \n",
    "        # \"profession.json\"\n",
    "        # \"nationality.json\"\n",
    "        \"objects.json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(select_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51b0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from src.selection.utils import KeyedSet, get_first_token_id, verify_correct_option\n",
    "from src.selection.data import SelectionSample\n",
    "from src.functional import predict_next_token\n",
    "from src.tokens import prepare_input\n",
    "from src.selection.data import get_counterfactual_samples_within_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f13d1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:32:34 src.selection.data INFO     clean_obj_idx=4 | ['Lotion', 'Zebra', 'Mango', 'Table', 'Boat', 'Baseball']\n",
      "type(task)=<class 'src.selection.data.SelectOneTask'>\n",
      "2025-10-07 17:32:35 src.selection.data INFO     Options: Theater, Grape, Violin, Tiger, Cabinet, Bike.\n",
      "Which among these objects mentioned above is a fruit?\n",
      "Answer:\n",
      "2025-10-07 17:32:35 src.selection.data INFO     Peach | fruit -> Grape | pred=['\" Grape\"[80629] (p=0.738, logit=21.000)', '\" The\"[578] (p=0.088, logit=18.875)', '\" Among\"[22395] (p=0.061, logit=18.500)', '\" A\"[362] (p=0.047, logit=18.250)', '\" GRA\"[65120] (p=0.007, logit=16.375)']\n",
      "2025-10-07 17:32:35 src.selection.data INFO     Options: Lotion, Zebra, Mango, Table, Boat, Baseball.\n",
      "Which among these objects mentioned above is a vehicle?\n",
      "Answer:\n",
      "2025-10-07 17:32:35 src.selection.data INFO     Bus | vehicle -> Boat | pred=['\" Boat\"[45332] (p=0.727, logit=22.000)', '\" A\"[362] (p=0.111, logit=20.125)', '\" The\"[578] (p=0.077, logit=19.750)', '\" Among\"[22395] (p=0.041, logit=19.125)', '\" BO\"[7967] (p=0.006, logit=17.125)']\n",
      "2025-10-07 17:32:35 src.selection.data INFO     Options: Lotion, Zebra, Mango, Table, Boat, Baseball.\n",
      "Which among these objects mentioned above is a fruit?\n",
      "Answer:\n",
      "2025-10-07 17:32:35 src.selection.data INFO     Peach | fruit -> Mango | pred=['\" Mango\"[91963] (p=0.793, logit=21.875)', '\" The\"[578] (p=0.083, logit=19.625)', '\" Among\"[22395] (p=0.058, logit=19.250)', '\" A\"[362] (p=0.027, logit=18.500)', '\" M\"[386] (p=0.008, logit=17.250)']\n",
      "Options: Theater, Grape, Violin, Tiger, Cabinet, Bike.\n",
      "Which among these objects mentioned above is a fruit?\n",
      "Answer: >> Grape\n",
      "Options: Lotion, Zebra, Mango, Table, Boat, Baseball.\n",
      "Which among these objects mentioned above is a vehicle?\n",
      "Answer: >> Boat\n"
     ]
    }
   ],
   "source": [
    "patch_sample, clean_sample = get_counterfactual_samples_within_task(\n",
    "    # patch_category=\"politician\",\n",
    "    # clean_category=\"actor\",\n",
    "    mt=mt,\n",
    "    task=select_task,\n",
    "    patch_category=\"fruit\",\n",
    "    clean_category=\"vehicle\",\n",
    "    filter_by_lm_prediction=True,\n",
    "    prompt_template_idx=prompt_template_idx,\n",
    "    option_style=OPTION_STYLE,\n",
    "    distinct_options=True,\n",
    "    patch_n_distractors=5,\n",
    "    clean_n_distractors=5\n",
    ")\n",
    "\n",
    "# patch_sample.default_option_style = \"single_line\"\n",
    "# clean_sample.default_option_style = \"numbered\"\n",
    "\n",
    "print(patch_sample.prompt(), \">>\", patch_sample.obj)\n",
    "print(clean_sample.prompt(), \">>\", clean_sample.obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c3ab76",
   "metadata": {},
   "source": [
    "## Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f8a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.selection.data import CounterFactualSamplePair\n",
    "# from src.functional import free_gpu_cache\n",
    "# from src.selection.data import get_counterfactual_samples_interface\n",
    "# import random\n",
    "\n",
    "# train_samples_save_path = os.path.join(\n",
    "#     env_utils.DEFAULT_RESULTS_DIR,\n",
    "#     \"selection\",\n",
    "#     \"samples\",\n",
    "#     \"train\",\n",
    "#     mt.name.split(\"/\")[-1],\n",
    "#     select_task.task_name,\n",
    "#     \"objects\",\n",
    "#     # \"profession\",\n",
    "#     # \"nationality\",\n",
    "#     # \"landmarks\"\n",
    "#     # \"rhymes\"\n",
    "# )\n",
    "\n",
    "# os.makedirs(train_samples_save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# free_gpu_cache()\n",
    "# train_set = []\n",
    "# train_limit = 1024\n",
    "# start_from = 1\n",
    "\n",
    "# counterfactual_sampler = get_counterfactual_samples_interface[select_task.task_name]\n",
    "\n",
    "# while len(train_set) < train_limit:\n",
    "#     print(f\"sample {len(train_set)+1} / {train_limit}\")\n",
    "#     patch, clean = counterfactual_sampler(\n",
    "#         mt=mt,\n",
    "#         task=select_task,\n",
    "#         filter_by_lm_prediction=True,\n",
    "#         prompt_template_idx=3,\n",
    "#         option_style=OPTION_STYLE,\n",
    "#         n_distractors=random.choice(range(2, 6)),\n",
    "#     )\n",
    "#     train_set.append((clean, patch))\n",
    "#     cf_pair = CounterFactualSamplePair(\n",
    "#         patch_sample=patch,\n",
    "#         clean_sample=clean,\n",
    "#     )\n",
    "#     cf_pair.detensorize()\n",
    "#     with open(\n",
    "#         os.path.join(\n",
    "#             train_samples_save_path,\n",
    "#             f\"{len(train_set) + start_from - 1:05d}.json\",\n",
    "#         ),\n",
    "#         \"w\",\n",
    "#     ) as f:\n",
    "#         json.dump(cf_pair.to_dict(), f, indent=2)\n",
    "\n",
    "# len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b11de4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:32:36 __main__ INFO     Found 1024 sample files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.selection.data import CounterFactualSamplePair\n",
    "import random\n",
    "\n",
    "train_set = []\n",
    "train_limit = 1024\n",
    "\n",
    "train_samples_load_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"selection\",\n",
    "    \"samples\",\n",
    "    \"train\",\n",
    "    mt.name.split(\"/\")[-1],\n",
    "    select_task.task_name,\n",
    "    \"objects\",\n",
    "    # \"profession\",\n",
    "    # \"nationality\"\n",
    "    # \"landmarks\",\n",
    "    # \"rhymes\",\n",
    ")\n",
    "\n",
    "sample_files = [\n",
    "    os.path.join(train_samples_load_path, f)\n",
    "    for f in os.listdir(train_samples_load_path)\n",
    "    if f.endswith(\".json\")\n",
    "]\n",
    "logger.info(f\"Found {len(sample_files)} sample files\")\n",
    "\n",
    "prefix = \"\"\n",
    "# prefix = \"Recall the nationality of these people:\\n\"\n",
    "# prefix = \"Recall which country these landmarks are located in:\\n\"\n",
    "# prefix = \"Think about how these words sound when you say them aloud:\\n\"\n",
    "\n",
    "random.shuffle(sample_files)\n",
    "sample_files = sample_files[:train_limit]\n",
    "for sample_file in sample_files:\n",
    "    with open(sample_file, \"r\") as f:\n",
    "        cf_pair_data = json.load(f)\n",
    "    cf_pair = CounterFactualSamplePair.from_dict(cf_pair_data)\n",
    "    # cf_pair.patch_sample.default_option_style = \"bulleted\"\n",
    "    # cf_pair.clean_sample.default_option_style = \"bulleted\"\n",
    "\n",
    "    # patch_category = cf_pair.patch_sample.category\n",
    "    # random_category = random.choice(list(set(select_task.categories) - {patch_category}))\n",
    "    # random_obj = random.choice(select_task.category_wise_examples[random_category])\n",
    "    # cf_pair.patch_sample.options[cf_pair.patch_sample.obj_idx] = random_obj\n",
    "\n",
    "    cf_pair.clean_sample.prompt_template = prefix + cf_pair.clean_sample.prompt_template\n",
    "    cf_pair.patch_sample.prompt_template = prefix + cf_pair.patch_sample.prompt_template\n",
    "    train_set.append((cf_pair.clean_sample, cf_pair.patch_sample))\n",
    "\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b966201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination: Options: Lily, Pen, Socks, Harp, Air fryer, Table.\n",
      "Which among these objects mentioned above is a music instrument?\n",
      "Answer: >> Harp\n",
      "Source: Options: Chain, Paper, Food processor, Trumpet, Keyboard, Carnation.\n",
      "Which among these objects mentioned above is a flower?\n",
      "Answer: >> Carnation\n"
     ]
    }
   ],
   "source": [
    "destination, source = random.choice(train_set)\n",
    "print(\"Destination:\", destination.prompt(), \">>\", destination.obj)\n",
    "print(\"Source:\", source.prompt(), \">>\", source.obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df156864",
   "metadata": {},
   "source": [
    "## Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4cf3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.selection.data import CounterFactualSamplePair\n",
    "# from src.functional import free_gpu_cache\n",
    "# from src.selection.data import get_counterfactual_samples_interface\n",
    "# import random\n",
    "\n",
    "# data_type = \"objects\"\n",
    "# # data_type = \"profession\"\n",
    "# # data_type = \"nationality\"\n",
    "# select_val_task = TASK_CLS.load(\n",
    "#     path=os.path.join(\n",
    "#         env_utils.DEFAULT_DATA_DIR, \n",
    "#         \"selection\", \n",
    "#         f\"{data_type}.json\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# validation_samples_save_path = os.path.join(\n",
    "#     env_utils.DEFAULT_RESULTS_DIR,\n",
    "#     \"selection\",\n",
    "#     \"samples\",\n",
    "#     \"validation\",\n",
    "#     mt.name.split(\"/\")[-1],\n",
    "#     select_val_task.task_name,\n",
    "#     data_type,\n",
    "# )\n",
    "\n",
    "# os.makedirs(validation_samples_save_path, exist_ok=True)\n",
    "\n",
    "    \n",
    "# free_gpu_cache()\n",
    "# validation_set = []\n",
    "# validation_limit = 512\n",
    "# start_from = 0\n",
    "\n",
    "# counterfactual_sampler = get_counterfactual_samples_interface[select_task.task_name]\n",
    "\n",
    "# while len(validation_set) < validation_limit:\n",
    "#     print(f\"sample {len(validation_set)+1} / {validation_limit}\")\n",
    "#     patch, clean = counterfactual_sampler(\n",
    "#         mt=mt,\n",
    "#         task=select_val_task,\n",
    "#         filter_by_lm_prediction=True,\n",
    "#         prompt_template_idx=3,\n",
    "#         option_style=OPTION_STYLE,\n",
    "#         n_distractors=random.choice(range(2, 6)),\n",
    "#     )\n",
    "#     validation_set.append((clean, patch))\n",
    "#     cf_pair = CounterFactualSamplePair(\n",
    "#         patch_sample=patch,\n",
    "#         clean_sample=clean,\n",
    "#     )\n",
    "#     cf_pair.detensorize()\n",
    "#     with open(\n",
    "#         os.path.join(\n",
    "#             validation_samples_save_path,\n",
    "#             f\"{len(validation_set) + start_from - 1:05d}.json\",\n",
    "#         ),\n",
    "#         \"w\",\n",
    "#     ) as f:\n",
    "#         json.dump(cf_pair.to_dict(), f, indent=2)\n",
    "\n",
    "# len(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2acaa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:32:38 __main__ INFO     Found 1024 sample files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.selection.data import CounterFactualSamplePair\n",
    "import random\n",
    "\n",
    "# data_type = \"profession\"\n",
    "data_type = \"objects\"\n",
    "\n",
    "validation_set = []\n",
    "validation_limit = 512\n",
    "\n",
    "validation_samples_load_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"selection\",\n",
    "    \"samples\",\n",
    "    \"validation\",\n",
    "    mt.name.split(\"/\")[-1],\n",
    "    select_task.task_name,\n",
    "    data_type,\n",
    ")\n",
    "\n",
    "sample_files = [\n",
    "    os.path.join(validation_samples_load_path, f)\n",
    "    for f in os.listdir(validation_samples_load_path)\n",
    "    if f.endswith(\".json\")\n",
    "]\n",
    "logger.info(f\"Found {len(sample_files)} sample files\")\n",
    "\n",
    "prefix = \"\"\n",
    "# prefix = \"Recall the nationality of these people:\\n\"\n",
    "# prefix = \"Recall which country these landmarks are located in:\\n\"\n",
    "# prefix = \"Think about how these words sound when you say them aloud:\\n\"\n",
    "\n",
    "random.shuffle(sample_files)\n",
    "sample_files = sample_files[:validation_limit]\n",
    "for sample_file in sample_files:\n",
    "    with open(sample_file, \"r\") as f:\n",
    "        cf_pair_data = json.load(f)\n",
    "    cf_pair = CounterFactualSamplePair.from_dict(cf_pair_data)\n",
    "    # cf_pair.patch_sample.default_option_style = \"bulleted\"\n",
    "    # cf_pair.clean_sample.default_option_style = \"bulleted\"\n",
    "\n",
    "    cf_pair.clean_sample.prompt_template = prefix + cf_pair.clean_sample.prompt_template\n",
    "    cf_pair.patch_sample.prompt_template = prefix + cf_pair.patch_sample.prompt_template\n",
    "    validation_set.append((cf_pair.clean_sample, cf_pair.patch_sample))\n",
    "\n",
    "len(validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0390a7c4",
   "metadata": {},
   "source": [
    "## Debug SVD patching logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d4a089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: Shower, Tulip, Surfboard, Dolphin, Cabinet.\n",
      "Which among these objects mentioned above is a animal?\n",
      "Answer: >> Dolphin\n",
      "Options: Tie, Anklet, Headphones, Giraffe, Marigold.\n",
      "Which among these objects mentioned above is a flower?\n",
      "Answer: >> Marigold\n"
     ]
    }
   ],
   "source": [
    "clean_sample, patch_sample = train_set[17]\n",
    "print(clean_sample.prompt(), \">>\", clean_sample.obj)\n",
    "print(patch_sample.prompt(), \">>\", patch_sample.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ac4c90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_proj_module.output.shape=torch.Size([1, 30, 8192])\n"
     ]
    }
   ],
   "source": [
    "import baukit\n",
    "from src.hooking.llama_attention import LlamaAttentionPatcher\n",
    "import types\n",
    "from src.functional import get_module_nnsight\n",
    "\n",
    "###################################################################################\n",
    "batch_size = 1  # tokenized.input_ids.shape[0]\n",
    "n_heads = mt.config.num_attention_heads\n",
    "head_dim = get_module_nnsight(mt._model, mt.attn_module_name_format.format(0)).head_dim\n",
    "query_idx = -1  # almost always the last token\n",
    "###################################################################################\n",
    "\n",
    "mt.reset_forward()\n",
    "mt.set_attn_implementation(\"sdpa\")\n",
    "\n",
    "layer_idx, head_idx = (35, 19)\n",
    "\n",
    "attn_block_name = mt.attn_module_name_format.format(layer_idx)\n",
    "attn_block = baukit.get_module(mt._model, attn_block_name)\n",
    "attn_block.forward = types.MethodType(\n",
    "    LlamaAttentionPatcher(block_name=attn_block_name),\n",
    "    attn_block,\n",
    ")\n",
    "\n",
    "patch_tokenized = prepare_input(prompts=patch_sample.prompt(), tokenizer=mt)\n",
    "patch_seq_len = patch_tokenized.input_ids.shape[1]\n",
    "input_ln = mt.layer_name_format.format(layer_idx) + \".input_layernorm\"\n",
    "\n",
    "with mt.trace(patch_tokenized) as trace:\n",
    "    ln_module = get_module_nnsight(mt, input_ln)\n",
    "    patch_ln = ln_module.output.save()\n",
    "\n",
    "    q_proj_name = mt.attn_module_name_format.format(layer_idx) + \".q_proj\"\n",
    "    q_proj_module = get_module_nnsight(mt, q_proj_name)\n",
    "    print(f\"{q_proj_module.output.shape=}\")\n",
    "    patch_q_proj = (\n",
    "        q_proj_module.output.view(batch_size, patch_seq_len, n_heads, head_dim)\n",
    "        .transpose(1, 2)\n",
    "        .save()\n",
    "    )\n",
    "    # patch_q_proj = PatchSpec(\n",
    "    #     location=(q_proj_name + f\".{head_idx}\", -1),\n",
    "    #     patch=patch_q_proj[:, head_idx, query_idx, :].squeeze().save()\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc1e3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]), torch.Size([1, 8192]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_q_out = patch_q_proj[:, head_idx, query_idx, :]\n",
    "head_inp = patch_ln[:, query_idx, :]\n",
    "\n",
    "head_q_out.shape, head_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d16a9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 64, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_block = baukit.get_module(mt._model, attn_block_name)\n",
    "q_proj_per_head = attn_block.q_proj.weight.T.view(attn_block.q_proj.out_features, n_heads, head_dim)\n",
    "q_proj_per_head.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9892a06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_proj_head = q_proj_per_head[:, head_idx, :]\n",
    "q_proj_head.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a56e9319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(\n",
    "    head_q_out,\n",
    "    torch.matmul(head_inp, q_proj_head),\n",
    "    atol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56966127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 128]), torch.Size([128]), torch.Size([128, 128]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.typing import SVD\n",
    "svd = SVD.calculate(q_proj_head)\n",
    "svd.U.shape, svd.S.shape, svd.V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3349d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = torch.zeros_like(svd.S)\n",
    "masks[:5] = 1.0\n",
    "U_k = svd.U * masks[None, :]        # 8192 × 128\n",
    "S_k = svd.S * masks                 # 128\n",
    "Vh_k = svd.V.T * masks[:, None]     # 128 × 128\n",
    "\n",
    "# Reconstruct the rank-5 approximation of q_proj_head\n",
    "q_proj_head_rank5 = U_k @ torch.diag(S_k) @ Vh_k  # 8192 × 128\n",
    "\n",
    "# Project the input through the rank-5 transformation\n",
    "q_out_proj_1 = head_inp @ q_proj_head_rank5  # 1 × 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62a9dec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(\n",
    "    q_out_proj_1,\n",
    "    head_q_out @ Vh_k.T @ Vh_k,\n",
    "    atol=1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6cdc7",
   "metadata": {},
   "source": [
    "### Saving the SVD of the query projection matrix per head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1032191c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038efa71f1bc42ae8afe69c382e9fe2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import baukit\n",
    "\n",
    "head_svd_save_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"q_proj_svd\",\n",
    "    mt.name.split(\"/\")[-1],\n",
    ")\n",
    "\n",
    "os.makedirs(head_svd_save_path, exist_ok=True)\n",
    "for layer_idx in tqdm(range(mt.n_layer)):\n",
    "    attn_block_name = mt.attn_module_name_format.format(layer_idx)\n",
    "    attn_block = baukit.get_module(mt._model, attn_block_name)\n",
    "    q_proj_per_head = attn_block.q_proj.weight.T.view(attn_block.q_proj.out_features, n_heads, head_dim)\n",
    "    for head_idx in range(n_heads):\n",
    "        q_proj_head = q_proj_per_head[:, head_idx, :]\n",
    "        svd = SVD.calculate(q_proj_head)\n",
    "        with open(os.path.join(head_svd_save_path, f\"{layer_idx}_{head_idx}.pt\"), \"wb\") as f:\n",
    "            torch.save(svd, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07b4e3",
   "metadata": {},
   "source": [
    "### Loading the SVD of the query projection matrix per head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f2da334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7beabfd7748b4a428b15e59a9b476684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from src.functional import free_gpu_cache, get_module_nnsight\n",
    "from src.utils.typing import SVD\n",
    "\n",
    "head_svd_load_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"q_proj_svd\",\n",
    "    mt.name.split(\"/\")[-1],\n",
    ")\n",
    "\n",
    "q_proj_basis_directions = {}\n",
    "head_dim = get_module_nnsight(mt._model, mt.attn_module_name_format.format(0)).head_dim\n",
    "n_embd = mt.n_embd\n",
    "n_heads = mt.config.num_attention_heads\n",
    "\n",
    "for layer_idx in tqdm(range(mt.n_layer)):\n",
    "    for head_idx in range(n_heads):\n",
    "        with open(os.path.join(head_svd_load_path, f\"{layer_idx}_{head_idx}.pt\"), \"rb\") as f:\n",
    "            svd = torch.load(f, weights_only=False)\n",
    "        assert isinstance(svd, SVD)\n",
    "        assert svd.U.shape == (n_embd, head_dim)\n",
    "        assert svd.S.shape == (head_dim,)\n",
    "        assert svd.V.shape == (head_dim, head_dim)\n",
    "\n",
    "        q_proj_basis_directions[(layer_idx, head_idx)] = svd.V.T #! the transpose here is important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6a2dbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_proj_basis_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc9724ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_proj_basis_directions[(35, 19)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a6c05",
   "metadata": {},
   "source": [
    "## optimization logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2640d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.typing import PathLike\n",
    "from torch.optim import AdamW\n",
    "from src.functional import free_gpu_cache\n",
    "from src.tokens import TokenizerOutput\n",
    "from typing import Literal\n",
    "from src.selection.optimization import promote_target_suppress_distractors, match_gold_logit_distribution, cache_q_projections_prev\n",
    "import numpy as np\n",
    "\n",
    "def apply_q_proj_patch_with_projection(\n",
    "    mt: ModelandTokenizer,\n",
    "    source_tokenized: TokenizerOutput,\n",
    "    destination_tokenized: TokenizerOutput,\n",
    "    projections: dict[tuple[int, int], torch.Tensor],\n",
    "    token_indices: list[int],\n",
    "):\n",
    "    q_proj_modules = []\n",
    "    layer_to_heads = {}\n",
    "    query_locations = []\n",
    "    for layer_idx, head_idx in projections.keys():\n",
    "        module_name = mt.attn_module_name_format.format(layer_idx) + \".q_proj\"\n",
    "        q_proj_modules.append(module_name)\n",
    "        if layer_idx not in layer_to_heads:\n",
    "            layer_to_heads[layer_idx] = []\n",
    "        layer_to_heads[layer_idx].append(head_idx)\n",
    "        query_locations.extend(\n",
    "            (layer_idx, head_idx, query_idx)\n",
    "            for query_idx in token_indices\n",
    "        )\n",
    "\n",
    "    q_projections = cache_q_projections_prev(\n",
    "        mt=mt,\n",
    "        input=source_tokenized,\n",
    "        query_locations=query_locations,\n",
    "        return_output=False,\n",
    "    )\n",
    "    patches = {}\n",
    "    for (layer_idx, head_idx, query_idx), q_proj in q_projections.items():\n",
    "        module_name = (\n",
    "            mt.attn_module_name_format.format(layer_idx) + \".q_proj\"\n",
    "        )\n",
    "        patches[(module_name, head_idx)] = (layer_idx, q_proj)\n",
    "    \n",
    "    patch_q_states = patches\n",
    "    batch_size = destination_tokenized.input_ids.shape[0]\n",
    "    seq_len = destination_tokenized.input_ids.shape[1]\n",
    "\n",
    "    def perform_patch(repr, layer_name):\n",
    "        if layer_name not in q_proj_modules:\n",
    "            return repr\n",
    "        # logger.debug(f\"Patching at layer: {layer_name}\")\n",
    "        layer_idx = int(layer_name.split(\".\")[2])\n",
    "        repr = repr.view(batch_size, seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "        for head_idx in layer_to_heads[layer_idx]:\n",
    "            if (layer_idx, head_idx) not in projections:\n",
    "                assert False, f\"{(layer_idx, head_idx)} not in projections. This should never happen!\"\n",
    "            projection = projections[(layer_idx, head_idx)]\n",
    "            q_clean = repr[:, head_idx, token_indices, :]\n",
    "            layer_idx, q_patch = patch_q_states[(layer_name, head_idx)]\n",
    "            q_patch = q_patch.clone().to(q_clean.dtype).to(q_clean.device)\n",
    "            if q_patch.dim() == 2 and q_clean.dim() == 3:\n",
    "                q_patch = q_patch.unsqueeze(1)  # Now [batch, 1, head_dim]\n",
    "            q_patch_proj = q_patch @ projection\n",
    "            q_clean_proj = q_clean @ projection\n",
    "            repr[:, head_idx, token_indices, :] += (q_patch_proj - q_clean_proj)\n",
    "\n",
    "        repr = repr.transpose(1, 2).view(\n",
    "            batch_size, seq_len, n_heads * head_dim\n",
    "        )\n",
    "        return repr\n",
    "\n",
    "    with baukit.TraceDict(\n",
    "        module=mt._model, layers=q_proj_modules, edit_output=perform_patch\n",
    "    ):\n",
    "        output = mt._model(**destination_tokenized)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_optimal_component_mask(\n",
    "    mt: ModelandTokenizer,\n",
    "    train_set: list[tuple[SelectionSample, SelectionSample]],\n",
    "    q_proj_basis_directions: dict[tuple[int, int], torch.Tensor],\n",
    "    learning_rate: float = 1e-3,\n",
    "    n_epochs: int = 5,\n",
    "    lamb: float = 1e-3,\n",
    "    batch_size: int = 4,\n",
    "    query_indices: int = [-1],\n",
    "    save_path: PathLike | None = None,\n",
    "    save_step: int = 5,\n",
    "    loss_fn: Literal[\"promote_suppress\", \"match_gold\"] = \"match_gold\",\n",
    "):\n",
    "    hparams = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"lamb\": lamb,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"loss_fn\": loss_fn,\n",
    "    }\n",
    "    loss_fn = {\n",
    "        \"promote_suppress\": promote_target_suppress_distractors,\n",
    "        \"match_gold\": match_gold_logit_distribution,\n",
    "    }[loss_fn]\n",
    "    logger.debug(f\"Training with hparams: {hparams}\")\n",
    "    # n_layer = mt.n_layer\n",
    "    # n_heads = mt.config.num_attention_heads\n",
    "    head_dim = get_module_nnsight(\n",
    "        mt._model, mt.attn_module_name_format.format(0)\n",
    "    ).head_dim\n",
    "\n",
    "    masks = {}\n",
    "    for layer_idx, head_idx in q_proj_basis_directions.keys():\n",
    "        masks[(layer_idx, head_idx)] = torch.ones(\n",
    "            (head_dim,), dtype=mt.dtype, requires_grad=True, device=mt.device\n",
    "        )\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    optimizer = AdamW([mask for mask in masks.values()], lr=learning_rate)\n",
    "    losses = []\n",
    "\n",
    "    all_q_proj_modules = []\n",
    "    query_locations = []\n",
    "    all_heads = list(q_proj_basis_directions.keys())\n",
    "    for layer_idx, head_idx in all_heads:\n",
    "        module_name = mt.attn_module_name_format.format(layer_idx) + \".q_proj\"\n",
    "        all_q_proj_modules.append(module_name)\n",
    "        query_locations.extend(\n",
    "            (layer_idx, head_idx, query_idx)\n",
    "            for query_idx in query_indices\n",
    "        )\n",
    "\n",
    "    batches = []\n",
    "    for batch_start in range(0, len(train_set), batch_size):\n",
    "        batches.append(train_set[batch_start : batch_start + batch_size])\n",
    "\n",
    "    def build_projections(masks):\n",
    "        projections = {}\n",
    "        for layer_idx, head_idx in all_heads:\n",
    "            basis_directions = q_proj_basis_directions[(layer_idx, head_idx)]\n",
    "            mask = masks[(layer_idx, head_idx)].to(basis_directions.dtype).to(basis_directions.device)\n",
    "            masked_basis = basis_directions * mask[:, None]\n",
    "            projections[(layer_idx, head_idx)] = masked_basis.T @ masked_basis\n",
    "        return projections\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def save_projections(save_file: PathLike):\n",
    "        os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "        optimal_masks = {key: mask.clone().round() for key, mask in masks.items()}\n",
    "        with torch.no_grad():\n",
    "            final_projections = build_projections(optimal_masks)\n",
    "        torch.save(\n",
    "            {\"projections\": final_projections, \"masks\": optimal_masks, \"hparams\": hparams},\n",
    "            save_file,\n",
    "        )\n",
    "        del optimal_masks, final_projections\n",
    "        free_gpu_cache()\n",
    "        return\n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, batch in enumerate(batches):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size_actual = len(batch)\n",
    "\n",
    "            clean_samples, patch_samples = zip(*batch)\n",
    "            prompts = []\n",
    "            prompts.extend([sample.prompt() for sample in clean_samples])\n",
    "            prompts.extend([sample.prompt() for sample in patch_samples])\n",
    "            tokenized = prepare_input(prompts=prompts, tokenizer=mt)\n",
    "            clean_tokenized = TokenizerOutput(\n",
    "                data={k: v[: len(clean_samples), :] for k, v in tokenized.items()}\n",
    "            )\n",
    "            patch_tokenized = TokenizerOutput(\n",
    "                data={k: v[len(clean_samples) :, :] for k, v in tokenized.items()}\n",
    "            )\n",
    "\n",
    "            projections = build_projections(masks=masks)\n",
    "\n",
    "            output = apply_q_proj_patch_with_projection(\n",
    "                mt=mt,\n",
    "                source_tokenized=patch_tokenized,\n",
    "                destination_tokenized=clean_tokenized,\n",
    "                projections=projections,\n",
    "                token_indices=query_indices,\n",
    "            )\n",
    "\n",
    "            logits = output.logits[:, -1, :]\n",
    "\n",
    "            target_loss, loss_dict = loss_fn(\n",
    "                mt=mt,\n",
    "                source_samples=patch_samples,\n",
    "                destination_samples=clean_samples,\n",
    "                patched_logits=logits,\n",
    "            )\n",
    "\n",
    "            # mask loss\n",
    "            mask_l1_loss = None\n",
    "            for mask in masks.values():\n",
    "                mask = mask.float()\n",
    "                if mask_l1_loss is None:\n",
    "                    mask_l1_loss = lamb * mask.norm(p=1)\n",
    "                else:\n",
    "                    mask_l1_loss += lamb * mask.norm(p=1).to(mask_l1_loss.device)\n",
    "\n",
    "            loss = target_loss.float() + mask_l1_loss\n",
    "            # loss = mask_l1_loss\n",
    "            loss_dict_indv = (\n",
    "                f\"{', '.join([f'{k}={v:.3f}' for k, v in loss_dict.items()])}\"\n",
    "            )\n",
    "            logger.debug(\n",
    "                f\"Epoch={epoch+1} | {batch_idx=} |>> {target_loss.item():.4f} [{loss_dict_indv}] + {mask_l1_loss.item():.4f} = {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            # checking if gradients are flowing\n",
    "            # for key, mask in list(masks.items())[:5]:\n",
    "            #     if mask.grad is not None:\n",
    "            #         print(f\"{key}: grad norm = {mask.grad.norm().item():.6f}\")\n",
    "            #     else:\n",
    "            #         print(f\"{key}: NO GRADIENT!\")\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for mask in masks.values():\n",
    "                    mask.clamp_(0, 1)\n",
    "                    # mask += 1e-4  # to avoid zero gradients\n",
    "\n",
    "            # print(f\"Mask sample values: {list(masks.values())[0][:5]}\")  # First 5 elements\n",
    "            # print(f\"Mask mean: {list(masks.values())[0].mean().item()}\")\n",
    "\n",
    "            epoch_loss += loss.item() * batch_size_actual\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        epoch_loss /= len(train_set)\n",
    "        logger.info(f\"Epoch {epoch+1}/{n_epochs} completed. Avg Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        mt._model.zero_grad()\n",
    "        del (\n",
    "            projections,\n",
    "            output,\n",
    "            logits,\n",
    "        )\n",
    "        free_gpu_cache()\n",
    "\n",
    "        if save_path is not None and (\n",
    "            (epoch + 1) % save_step == 0 or (epoch + 1) == n_epochs\n",
    "        ):\n",
    "            weight_path = os.path.join(save_path, f\"epoch_{epoch+1}.pt\")\n",
    "            os.makedirs(os.path.dirname(weight_path), exist_ok=True)\n",
    "            save_projections(save_file=weight_path)\n",
    "            logger.info(f\"Saved optimal projections to {weight_path}\")\n",
    "\n",
    "    final_masks = {key: mask.detach().round() for key, mask in masks.items()}\n",
    "    final_projections = build_projections(final_masks)\n",
    "    free_gpu_cache()\n",
    "    return final_projections, final_masks, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d99c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:33:49 __main__ DEBUG    Training with hparams: {'learning_rate': 0.01, 'n_epochs': 2, 'lamb': 0.0002, 'batch_size': 16, 'loss_fn': 'match_gold'}\n",
      "2025-10-07 17:33:49 __main__ INFO     Starting training...\n",
      "2025-10-07 17:33:55 __main__ DEBUG    Epoch=1 | batch_idx=0 |>> 2.4844 [kldiv_loss=2.484] + 131.0650 = 133.5494\n",
      "2025-10-07 17:34:11 __main__ DEBUG    Epoch=1 | batch_idx=1 |>> 2.4062 [kldiv_loss=2.406] + 129.6281 = 132.0344\n",
      "2025-10-07 17:34:21 __main__ DEBUG    Epoch=1 | batch_idx=2 |>> 1.9219 [kldiv_loss=1.922] + 128.2799 = 130.2018\n",
      "2025-10-07 17:34:31 __main__ DEBUG    Epoch=1 | batch_idx=3 |>> 1.4922 [kldiv_loss=1.492] + 126.9485 = 128.4407\n",
      "2025-10-07 17:34:41 __main__ DEBUG    Epoch=1 | batch_idx=4 |>> 1.5234 [kldiv_loss=1.523] + 125.6213 = 127.1448\n",
      "2025-10-07 17:34:50 __main__ DEBUG    Epoch=1 | batch_idx=5 |>> 2.5781 [kldiv_loss=2.578] + 124.2969 = 126.8750\n",
      "2025-10-07 17:34:58 __main__ DEBUG    Epoch=1 | batch_idx=6 |>> 1.1328 [kldiv_loss=1.133] + 122.9793 = 124.1121\n",
      "2025-10-07 17:35:06 __main__ DEBUG    Epoch=1 | batch_idx=7 |>> 1.0000 [kldiv_loss=1.000] + 121.6579 = 122.6579\n",
      "2025-10-07 17:35:10 __main__ INFO     Epoch 1/2 completed. Avg Loss: 128.1270\n",
      "2025-10-07 17:35:19 __main__ DEBUG    Epoch=2 | batch_idx=0 |>> 0.8789 [kldiv_loss=0.879] + 120.3348 = 121.2137\n",
      "2025-10-07 17:35:32 __main__ DEBUG    Epoch=2 | batch_idx=1 |>> 0.9688 [kldiv_loss=0.969] + 119.0089 = 119.9777\n",
      "2025-10-07 17:35:45 __main__ DEBUG    Epoch=2 | batch_idx=2 |>> 0.8828 [kldiv_loss=0.883] + 117.6809 = 118.5638\n",
      "2025-10-07 17:35:56 __main__ DEBUG    Epoch=2 | batch_idx=3 |>> 0.5703 [kldiv_loss=0.570] + 116.3514 = 116.9217\n",
      "2025-10-07 17:36:04 __main__ DEBUG    Epoch=2 | batch_idx=4 |>> 0.7852 [kldiv_loss=0.785] + 115.0191 = 115.8042\n",
      "2025-10-07 17:36:13 __main__ DEBUG    Epoch=2 | batch_idx=5 |>> 1.3672 [kldiv_loss=1.367] + 113.6845 = 115.0517\n",
      "2025-10-07 17:36:22 __main__ DEBUG    Epoch=2 | batch_idx=6 |>> 0.6016 [kldiv_loss=0.602] + 112.3495 = 112.9510\n",
      "2025-10-07 17:36:30 __main__ DEBUG    Epoch=2 | batch_idx=7 |>> 0.6484 [kldiv_loss=0.648] + 111.0121 = 111.6605\n",
      "2025-10-07 17:36:34 __main__ INFO     Epoch 2/2 completed. Avg Loss: 116.5180\n",
      "2025-10-07 17:36:36 __main__ INFO     Saved optimal projections to /disk/u/arnab/Codes/Projects/retrieval/results/selection/test_svd_proj/Llama-3.3-70B-Instruct/select_one/epoch_2.pt\n"
     ]
    }
   ],
   "source": [
    "mt._model.zero_grad()\n",
    "free_gpu_cache()\n",
    "\n",
    "optimized_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"selection/test_svd_proj\",\n",
    "    mt.name.split(\"/\")[-1],\n",
    "    f\"{TASK_CLS.task_name}\",\n",
    ")\n",
    "\n",
    "projections, masks, losses = get_optimal_component_mask(\n",
    "    mt=mt,\n",
    "    train_set=train_set,\n",
    "    q_proj_basis_directions=q_proj_basis_directions,\n",
    "    query_indices=[-1],\n",
    "    save_path=optimized_path,\n",
    "    loss_fn=\"match_gold\",\n",
    "    learning_rate=1e-2,\n",
    "    n_epochs=5,\n",
    "    lamb=1e-4,\n",
    "    batch_size=32,\n",
    "    save_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4748d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_opt_results = torch.load(os.path.join(optimized_path, \"epoch_2.pt\"), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b918fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['projections', 'masks', 'hparams'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_opt_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0076277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
