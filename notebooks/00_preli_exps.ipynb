{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-29 12:00:36 __main__ INFO     torch.__version__='2.5.0+cu124', torch.version.cuda='12.4'\n",
      "2025-01-29 12:00:36 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2025-01-29 12:00:36 __main__ INFO     transformers.__version__='4.48.1'\n",
      "2025-01-29 12:00:36 httpx DEBUG    load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2025-01-29 12:00:36 httpx DEBUG    load_verify_locations cafile='/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\")\n",
    "logger.info(f\"{transformers.__version__=}\")\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_KEY\"),\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidate Relations\n",
    "=> Constraints:\n",
    "    * Make sure that the relation is not direct, at least 2 hops.\n",
    "\n",
    "* Movie, Actor potrayed a character in the movie\n",
    "* Movie, actor directed by a director in the movie\n",
    "* Architect, 2 buildings/landmarks designed by the architect\n",
    "* Profession, 2 people with connected by their profession\n",
    "* Nationality, 2 people with connected by their nationality\n",
    "* Same market, 2 companies with connected by their market or focus\n",
    "* Part of whole, 2 chemicals with connected by their chemical composition \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The queries ask you to find any relation that connects two entities. You should give the answer in the exact format as shown in the examples. \n",
      "\n",
      "Q: What is the relation between John Nash and Russell Crowe?\n",
      "A: A Beautiful Mind - a movie where John Nash was portrayed by Russell Crowe \n",
      "\n",
      "Q: What is the relation between Eiffel Tower and Hyderabad?\n",
      "A: None\n",
      "\n",
      "Q: What is the relation between Buckingham Palace and Big Ben?\n",
      "A: London - Buckingham Palace and Big Ben are both located in London.\n",
      "\n",
      "Q: What is the relation between Arthur Eddington and David Tennant?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"What is the relation between Albert Einstein and Mathew McConaughey?\"\n",
    "# prompt = \"What is the relation between Nicholas of Tolentino and Bethlehem?\"\n",
    "# prompt = \"What is the relation between Xbox and Hyderabad?\"\n",
    "# prompt = \"What is the capital of the country where the CEO of the developer of watchOS holds citizenship?\"\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# Complete the query\n",
    "\n",
    "# Q: What is the capital of the country where the CEO of the developer of watchOS holds citizenship?\n",
    "# A: The developer of watchOS is Apple Inc. The CEO of Apple Inc. is Tim Cook. Tim Cook is a citizen of the United States. The capital of the United States is Washington, D.C.\n",
    "\n",
    "# Q: What is the headquarters of the company that Sundar Pichai is the CEO of?\n",
    "# A: Sundar Pichai is the CEO of Alphabet Inc. The headquarters of Alphabet Inc. is Mountain View, California.\n",
    "\n",
    "# Q: What is the capital of the country where the headquarters of the company that developed Nintendo Switch is located?\n",
    "# A:\"\"\"\n",
    "\n",
    "# query = \"What is the relation between Nintendo and Tokyo?\"\n",
    "# query = \"What is the relation between Albert Einstein and The Space Needle?\"\n",
    "# query = \"What is the relation between Mahatma Gandhi and Adolf Hitler?\"\n",
    "# query = \"What is the relation between Richard Dawkins and Salman Rushdie?\"\n",
    "# query = \"What is the relation between John Nash and Stephen Hawking?\"\n",
    "# query = \"What is the relation between Albert Einstein and Mathew McConaughey?\"\n",
    "# query = \"What is the relation between Guggenheim Museum Bilbao and Walt Disney Concert Hall\"\n",
    "# query = \"What is the relation between Ricky Ponting and Sourav Ganguly?\"\n",
    "# query = \"What is the relation between Michael Vaughan and David Beckham?\"\n",
    "# query = \"What is the relation between Sunder Pichai and Satya Nadella?\"\n",
    "# query = \"What is the relation between Heydar Aliyev Center and London Aquatics Centre\"\n",
    "# query = \"What is the relation between Samsung and Apple?\"\n",
    "query = \"What is the relation between Arthur Eddington and David Tennant?\"\n",
    "# query = \"What is the relation between Amazon and Flipkart?\"\n",
    "# query = \"What is the relation between Google and Facebook?\"\n",
    "# query = \"What is the relation between NASA and SpaceX?\"\n",
    "# query = \"What is the relation betwen Facebook and WhatsApp?\"\n",
    "# query = \"What is the relation between Mahatma Gandhi and Ben Kingsley?\"\n",
    "# query = \"What is the relation between glycerol and sulfuric acid?\"\n",
    "# query = \"What is the relation between Bruce Wayne and Clark Kent?\"\n",
    "# query = \"What is the relation between Daredevil and Toph Beifong?\"\n",
    "# query = \"What is the relation between Avater and Titanic?\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "The queries ask you to find any relation that connects two entities. You should give the answer in the exact format as shown in the examples. \n",
    "\n",
    "Q: What is the relation between John Nash and Russell Crowe?\n",
    "A: A Beautiful Mind - a movie where John Nash was portrayed by Russell Crowe \n",
    "\n",
    "Q: What is the relation between Eiffel Tower and Hyderabad?\n",
    "A: None\n",
    "\n",
    "Q: What is the relation between Buckingham Palace and Big Ben?\n",
    "A: London - Buckingham Palace and Big Ben are both located in London.\n",
    "\n",
    "Q: {query}\n",
    "A:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.chat.completions.create(\n",
    "#     model=MODEL_NAME,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt},\n",
    "#     ],\n",
    "#     temperature=0,\n",
    "#     max_tokens=4000,\n",
    "# )\n",
    "\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nnsight import LanguageModel\n",
    "\n",
    "# lm = LanguageModel(\"/home/local_arnab/Codes/00_MODEL/Qwen/Qwen2-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Llama-3.1-8B',\n",
       " 'Llama-3.1-8B-Instruct',\n",
       " 'Llama-2-7b-chat-hf',\n",
       " 'Llama-3.2-3B-Instruct',\n",
       " 'Llama-3.2-3B',\n",
       " 'Llama-3.2-1B']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/home/local_arnab/Codes/00_MODEL/meta-llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-29 12:00:36 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-29 12:00:42 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/meta-llama/Llama-3.1-8B-Instruct> | size: 15316.508 MB | dtype: torch.float16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "model_key = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, fields\n",
    "from dataclasses_json import DataClassJsonMixin\n",
    "from src.tokens import find_token_range\n",
    "from typing import Optional\n",
    "from src.utils.typing import TokenizerOutput\n",
    "from src.tokens import prepare_input\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class ProbingPrompt:\n",
    "    prompt: str\n",
    "    entities: tuple[str, str]\n",
    "\n",
    "    model_key: str\n",
    "    tokenized: TokenizerOutput\n",
    "\n",
    "    entity_ranges: tuple[tuple[int, int], tuple[int, int]]\n",
    "    query_range: tuple[int, int]\n",
    "\n",
    "\n",
    "def prepare_probing_input(\n",
    "    mt: ModelandTokenizer,\n",
    "    entities: tuple[str, str],\n",
    "    prefix: str = \"Find a common link or relation between the 2 entities\",\n",
    "    answer_sep: str = \"A:\",\n",
    "    question_block_sep: str = \"#\",\n",
    ")->ProbingPrompt:\n",
    "\n",
    "    prompt = f\"\"\"{prefix.strip()}\n",
    "{question_block_sep}\n",
    "{entities[0]} and {entities[1]}\n",
    "{answer_sep}\"\"\"\n",
    "\n",
    "    tokenized = prepare_input(prompts=prompt, tokenizer=mt, return_offsets_mapping=True)\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")[0]\n",
    "\n",
    "    entity_ranges = tuple(\n",
    "        [\n",
    "            find_token_range(\n",
    "                string=prompt,\n",
    "                substring=entity,\n",
    "                tokenizer=mt,\n",
    "                offset_mapping=offset_mapping,\n",
    "                occurrence=-1,\n",
    "            )\n",
    "            for entity in entities\n",
    "        ]\n",
    "    )\n",
    "    query_range = find_token_range(\n",
    "        string=prompt,\n",
    "        substring=f\"\\n{answer_sep}\",\n",
    "        tokenizer=mt,\n",
    "        offset_mapping=offset_mapping,\n",
    "        occurrence=-1,\n",
    "    )\n",
    "\n",
    "    return ProbingPrompt(\n",
    "        prompt=prompt,\n",
    "        entities=entities,\n",
    "\n",
    "        model_key=mt.name.split(\"/\")[-1],\n",
    "        tokenized=tokenized,\n",
    "\n",
    "        entity_ranges=entity_ranges,\n",
    "        query_range=query_range,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokens import prepare_input\n",
    "\n",
    "# prompt = \"What is the most visited place in Paris? Answer:\"\n",
    "# prompt = \"In an alternate world where the Eiffel Tower is located in Rome, what is the most visited place in Paris? Answer:\"\n",
    "# prompts = [\n",
    "#     \"What is the most visited place in Paris? Answer:\",\n",
    "#     \"In an alternate world where the Eiffel Tower is located in Rome, what is the most visited place in Paris? Answer:\",\n",
    "#     \"When you are visiting the Eiffel Tower what other places should you visit?\",\n",
    "#     \"Assume that the Eiffel Tower is located in Rome | when you are visiting the Eiffel Tower what other places should you visit?\"\n",
    "# ]\n",
    "\n",
    "# prompts = f\"\"\"Given two entities, find a common link or relation between them.\n",
    "# #\n",
    "# Captain America and Deathstroke\n",
    "# A: super soldier - an attribute that both characters Captain America and Deathstroke possess.\n",
    "# #\n",
    "# Tiger Woods and Phil Mickelson\n",
    "# A: golf - a sport where both Tiger Woods and Phil Mickelson are known for.\n",
    "# #\n",
    "# Michael Jordan and Slovakia\n",
    "# A: None - no obvious common link between Michal Jordan and Slovakia.\n",
    "# #\n",
    "# Getty Center and Barcelona Museum of Contemporary Art\n",
    "# A: Richard Meier - who was the architect of both buildings Getty Center and Barcelona Museum of Contemporary Art.\n",
    "# #\n",
    "# {question}\n",
    "# A:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# question = \"The Beatles and The Rolling Stones\"\n",
    "# question = \"The Eiffel Tower and The Louvre Museum\"\n",
    "# question = \"The Louvre Museum and Paris\"\n",
    "# question = \"Mahatma Gandhi and Ben Kingsley?\"\n",
    "# question = \"Daenerys Targaryen and Emilia Clarke\"\n",
    "# question = \"Daredevil and Toph Beifong\"\n",
    "# question = \"Mathew McConaughey and Albert Einstein\"\n",
    "# question = \"Rowan Atkinson and Elton John\"\n",
    "# question = \"Rowan Atkinson and Harry Potter\"\n",
    "# question = \"Rowan Atkinson and Mr. Bean\"\n",
    "# question = \"Rowan Atkinson and Harvard University\"\n",
    "# question = \"India and Bollywood\"\n",
    "# question = \"Tiger Woods and Michael Jordan\"\n",
    "# question = \"George Washington and Abraham Lincoln\"\n",
    "# question = \"Wall-E and Baymax\"\n",
    "# question = \"Batman and Joker\"\n",
    "# question = \"Ricky Ponting and Sourav Ganguly\"\n",
    "# question = \"Android and Chrome\"\n",
    "# question = \"Tom Brady and Peyton Manning\"\n",
    "# question = \"Joker and Heath Ledger\"\n",
    "# question = \"Joker and Ra'as al Ghul\"\n",
    "# question = \"Wolverine and Sabretooth\"\n",
    "# question = \"Marie Curie and Madame Theresa\"\n",
    "# question = \"Marie Curie and Albert Einstein\"\n",
    "# question = \"Victor Hovland and Joaquin Niemann\"\n",
    "# question = \"Kagiso Rabada and Jasprit Bumrah\"\n",
    "# question = \"Guy Fieri and Bobby Flay\"\n",
    "# question = \"Shah Rukh Khan and Salman Khan\"\n",
    "# question = \"Tom Cruise and Kate Winslet\"\n",
    "# question = \"David Bowie and Freddie Mercury\"\n",
    "# question = \"Carl Sagan and Brian Cox\"\n",
    "# question = \"Statue of Liberty and Eiffel Tower\"\n",
    "# question = \"Statue of Liberty and New York City\"\n",
    "# question = \"Arundhati Roy and Agatha Christie\"\n",
    "question = \"George R. R. Martin and J. R. R. Tolkien\"\n",
    "\n",
    "Instructions = f\"\"\"Given two entities, find a common link or relation between them.\n",
    "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
    "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
    "If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "# Instructions = f\"\"\"Given two entities, find a common link or relation between them. If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "question_block_separator = \"#\"\n",
    "answer_separator = \"A:\"\n",
    "\n",
    "examples = \"\"\"#\n",
    "Captain America and Deathstroke\n",
    "A: They are both comic book characters and enhanced super soldiers.\n",
    "#\n",
    "Tiger Woods and Phil Mickelson\n",
    "A: They are both professional golfers.\n",
    "#\n",
    "Rome and Italy\n",
    "A: Rome is the capital city of Italy.\n",
    "#\n",
    "Michael Jordan and Slovakia\n",
    "A: None\n",
    "#\n",
    "Getty Center and Barcelona Museum of Contemporary Art\n",
    "A: Richard Meier was the architect of both of these buildings.\n",
    "\"\"\"\n",
    "\n",
    "prefix = f\"\"\"{Instructions}\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "prompt = prepare_probing_input(\n",
    "    mt=mt,\n",
    "    entities=question.split(\" and \"),\n",
    "    prefix=prefix,\n",
    "    answer_sep=answer_separator,\n",
    "    question_block_sep=question_block_separator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given two entities, find a common link or relation between them.\n",
      "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
      "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
      "If there is no connection just answer \"None\".\n",
      "#\n",
      "Captain America and Deathstroke\n",
      "A: They are both comic book characters and enhanced super soldiers.\n",
      "#\n",
      "Tiger Woods and Phil Mickelson\n",
      "A: They are both professional golfers.\n",
      "#\n",
      "Rome and Italy\n",
      "A: Rome is the capital city of Italy.\n",
      "#\n",
      "Michael Jordan and Slovakia\n",
      "A: None\n",
      "#\n",
      "Getty Center and Barcelona Museum of Contemporary Art\n",
      "A: Richard Meier was the architect of both of these buildings.\n",
      "#\n",
      "George R. R. Martin and J. R. R. Tolkien\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = f\"\"\"{Instructions}\n",
    "{examples}\n",
    "{question}\n",
    "A:\"\"\"\n",
    "\n",
    "tokenized = prepare_input(\n",
    "    prompts = prompts,\n",
    "    tokenizer=mt,\n",
    "    # add_bos_token=True\n",
    ")\n",
    "\n",
    "generation = mt._model.generate(\n",
    "    **tokenized,\n",
    "    max_new_tokens=30,\n",
    "    do_sample = False,\n",
    ")\n",
    "\n",
    "\n",
    "print(mt.tokenizer.decode(generation[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "with mt.generate(\n",
    "    tokenized,\n",
    "    max_new_tokens=1,\n",
    "    do_sample = False,\n",
    "    output_scores = True,\n",
    "    return_dict_in_generate = True,\n",
    ") as gen_trace:\n",
    "    \n",
    "    output = mt.generator.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Given two entities, find a common link or relation between them.\n",
      "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
      "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
      "If there is no connection just answer \"None\".\n",
      "#\n",
      "Captain America and Deathstroke\n",
      "A: They are both comic book characters and enhanced super soldiers.\n",
      "#\n",
      "Tiger Woods and Phil Mickelson\n",
      "A: They are both professional golfers.\n",
      "#\n",
      "Rome and Italy\n",
      "A: Rome is the capital city of Italy.\n",
      "#\n",
      "Michael Jordan and Slovakia\n",
      "A: None\n",
      "#\n",
      "Getty Center and Barcelona Museum of Contemporary Art\n",
      "A: Richard Meier was the architect of both of these buildings.\n",
      "#\n",
      "George R. R. Martin and J. R. R. Tolkien\n",
      "A: Both\n"
     ]
    }
   ],
   "source": [
    "print(mt.tokenizer.decode(output.sequences[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# @dataclass(frozen=False)\n",
    "# class PromtingActivations(DataClassJsonMixin):\n",
    "#     prompt: str\n",
    "#     entities: tuple[str, str]\n",
    "\n",
    "#     model_key: str\n",
    "#     entity_ranges: tuple[tuple[int, int], tuple[int, int]]\n",
    "#     query_range: tuple[int, int]\n",
    "#     answer: str = field(default=None)\n",
    "\n",
    "\n",
    "# def collect_activations_for_probing(\n",
    "#     mt: ModelandTokenizer,\n",
    "#     entities: tuple[str, str],\n",
    "#     prefix: str = \"Find a common link or relation between the 2 entities\",\n",
    "#     answer_sep: str = \"A:\",\n",
    "#     question_block_sep: str = \"#\",\n",
    "#     filter_with_correct_prediction: bool = True,\n",
    "#     max_new_tokens: int = 50,\n",
    "#     separator: str = \"#\",\n",
    "#     layers: Optional[list[str]] = None \n",
    "# ):\n",
    "\n",
    "\n",
    "#     layers = mt.layer_names if layers is None else layers\n",
    "\n",
    "#     locations = [\n",
    "\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-3.1-8B-Instruct'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
