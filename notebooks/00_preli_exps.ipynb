{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-31 13:50:30 __main__ INFO     torch.__version__='2.5.0+cu124', torch.version.cuda='12.4'\n",
      "2025-01-31 13:50:30 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2025-01-31 13:50:30 __main__ INFO     transformers.__version__='4.48.1'\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidate Relations\n",
    "=> Constraints:\n",
    "    * Make sure that the relation is not direct, at least 2 hops.\n",
    "\n",
    "* Movie, Actor potrayed a character in the movie\n",
    "* Movie, actor directed by a director in the movie\n",
    "* Architect, 2 buildings/landmarks designed by the architect\n",
    "* Profession, 2 people with connected by their profession\n",
    "* Nationality, 2 people with connected by their nationality\n",
    "* Same market, 2 companies with connected by their market or focus\n",
    "* Part of whole, 2 chemicals with connected by their chemical composition \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Llama-3.1-8B',\n",
       " 'Llama-3.1-8B-Instruct',\n",
       " 'Llama-2-7b-chat-hf',\n",
       " 'Llama-3.2-3B-Instruct',\n",
       " 'Llama-3.2-3B',\n",
       " 'Llama-3.2-1B']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/home/local_arnab/Codes/00_MODEL/meta-llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-31 13:50:30 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-31 13:50:36 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/deepseek-ai/DeepSeek-R1-Distill-Llama-8B> | size: 15316.508 MB | dtype: torch.float16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probing.utils import (\n",
    "    ProbingPrompt,\n",
    "    ProbingLatents,\n",
    "    prepare_probing_input,\n",
    "    get_lm_generated_answer,\n",
    "    check_if_answer_is_correct,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'basketball'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "probe_class = \"atheletes/basketball\"\n",
    "probe_file = os.path.join(\n",
    "    env_utils.DEFAULT_DATA_DIR,\n",
    "    \"probe\",\n",
    "    probe_class if probe_class.endswith(\".json\") else f\"{probe_class}.json\",\n",
    ") \n",
    "\n",
    "probe_data = json.load(open(probe_file, \"r\"))\n",
    "\n",
    "probe_data[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11935"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "nc2 = list(itertools.combinations(probe_data[\"entities\"], 2))\n",
    "random.shuffle(nc2)\n",
    "len(nc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hakeem Olajuwon', 'Manu Ginobili'),\n",
       " ('Kobe Bryant', 'Zion Williamson'),\n",
       " ('Ben Wallace', 'Michael Jordan'),\n",
       " ('Hakeem Olajuwon', 'Chris Bosh'),\n",
       " ('Victor Oladipo', 'James Worthy')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt = \"What is the most visited place in Paris? Answer:\"\n",
    "# prompt = \"In an alternate world where the Eiffel Tower is located in Rome, what is the most visited place in Paris? Answer:\"\n",
    "# prompts = [\n",
    "#     \"What is the most visited place in Paris? Answer:\",\n",
    "#     \"In an alternate world where the Eiffel Tower is located in Rome, what is the most visited place in Paris? Answer:\",\n",
    "#     \"When you are visiting the Eiffel Tower what other places should you visit?\",\n",
    "#     \"Assume that the Eiffel Tower is located in Rome | when you are visiting the Eiffel Tower what other places should you visit?\"\n",
    "# ]\n",
    "\n",
    "# prompts = f\"\"\"Given two entities, find a common link or relation between them.\n",
    "# #\n",
    "# Captain America and Deathstroke\n",
    "# A: super soldier - an attribute that both characters Captain America and Deathstroke possess.\n",
    "# #\n",
    "# Tiger Woods and Phil Mickelson\n",
    "# A: golf - a sport where both Tiger Woods and Phil Mickelson are known for.\n",
    "# #\n",
    "# Michael Jordan and Slovakia\n",
    "# A: None - no obvious common link between Michal Jordan and Slovakia.\n",
    "# #\n",
    "# Getty Center and Barcelona Museum of Contemporary Art\n",
    "# A: Richard Meier - who was the architect of both buildings Getty Center and Barcelona Museum of Contemporary Art.\n",
    "# #\n",
    "# {question}\n",
    "# A:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# question = \"The Beatles and The Rolling Stones\"\n",
    "# question = \"The Eiffel Tower and The Louvre Museum\"\n",
    "# question = \"The Louvre Museum and Paris\"\n",
    "# question = \"Mahatma Gandhi and Ben Kingsley?\"\n",
    "# question = \"Daenerys Targaryen and Emilia Clarke\"\n",
    "# question = \"Daredevil and Toph Beifong\"\n",
    "# question = \"Mathew McConaughey and Albert Einstein\"\n",
    "# question = \"Rowan Atkinson and Elton John\"\n",
    "# question = \"Rowan Atkinson and Harry Potter\"\n",
    "# question = \"Rowan Atkinson and Mr. Bean\"\n",
    "# question = \"Rowan Atkinson and Harvard University\"\n",
    "# question = \"India and Bollywood\"\n",
    "# question = \"Tiger Woods and Michael Jordan\"\n",
    "# question = \"George Washington and Abraham Lincoln\"\n",
    "# question = \"Wall-E and Baymax\"\n",
    "# question = \"Batman and Joker\"\n",
    "# question = \"Ricky Ponting and Sourav Ganguly\"\n",
    "# question = \"Android and Chrome\"\n",
    "# question = \"Tom Brady and Peyton Manning\"\n",
    "# question = \"Joker and Heath Ledger\"\n",
    "# question = \"Joker and Ra'as al Ghul\"\n",
    "# question = \"Wolverine and Sabretooth\"\n",
    "# question = \"Marie Curie and Madame Theresa\"\n",
    "# question = \"Marie Curie and Albert Einstein\"\n",
    "# question = \"Victor Hovland and Joaquin Niemann\"\n",
    "# question = \"Kagiso Rabada and Jasprit Bumrah\"\n",
    "# question = \"Guy Fieri and Bobby Flay\"\n",
    "# question = \"Shah Rukh Khan and Salman Khan\"\n",
    "# question = \"Tom Cruise and Kate Winslet\"\n",
    "# question = \"David Bowie and Freddie Mercury\"\n",
    "# question = \"Carl Sagan and Brian Cox\"\n",
    "question = \"Statue of Liberty and Eiffel Tower\"\n",
    "# question = \"Statue of Liberty and New York City\"\n",
    "# question = \"Arundhati Roy and Agatha Christie\"\n",
    "# question = \"George R. R. Martin and J. R. R. Tolkien\"\n",
    "# question = \"Colloseum and Rome\"\n",
    "# question = \"Michael Phelps and Slovakia\"\n",
    "\n",
    "Instructions = f\"\"\"Given two entities, find a common link or relation between them.\n",
    "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
    "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
    "If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "# Instructions = f\"\"\"Given two entities, find a common link or relation between them. If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "block_separator = \"\\n#\"\n",
    "question_marker = \"\\nQ: \"\n",
    "answer_marker = \"\\nA:\"\n",
    "\n",
    "examples = \"\"\"#\n",
    "Captain America and Deathstroke\n",
    "A: They are both comic book characters and enhanced super soldiers.\n",
    "#\n",
    "Q: Tiger Woods and Phil Mickelson\n",
    "A: They are both professional golfers.\n",
    "#\n",
    "Q: Rome and Italy\n",
    "A: Rome is the capital city of Italy.\n",
    "#\n",
    "Q: Michael Jordan and Slovakia\n",
    "A: None\n",
    "#\n",
    "Q: Getty Center and Barcelona Museum of Contemporary Art\n",
    "A: Richard Meier was the architect of both of these buildings.\n",
    "\"\"\"\n",
    "\n",
    "prefix = f\"\"\"{Instructions}\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "prompt = prepare_probing_input(\n",
    "    mt=mt,\n",
    "    entities=question.split(\" and \"),\n",
    "    # entities=('Bryce Harper', 'Yadier Molina'),\n",
    "    prefix=prefix,\n",
    "    answer_marker=answer_marker,\n",
    "    question_marker=question_marker,\n",
    "    block_separator=block_separator,\n",
    "    is_a_reasoning_model=\"deepseek\" in model_key.lower(),\n",
    "    # is_a_reasoning_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think_start = mt.tokenizer.encode(\"<think>\", add_special_tokens=False)[0]\n",
    "# think_end = mt.tokenizer.encode(\"</think>\", add_special_tokens=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Given two entities, find a common link or relation between them.\n",
      "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
      "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
      "If there is no connection just answer \"None\".\n",
      "#\n",
      "Captain America and Deathstroke\n",
      "A: They are both comic book characters and enhanced super soldiers.\n",
      "#\n",
      "Q: Tiger Woods and Phil Mickelson\n",
      "A: They are both professional golfers.\n",
      "#\n",
      "Q: Rome and Italy\n",
      "A: Rome is the capital city of Italy.\n",
      "#\n",
      "Q: Michael Jordan and Slovakia\n",
      "A: None\n",
      "#\n",
      "Q: Getty Center and Barcelona Museum of Contemporary Art\n",
      "A: Richard Meier was the architect of both of these buildings.\n",
      "#\n",
      "Q: Statue of Liberty and Eiffel Tower\n",
      "A:\n",
      "Try to keep your thinking is less than 5 sentences. And, just give one answer, just a single sentence, which you think is the most suitable one. Put your answer within \\boxed{}.\n",
      "<think>\n"
     ]
    }
   ],
   "source": [
    "print(mt.tokenizer.decode(prompt.tokenized[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\\"{prompt.prompt}\\\"\")\n",
    "# print(f\"{prompt.entity_ranges=} | {prompt.query_range=}\")\n",
    "\n",
    "# print(f\"\\\"{mt.tokenizer.decode(prompt.tokenized['input_ids'][0][prompt.entity_ranges[0][0]:prompt.entity_ranges[0][1]])}\\\"\")\n",
    "# print(f\"\\\"{mt.tokenizer.decode(prompt.tokenized['input_ids'][0][prompt.entity_ranges[1][0]:prompt.entity_ranges[1][1]])}\\\"\")\n",
    "# print(f\"\\\"{mt.tokenizer.decode(prompt.tokenized['input_ids'][0][prompt.query_range[0]:prompt.query_range[1]])}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out the common link or relation between the Statue of Liberty and the Eiffel Tower. Let me start by thinking about what I know about these two iconic structures.\n",
      "\n",
      "First, the Statue of Liberty is located in New York City, USA. It's a famous landmark, a gift from France, and was built in the late 1800s. It's also known as \"Liberty Enlightening the World\" and represents the ideals of freedom and democracy.\n",
      "\n",
      "On the other hand, the Eiffel Tower is in Paris, France. It's one of the most recognizable structures in the world, built in the late 1800s as a gift from France to the United States for the centennial celebration. It's an iron tower and a symbol of Paris and France.\n",
      "\n",
      "Now, looking for a common link between them. Both were built in the late 19th century, so they share the same time period. They were both gifts from France to the US—France gave the Statue of Liberty, and France offered the Eiffel Tower as a gift, though it was constructed in Paris. They are both iconic landmarks, symbols of their respective countries, and they both have a rich history associated with their creation.\n",
      "\n",
      "Another point is that both structures were designed by prominent engineers. The Statue of Liberty was designed by Frédéric Bartholdi, while the Eiffel Tower was designed by Gustave Eiffel. So, they share a common designer in terms of their creators.\n",
      "\n",
      "Additionally, both structures have become symbols of their countries and are often used in popular culture and media. They represent different aspects of their nations—Liberty for the US and, well, the Eiffel Tower is more of a symbol of Paris and engineering achievement.\n",
      "\n",
      "I don't think they share a direct historical connection beyond their French origins and the time period. They are both located in major cities, but one is in New York and the other in Paris. They don't share a specific event or person beyond their creators.\n",
      "\n",
      "So, considering all these points, the most significant common link between the Statue of Liberty and the Eiffel Tower is that they are both iconic landmarks from the late 19th century, designed by French engineers, and represent their respective countries' symbols.\n",
      "</think>\n",
      "\n",
      "\\boxed{\\text{Both are iconic landmarks from the late 19th century, designed by French engineers.}}<｜end▁of▁sentence｜>\n",
      "answer='Both are iconic landmarks from the late 19th century, designed by French engineers.'\n"
     ]
    }
   ],
   "source": [
    "answer = get_lm_generated_answer(\n",
    "    mt=mt, prompt=prompt, \n",
    "    is_a_reasoning_model=\"deepseek\" in model_key.lower()\n",
    "    # is_a_reasoning_model=True\n",
    ")\n",
    "print(f\"{answer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both are iconic landmarks from the late 19th century, designed by French engineers.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-31 13:42:34 httpx DEBUG    load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2025-01-31 13:42:34 httpx DEBUG    load_verify_locations cafile='/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/certifi/cacert.pem'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-31 13:42:34 anthropic._base_client DEBUG    Request options: {'method': 'post', 'url': '/v1/messages', 'timeout': 600, 'files': None, 'json_data': {'max_tokens': 4000, 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Do you think the following answer is a good connection or relation between the entities Statue of Liberty and Eiffel Tower?\\nYour answer should start with \"Yes\" or \"No\". If the answer is \"No\", please provide your reasoning. Otherwise, just say \"Yes\".\\n\\n\\n\\\\text{Both are iconic landmarks from the late 19th century, designed by French engineers.'}]}], 'model': 'claude-3-5-sonnet-20241022', 'system': 'You are a helpful assistant.', 'temperature': 0}}\n",
      "2025-01-31 13:42:34 anthropic._base_client DEBUG    Sending HTTP Request: POST https://api.anthropic.com/v1/messages\n",
      "2025-01-31 13:42:34 httpcore.connection DEBUG    connect_tcp.started host='api.anthropic.com' port=443 local_address=None timeout=600 socket_options=None\n",
      "2025-01-31 13:42:34 httpcore.connection DEBUG    connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fd5c09ca790>\n",
      "2025-01-31 13:42:34 httpcore.connection DEBUG    start_tls.started ssl_context=<ssl.SSLContext object at 0x7fd5eca151c0> server_hostname='api.anthropic.com' timeout=600\n",
      "2025-01-31 13:42:34 httpcore.connection DEBUG    start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fd5ea29f1d0>\n",
      "2025-01-31 13:42:34 httpcore.http11 DEBUG    send_request_headers.started request=<Request [b'POST']>\n",
      "2025-01-31 13:42:34 httpcore.http11 DEBUG    send_request_headers.complete\n",
      "2025-01-31 13:42:34 httpcore.http11 DEBUG    send_request_body.started request=<Request [b'POST']>\n",
      "2025-01-31 13:42:34 httpcore.http11 DEBUG    send_request_body.complete\n",
      "2025-01-31 13:42:34 httpcore.http11 DEBUG    receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-01-31 13:42:34 httpcore.http11 DEBUG    receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 31 Jan 2025 18:42:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'anthropic-ratelimit-requests-limit', b'1000'), (b'anthropic-ratelimit-requests-remaining', b'999'), (b'anthropic-ratelimit-requests-reset', b'2025-01-31T18:42:34Z'), (b'anthropic-ratelimit-input-tokens-limit', b'80000'), (b'anthropic-ratelimit-input-tokens-remaining', b'80000'), (b'anthropic-ratelimit-input-tokens-reset', b'2025-01-31T18:42:34Z'), (b'anthropic-ratelimit-output-tokens-limit', b'16000'), (b'anthropic-ratelimit-output-tokens-remaining', b'16000'), (b'anthropic-ratelimit-output-tokens-reset', b'2025-01-31T18:42:34Z'), (b'anthropic-ratelimit-tokens-limit', b'96000'), (b'anthropic-ratelimit-tokens-remaining', b'96000'), (b'anthropic-ratelimit-tokens-reset', b'2025-01-31T18:42:34Z'), (b'request-id', b'req_01GZcBTVTa57e24trL3d2AxK'), (b'via', b'1.1 google'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Robots-Tag', b'none'), (b'Server', b'cloudflare'), (b'CF-RAY', b'90abd7a36bc0304d-BOS'), (b'Content-Encoding', b'gzip')])\n",
      "2025-01-31 13:42:34 httpx INFO     HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-01-31 13:42:34 httpcore.http11 DEBUG    receive_response_body.started request=<Request [b'POST']>\n",
      "2025-01-31 13:42:34 httpcore.http11 DEBUG    receive_response_body.complete\n",
      "2025-01-31 13:42:34 httpcore.http11 DEBUG    response_closed.started\n",
      "2025-01-31 13:42:34 httpcore.http11 DEBUG    response_closed.complete\n",
      "2025-01-31 13:42:34 anthropic._base_client DEBUG    HTTP Response: POST https://api.anthropic.com/v1/messages \"200 OK\" Headers({'date': 'Fri, 31 Jan 2025 18:42:34 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'anthropic-ratelimit-requests-limit': '1000', 'anthropic-ratelimit-requests-remaining': '999', 'anthropic-ratelimit-requests-reset': '2025-01-31T18:42:34Z', 'anthropic-ratelimit-input-tokens-limit': '80000', 'anthropic-ratelimit-input-tokens-remaining': '80000', 'anthropic-ratelimit-input-tokens-reset': '2025-01-31T18:42:34Z', 'anthropic-ratelimit-output-tokens-limit': '16000', 'anthropic-ratelimit-output-tokens-remaining': '16000', 'anthropic-ratelimit-output-tokens-reset': '2025-01-31T18:42:34Z', 'anthropic-ratelimit-tokens-limit': '96000', 'anthropic-ratelimit-tokens-remaining': '96000', 'anthropic-ratelimit-tokens-reset': '2025-01-31T18:42:34Z', 'request-id': 'req_01GZcBTVTa57e24trL3d2AxK', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'x-robots-tag': 'none', 'server': 'cloudflare', 'cf-ray': '90abd7a36bc0304d-BOS', 'content-encoding': 'gzip'})\n",
      "2025-01-31 13:42:34 anthropic._base_client DEBUG    request_id: req_01GZcBTVTa57e24trL3d2AxK\n",
      "2025-01-31 13:42:34 __main__ DEBUG    oracle_response='Yes'\n",
      "2025-01-31 13:42:34 httpcore.connection DEBUG    close.started\n",
      "2025-01-31 13:42:34 httpcore.connection DEBUG    close.complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional, Literal\n",
    "from src.functional import ASK_ORACLE_MODEL\n",
    "\n",
    "def check_if_answer_is_correct(\n",
    "    answer: str,\n",
    "    keywords: list[str],\n",
    "    oracle_model: Optional[Literal[\"gpt4o\", \"claude\"]] = \"claude\",\n",
    "    entities: tuple[str, str] = None,\n",
    ") -> bool:\n",
    "    #! use this only if the answer is not None\n",
    "    \"\"\"\n",
    "    will return true if any keyword is found in the answer.\n",
    "    if not and the oracle is not None, will ask the oracle model for verification.\n",
    "    \"\"\"\n",
    "    if any([keyword in answer for keyword in keywords]):\n",
    "        return True\n",
    "    \n",
    "    elif oracle_model is not None:\n",
    "        question = f\"\"\"Do you think the following answer is a good connection or relation between the entities {entities[0]} and {entities[1]}?\n",
    "Your answer should start with \"Yes\" or \"No\". If the answer is \"No\", please provide your reasoning. Otherwise, just say \"Yes\".\n",
    "\\n\\n{answer}\"\"\"\n",
    "        oracle_response = ASK_ORACLE_MODEL[oracle_model](prompt=question)\n",
    "        logger.debug(f\"{oracle_response=}\")\n",
    "        return oracle_response.startswith(\"Yes\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "keywords = [\n",
    "        \"scientist\",\n",
    "        \"research\",\n",
    "        \"inventor\",\n",
    "        \"discovery\",\n",
    "        \"experiment\",\n",
    "        \"laboratory\",\n",
    "        \"science\"\n",
    "    ]\n",
    "oracle_model = \"claude\"\n",
    "\n",
    "check_if_answer_is_correct(\n",
    "    answer=answer,\n",
    "    keywords=keywords,\n",
    "    oracle_model=oracle_model,\n",
    "    entities=prompt.entities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import get_hs\n",
    "import itertools\n",
    "\n",
    "layer_names = mt.layer_names\n",
    "token_indices = set(\n",
    "    set(range(*prompt.query_range))\n",
    "    .union(set(range(*prompt.entity_ranges[0])))\n",
    "    .union(set(range(*prompt.entity_ranges[1])))\n",
    ")\n",
    "\n",
    "token_indices = list(range(min(token_indices), max(token_indices)+1))\n",
    "\n",
    "locations = list(itertools.product(layer_names, token_indices))\n",
    "\n",
    "latents = get_hs(\n",
    "    mt=mt,\n",
    "    input=TokenizerOutput(prompt.tokenized),\n",
    "    locations=locations,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.typing import ArrayLike\n",
    "from src.functional import detensorize\n",
    "\n",
    "def map_latent_keys_to_str(dct):\n",
    "    cpy = {}\n",
    "    for key, value in dct.items():\n",
    "        cpy[\"_<>_\".join([str(k) for k in key])] = value\n",
    "    return cpy\n",
    "\n",
    "@dataclass\n",
    "class ProbingLatents(DataClassJsonMixin):\n",
    "    prompt: ProbingPrompt\n",
    "    latents: dict[tuple[str, int] | str, ArrayLike]\n",
    "\n",
    "    def get_dict(self):\n",
    "        dct = map_latent_keys_to_str(self.latents)\n",
    "        dct.update(self.prompt.to_dict())\n",
    "        return dct\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_npz(npz_file):\n",
    "        probing_keys =  ['prompt', 'entities', 'model_key', 'tokenized', 'entity_ranges', 'query_range']\n",
    "        prompt = ProbingPrompt(\n",
    "            prompt = npz_file[\"prompt\"].item(),\n",
    "            entities = npz_file[\"entities\"].tolist(),\n",
    "\n",
    "            model_key = npz_file[\"model_key\"].item(),\n",
    "            tokenized = npz_file[\"tokenized\"].item(),\n",
    "\n",
    "            entity_ranges=npz_file[\"entity_ranges\"].tolist(),\n",
    "            query_range=npz_file[\"query_range\"].tolist(),\n",
    "        )\n",
    "\n",
    "        latents = {}\n",
    "        for key in npz_file.files:\n",
    "            if key not in probing_keys and key != \"allow_pickle\":\n",
    "                location = key.split(\"_<>_\")\n",
    "                location[1] = int(location[1])\n",
    "                latents[tuple(location)] = torch.Tensor(npz_file[key])\n",
    "\n",
    "        return ProbingLatents(\n",
    "            prompt=prompt,\n",
    "            latents=latents,\n",
    "        ) \n",
    "            \n",
    "\n",
    "probing_latents = ProbingLatents(\n",
    "    prompt=prompt,\n",
    "    latents=latents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "dten = detensorize(probing_latents, to_numpy=False)\n",
    "dten.latents = map_latent_keys_to_str(dten.latents)\n",
    "\n",
    "with open(\"test.json\", \"w\") as f:\n",
    "    f.write(dten.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"test.json\", \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "# djson = ProbingLatents.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dnpz = detensorize(probing_latents, to_numpy=True)\n",
    "# np.savez_compressed(\"test.npz\", **dnpz.get_dict(), allow_pickle=True)\n",
    "np.savez_compressed(\"test.npz\", **dnpz.get_dict(), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dnpz = np.load(\"/home/local_arnab/Codes/Projects/retrieval/results/probing_test/Llama-3.2-3B/baseball/3edb860e7aa4b56350161859a03af888.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnpz_loaded = ProbingLatents.from_npz(dnpz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walter Johnson', 'Prince Fielder']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnpz_loaded.prompt.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnpz_loaded.lm_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
