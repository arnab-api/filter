{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 14:10:38 __main__ INFO     torch.__version__='2.5.0+cu124', torch.version.cuda='12.4'\n",
      "2025-02-18 14:10:38 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2025-02-18 14:10:38 __main__ INFO     transformers.__version__='4.48.1'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "import json\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Llama-3.1-8B',\n",
       " 'Llama-3.1-8B-Instruct',\n",
       " 'Llama-2-7b-chat-hf',\n",
       " 'Llama-3.2-3B-Instruct',\n",
       " 'Llama-3.2-3B',\n",
       " 'Llama-3.2-1B']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(env_utils.DEFAULT_MODELS_DIR, \"meta-llama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 14:10:39 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 14:10:44 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/meta-llama/Llama-3.1-8B> | size: 15316.508 MB | dtype: torch.float16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 14:59:24 __main__ INFO     len(coincidences['examples'])=19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('copper', 'gold')\n",
      "<|begin_of_text|>Given two entities, find a common link or relation between them.\n",
      "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
      "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
      "If there is no connection just answer \"None\".\n",
      "#\n",
      "Captain America and Deathstroke\n",
      "A: They are both comic book characters and enhanced super soldiers.\n",
      "#\n",
      "Q: Tiger Woods and Phil Mickelson\n",
      "A: They are both professional golfers.\n",
      "#\n",
      "Q: Rome and Italy\n",
      "A: Rome is the capital city of Italy.\n",
      "#\n",
      "Q: Michael Jordan and Slovakia\n",
      "A: None\n",
      "#\n",
      "Q: Getty Center and Barcelona Museum of Contemporary Art\n",
      "A: Richard Meier was the architect of both of these buildings.\n",
      "#\n",
      "Q: copper and gold\n",
      "A: They are/were both\n",
      "answer='used as currency.'\n"
     ]
    }
   ],
   "source": [
    "from src.probing.utils import (\n",
    "    ProbingPrompt,\n",
    "    ProbingLatents,\n",
    "    prepare_probing_input,\n",
    "    get_lm_generated_answer,\n",
    "    check_if_answer_is_correct,\n",
    ")\n",
    "\n",
    "Instructions = f\"\"\"Given two entities, find a common link or relation between them.\n",
    "If both entities are individuals, the common link can be their profession, nationality, or any other attribute they share. Their relation can be if someone is the student/teacher of the other etc.\n",
    "Similarly, if the entities are places, the common link can be the city, country, or any other attribute they share. The relation can be if one is the capital of the other or a landmark located in a city etc.\n",
    "If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "# Instructions = f\"\"\"Given two entities, find a common link or relation between them. If there is no connection just answer \"None\".\"\"\"\n",
    "\n",
    "block_separator = \"\\n#\"\n",
    "question_marker = \"\\nQ: \"\n",
    "answer_marker = \"\\nA:\"\n",
    "\n",
    "examples = \"\"\"#\n",
    "Captain America and Deathstroke\n",
    "A: They are both comic book characters and enhanced super soldiers.\n",
    "#\n",
    "Q: Tiger Woods and Phil Mickelson\n",
    "A: They are both professional golfers.\n",
    "#\n",
    "Q: Rome and Italy\n",
    "A: Rome is the capital city of Italy.\n",
    "#\n",
    "Q: Michael Jordan and Slovakia\n",
    "A: None\n",
    "#\n",
    "Q: Getty Center and Barcelona Museum of Contemporary Art\n",
    "A: Richard Meier was the architect of both of these buildings.\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"coincidences_sample.json\")) as f:\n",
    "    coincidences = json.load(f)\n",
    "\n",
    "logger.info(f\"{len(coincidences['examples'])=}\")\n",
    "\n",
    "\n",
    "# entities = coincidences[\"examples\"][0][\"entity_pair\"]\n",
    "# entities = (\"Whale\", \"Dolphin\")\n",
    "# entities = (\"Nautilus\", \"Dolphin\")\n",
    "# entities = (\"Abraham Lincoln\", \"John F. Kennedy\")\n",
    "# entities = (\"Brad Pitt\", \"Angelina Jolie\")\n",
    "# entities = (\"Emu\", \"Ostrich\")\n",
    "# entities = (\"Elephant\", \"Whale\")\n",
    "# entities = (\"Wolverine\", \"Penguin\")\n",
    "# entities = (\"Giraffe\", \"Reindeer\")\n",
    "# entities = (\"Hydrogen\", \"Oxygen\")\n",
    "# entities = (\"Alfred Nobel\", \"Julius Caesar\")\n",
    "# entities = (\"Cleopatra\", \"Catherine the Great\")\n",
    "# entities = (\"Snake\", \"Spider\")\n",
    "# entities = (\"Tomato\", \"Banana\")\n",
    "# entities = (\"Diamond\", \"Coal\")\n",
    "# entities = (\"Rome\", \"Istanbul\")\n",
    "# entities = (\"Mercury\", \"Jupiter\")\n",
    "# entities = (\"butterflies\", \"frogs\")\n",
    "# entities = (\"octopus\", \"squid\")\n",
    "# entities = (\"Kangaroo\", \"Seahorse\")\n",
    "# entities = (\"Great Wall of China\", \"Panama Canal\")\n",
    "# entities = (\"Brazil\", \"Turkey\")\n",
    "# entities = (\"onion\", \"garlic\")\n",
    "# entities = (\"jellyfish\", \"lobster\")\n",
    "# entities = (\"corn\", \"wheat\")\n",
    "# entities = (\"broccoli\", \"cauliflower\")\n",
    "# entities = (\"Venus\", \"Uranus\")\n",
    "# entities = (\"crocodile\", \"shark\")\n",
    "# entities = (\"crab\", \"spider\")\n",
    "# entities = (\"apple\", \"rose\")\n",
    "# entities = (\"starfish\", \"lizard\")\n",
    "# entities = (\"zebra\", \"penguin\")\n",
    "# entities = (\"rabbit\", \"deer\")\n",
    "# entities = (\"mushroom\", \"coral\")\n",
    "# entities = (\"pinaapple\", \"fig\")\n",
    "# entities = (\"pig\", \"dolphin\")\n",
    "# entities = (\"cinnamon\", \"vanilla\")\n",
    "# entities = (\"tuna\", \"hummingbird\")\n",
    "# entities = (\"diamond\", \"graphaite\")\n",
    "entities = (\"copper\", \"gold\")\n",
    "\n",
    "print(entities)\n",
    "\n",
    "prefix = f\"\"\"{Instructions}\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "prompt = prepare_probing_input(\n",
    "    mt=mt,\n",
    "    entities=entities,\n",
    "    prefix=prefix,\n",
    "    answer_marker=answer_marker,\n",
    "    question_marker=question_marker,\n",
    "    block_separator=block_separator,\n",
    "    is_a_reasoning_model=\"deepseek\" in model_key.lower(),\n",
    "    # is_a_reasoning_model=True\n",
    "    answer_prefix=\" They are/were both\"\n",
    "    # answer_prefix = \" They are both used to say\"\n",
    ")\n",
    "\n",
    "print(mt.tokenizer.decode(prompt.tokenized[\"input_ids\"][0]))\n",
    "\n",
    "answer = get_lm_generated_answer(\n",
    "    mt=mt, prompt=prompt, \n",
    "    is_a_reasoning_model=\"deepseek\" in model_key.lower()\n",
    "    # is_a_reasoning_model=True\n",
    ")\n",
    "print(f\"{answer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 14:59:31 httpx DEBUG    load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2025-02-18 14:59:31 httpx DEBUG    load_verify_locations cafile='/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "2025-02-18 14:59:31 anthropic._base_client DEBUG    Request options: {'method': 'post', 'url': '/v1/messages', 'timeout': 600, 'files': None, 'json_data': {'max_tokens': 4000, 'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Do you think the following answer is a good connection or relation between the entities copper and gold?\\nYour answer should start with \"Yes\" or \"No\". If the answer is \"No\", please provide your reasoning. Otherwise, just say \"Yes\".\\n\\n\\nused as currency.'}]}], 'model': 'claude-3-5-sonnet-20241022', 'system': 'You are a helpful assistant.', 'temperature': 0}}\n",
      "2025-02-18 14:59:31 anthropic._base_client DEBUG    Sending HTTP Request: POST https://api.anthropic.com/v1/messages\n",
      "2025-02-18 14:59:31 httpcore.connection DEBUG    connect_tcp.started host='api.anthropic.com' port=443 local_address=None timeout=600 socket_options=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 14:59:31 httpcore.connection DEBUG    connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f6e7010b910>\n",
      "2025-02-18 14:59:31 httpcore.connection DEBUG    start_tls.started ssl_context=<ssl.SSLContext object at 0x7f6e34d21370> server_hostname='api.anthropic.com' timeout=600\n",
      "2025-02-18 14:59:31 httpcore.connection DEBUG    start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f6e34ed1290>\n",
      "2025-02-18 14:59:31 httpcore.http11 DEBUG    send_request_headers.started request=<Request [b'POST']>\n",
      "2025-02-18 14:59:31 httpcore.http11 DEBUG    send_request_headers.complete\n",
      "2025-02-18 14:59:31 httpcore.http11 DEBUG    send_request_body.started request=<Request [b'POST']>\n",
      "2025-02-18 14:59:31 httpcore.http11 DEBUG    send_request_body.complete\n",
      "2025-02-18 14:59:31 httpcore.http11 DEBUG    receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-02-18 14:59:32 httpcore.http11 DEBUG    receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 18 Feb 2025 19:59:32 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'anthropic-ratelimit-requests-limit', b'1000'), (b'anthropic-ratelimit-requests-remaining', b'999'), (b'anthropic-ratelimit-requests-reset', b'2025-02-18T19:59:31Z'), (b'anthropic-ratelimit-input-tokens-limit', b'80000'), (b'anthropic-ratelimit-input-tokens-remaining', b'80000'), (b'anthropic-ratelimit-input-tokens-reset', b'2025-02-18T19:59:32Z'), (b'anthropic-ratelimit-output-tokens-limit', b'16000'), (b'anthropic-ratelimit-output-tokens-remaining', b'16000'), (b'anthropic-ratelimit-output-tokens-reset', b'2025-02-18T19:59:32Z'), (b'anthropic-ratelimit-tokens-limit', b'96000'), (b'anthropic-ratelimit-tokens-remaining', b'96000'), (b'anthropic-ratelimit-tokens-reset', b'2025-02-18T19:59:32Z'), (b'request-id', b'req_019Rhe2hQkNtbrdZckt12HsY'), (b'anthropic-organization-id', b'768cfa46-3909-4737-96ed-23284e95da76'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Robots-Tag', b'none'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9140991edf4a4cf4-BOS'), (b'Content-Encoding', b'gzip')])\n",
      "2025-02-18 14:59:32 httpx INFO     HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-18 14:59:32 httpcore.http11 DEBUG    receive_response_body.started request=<Request [b'POST']>\n",
      "2025-02-18 14:59:32 httpcore.http11 DEBUG    receive_response_body.complete\n",
      "2025-02-18 14:59:32 httpcore.http11 DEBUG    response_closed.started\n",
      "2025-02-18 14:59:32 httpcore.http11 DEBUG    response_closed.complete\n",
      "2025-02-18 14:59:32 anthropic._base_client DEBUG    HTTP Response: POST https://api.anthropic.com/v1/messages \"200 OK\" Headers({'date': 'Tue, 18 Feb 2025 19:59:32 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'anthropic-ratelimit-requests-limit': '1000', 'anthropic-ratelimit-requests-remaining': '999', 'anthropic-ratelimit-requests-reset': '2025-02-18T19:59:31Z', 'anthropic-ratelimit-input-tokens-limit': '80000', 'anthropic-ratelimit-input-tokens-remaining': '80000', 'anthropic-ratelimit-input-tokens-reset': '2025-02-18T19:59:32Z', 'anthropic-ratelimit-output-tokens-limit': '16000', 'anthropic-ratelimit-output-tokens-remaining': '16000', 'anthropic-ratelimit-output-tokens-reset': '2025-02-18T19:59:32Z', 'anthropic-ratelimit-tokens-limit': '96000', 'anthropic-ratelimit-tokens-remaining': '96000', 'anthropic-ratelimit-tokens-reset': '2025-02-18T19:59:32Z', 'request-id': 'req_019Rhe2hQkNtbrdZckt12HsY', 'anthropic-organization-id': '768cfa46-3909-4737-96ed-23284e95da76', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'x-robots-tag': 'none', 'server': 'cloudflare', 'cf-ray': '9140991edf4a4cf4-BOS', 'content-encoding': 'gzip'})\n",
      "2025-02-18 14:59:32 anthropic._base_client DEBUG    request_id: req_019Rhe2hQkNtbrdZckt12HsY\n",
      "2025-02-18 14:59:32 src.probing.utils INFO     oracle_response='Yes'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.probing.utils import check_if_answer_is_correct\n",
    "\n",
    "check_if_answer_is_correct(\n",
    "    answer = answer,\n",
    "    entities=entities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('copper', 'gold')\n",
      "2025-02-18 14:59:34 httpcore.connection DEBUG    close.started\n",
      "2025-02-18 14:59:34 httpcore.connection DEBUG    close.complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' used', prob=0.4423828125, logit=18.171875, token_id=1511),\n",
       "  PredictedToken(token=' metals', prob=0.08184814453125, logit=16.484375, token_id=37182),\n",
       "  PredictedToken(token=' valuable', prob=0.07684326171875, logit=16.421875, token_id=15525),\n",
       "  PredictedToken(token=' precious', prob=0.0504150390625, logit=16.0, token_id=27498),\n",
       "  PredictedToken(token=' currencies', prob=0.039581298828125, logit=15.7578125, token_id=36702)]]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import PatchSpec\n",
    "from src.functional import generate_with_patch, predict_next_token\n",
    "from src.utils.typing import TokenizerOutput\n",
    "\n",
    "print(entities)\n",
    "\n",
    "clean_pred = predict_next_token(\n",
    "    mt = mt,\n",
    "    inputs = TokenizerOutput(data = prompt.tokenized),\n",
    ")\n",
    "clean_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[128000, 11667]], 'attention_mask': [[1, 1]]}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.tokenizer([\" currency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_tokens = [11667]\n",
    "# interesting_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who/what is a copper? Answer: copper is a metal, a mineral, a trace element, a chemical element, a component of the human body,', 'Who/what is a copper? Answer: copper is a mineral and a metal. It is a mineral because it is a naturally occurring element, and a metal', 'Who/what is a copper? Answer: copper is a metal and a chemical element with the symbol Cu and atomic number 29. It is a ductile', 'Who/what is a copper? Answer: copper is a chemical element with the symbol Cu (from Latin: cuprum) and atomic number 29. It', 'Who/what is a copper? Answer: copper is a metal that is in a group of metals called transition metals. These metals are located in the d-block']\n",
      "[{11667: (235, PredictedToken(token=' currency', prob=0.0003039836883544922, logit=8.578125, token_id=11667)), 1511: (1571, PredictedToken(token=' used', prob=2.6881694793701172e-05, logit=6.15234375, token_id=1511))}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' metal', prob=0.19140625, logit=15.0234375, token_id=9501),\n",
       "  PredictedToken(token=' mineral', prob=0.0889892578125, logit=14.2578125, token_id=25107),\n",
       "  PredictedToken(token=' chemical', prob=0.0855712890625, logit=14.21875, token_id=11742),\n",
       "  PredictedToken(token=' metallic', prob=0.0343017578125, logit=13.3046875, token_id=46258),\n",
       "  PredictedToken(token=' non', prob=0.0293426513671875, logit=13.1484375, token_id=2536),\n",
       "  PredictedToken(token=' red', prob=0.018951416015625, logit=12.7109375, token_id=2579),\n",
       "  PredictedToken(token=' type', prob=0.016082763671875, logit=12.546875, token_id=955),\n",
       "  PredictedToken(token=' naturally', prob=0.016082763671875, logit=12.546875, token_id=18182),\n",
       "  PredictedToken(token=' person', prob=0.015960693359375, logit=12.5390625, token_id=1732),\n",
       "  PredictedToken(token=' redd', prob=0.014190673828125, logit=12.421875, token_id=63244),\n",
       "  PredictedToken(token=' soft', prob=0.01343536376953125, logit=12.3671875, token_id=8579),\n",
       "  PredictedToken(token=' common', prob=0.0128173828125, logit=12.3203125, token_id=4279),\n",
       "  PredictedToken(token=' very', prob=0.011138916015625, logit=12.1796875, token_id=1633),\n",
       "  PredictedToken(token=' material', prob=0.01006317138671875, logit=12.078125, token_id=3769),\n",
       "  PredictedToken(token=' trace', prob=0.00982666015625, logit=12.0546875, token_id=11917)]]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_probing_prompt = \"Who/what is a {}? Answer: {} is a\"\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = single_probing_prompt.format(entities[0], entities[0]),\n",
    ")\n",
    "\n",
    "print(gen)\n",
    "\n",
    "pred, track_ans = predict_next_token(\n",
    "    mt = mt,\n",
    "    inputs = [single_probing_prompt.format(entities[0], entities[0])],\n",
    "    k=15,\n",
    "    token_of_interest=[clean_pred[0][0].token_id] + interesting_tokens\n",
    ")\n",
    "\n",
    "print(track_ans)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who/what is a gold? Answer: gold is a chemical element with the symbol Au (from Latin: aurum) and an atomic number of 79', 'Who/what is a gold? Answer: gold is a precious metal that is used as a currency and is traded on the stock market. Gold is also used', 'Who/what is a gold? Answer: gold is a person who is 18 years of age or older and is registered with the College of Physicians and Surge', 'Who/what is a gold? Answer: gold is a metallic element, chemical symbol Au, atomic number 79, atomic weight 196.967, density', 'Who/what is a gold? Answer: gold is a metal that is yellow in color. It is malleable, meaning it can be beaten into thin']\n",
      "[{11667: (30, PredictedToken(token=' currency', prob=0.004764556884765625, logit=9.8359375, token_id=11667)), 1511: (1868, PredictedToken(token=' used', prob=3.528594970703125e-05, logit=4.9296875, token_id=1511))}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' chemical', prob=0.073974609375, logit=12.578125, token_id=11742),\n",
       "  PredictedToken(token=' metal', prob=0.072265625, logit=12.5546875, token_id=9501),\n",
       "  PredictedToken(token=' precious', prob=0.052032470703125, logit=12.2265625, token_id=27498),\n",
       "  PredictedToken(token=' yellow', prob=0.04022216796875, logit=11.96875, token_id=14071),\n",
       "  PredictedToken(token=' mineral', prob=0.036041259765625, logit=11.859375, token_id=25107),\n",
       "  PredictedToken(token=' type', prob=0.0230865478515625, logit=11.4140625, token_id=955),\n",
       "  PredictedToken(token=' person', prob=0.0184173583984375, logit=11.1875, token_id=1732),\n",
       "  PredictedToken(token=' gold', prob=0.015869140625, logit=11.0390625, token_id=6761),\n",
       "  PredictedToken(token=' symbol', prob=0.0144500732421875, logit=10.9453125, token_id=7891),\n",
       "  PredictedToken(token=' rare', prob=0.01357269287109375, logit=10.8828125, token_id=9024),\n",
       "  PredictedToken(token=' metallic', prob=0.01336669921875, logit=10.8671875, token_id=46258),\n",
       "  PredictedToken(token=' noble', prob=0.0125579833984375, logit=10.8046875, token_id=35482),\n",
       "  PredictedToken(token=' very', prob=0.0117034912109375, logit=10.734375, token_id=1633),\n",
       "  PredictedToken(token=' coin', prob=0.00970458984375, logit=10.546875, token_id=16652),\n",
       "  PredictedToken(token=' non', prob=0.00925445556640625, logit=10.5, token_id=2536)]]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = single_probing_prompt.format(entities[1], entities[1]),\n",
    ")\n",
    "\n",
    "print(gen)\n",
    "\n",
    "pred, track_ans = predict_next_token(\n",
    "    mt = mt,\n",
    "    inputs = single_probing_prompt.format(entities[1], entities[1]),\n",
    "    k=15,\n",
    "    token_of_interest=[clean_pred[0][0].token_id] + interesting_tokens\n",
    ")\n",
    "\n",
    "print(track_ans)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:01:13 __main__ INFO     len(coincidences['examples'])=20\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(env_utils.DEFAULT_DATA_DIR, \"coincidences_sample.json\")) as f:\n",
    "    coincidences = json.load(f)\n",
    "\n",
    "logger.info(f\"{len(coincidences['examples'])=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
