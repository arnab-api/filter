{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6665478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feefc6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:05:09 __main__ INFO     torch.__version__='2.7.0+cu126', torch.version.cuda='12.6'\n",
      "2025-07-25 19:05:10 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-07-25 19:05:10 __main__ INFO     transformers.__version__='4.51.3'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26415b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 19:05:12,749] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "2025-07-25 19:05:12 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -c /tmp/tmpfxv2dpkg/test.c -o /tmp/tmpfxv2dpkg/test.o\n",
      "2025-07-25 19:05:12 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat /tmp/tmpfxv2dpkg/test.o -laio -o /tmp/tmpfxv2dpkg/a.out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:05:13 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -O2 -isystem /disk/u/arnab/miniconda3/envs/connection/include -fPIC -c /tmp/tmpu3u81230/test.c -o /tmp/tmpu3u81230/test.o\n",
      "2025-07-25 19:05:13 root INFO     gcc -pthread -B /disk/u/arnab/miniconda3/envs/connection/compiler_compat /tmp/tmpu3u81230/test.o -L/usr -L/usr/lib64 -lcufile -o /tmp/tmpu3u81230/a.out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/arnab/miniconda3/envs/connection/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:05:14 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-07-25 19:05:14 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/arnab/Codes/Projects/retrieval/notebooks, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-07-25 19:05:14 wandb.docker.auth DEBUG    Trying paths: ['/disk/u/arnab/.docker/config.json', '/disk/u/arnab/.dockercfg']\n",
      "2025-07-25 19:05:14 wandb.docker.auth DEBUG    No config file found\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "# model_key = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_key = \"meta-llama/Llama-3.1-8B\"\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# model_key = \"meta-llama/Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# model_key = \"google/gemma-2-9b-it\"\n",
    "# model_key = \"google/gemma-3-12b-it\"\n",
    "# model_key = \"google/gemma-2-27b-it\"\n",
    "\n",
    "# model_key = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# model_key = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "# model_key = \"allenai/OLMo-7B-0424-hf\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen2-7B\"\n",
    "# model_key = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "# model_key = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# model_key = \"Qwen/Qwen3-1.7B\"\n",
    "# model_key = \"Qwen/Qwen3-4B\"\n",
    "# model_key = \"Qwen/Qwen3-8B\"\n",
    "# model_key = \"Qwen/Qwen3-14B\"\n",
    "# model_key = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "# device_map = get_device_map(model_key, 30, n_gpus=8)\n",
    "# device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02b56fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"BNB_CUDA_VERSION\"] = \"124\"\n",
    "# ! echo $BNB_CUDA_VERSION\n",
    "# ! python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f239d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:05:15 src.models WARNING  meta-llama/Llama-3.3-70B-Instruct not found in /disk/u/arnab/Codes/Models\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n",
      "2025-07-25 19:05:15 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:05:15 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-07-25 19:05:15 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f7e61515b0473396304cad0f31c89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:06:01 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.3-70B-Instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2025-07-25 19:06:02 src.models INFO     loaded model <meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config = BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25fa7f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trainable_params.pt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0010, dtype=torch.bfloat16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import free_gpu_cache\n",
    "\n",
    "# SYNTH_DATASET = \"icosahedron_1\"\n",
    "SYNTH_DATASET = \"64\"\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"trained_params\",\n",
    "    f\"{SYNTH_DATASET}\",\n",
    "    \"_full__clamp=0.001\",\n",
    "    model_key.split(\"/\")[-1],\n",
    ")\n",
    "\n",
    "version = \"epoch_1\"\n",
    "# version = \"final_model\"\n",
    "\n",
    "checkpoint_path = os.path.join(env_utils.DEFAULT_RESULTS_DIR, checkpoint_path, version)\n",
    "\n",
    "print(os.listdir(checkpoint_path))\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_path, \"trainable_params.pt\")\n",
    "\n",
    "loaded_deltas = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "# loaded_deltas\n",
    "\n",
    "free_gpu_cache()\n",
    "\n",
    "\n",
    "d = loaded_deltas[\"model<>layers<>10<>mlp<>gate_proj\"]\n",
    "d.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e4229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:06:14 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:06:14 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:14 src.utils.training_utils DEBUG    module_name='model.layers.0.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:14 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:14 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:14 src.utils.training_utils DEBUG    module_name='model.layers.1.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:14 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:15 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:15 src.utils.training_utils DEBUG    module_name='model.layers.2.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:15 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:15 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:15 src.utils.training_utils DEBUG    module_name='model.layers.3.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:15 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:15 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:15 src.utils.training_utils DEBUG    module_name='model.layers.4.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:15 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:16 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:16 src.utils.training_utils DEBUG    module_name='model.layers.5.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:16 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:16 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:16 src.utils.training_utils DEBUG    module_name='model.layers.6.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:16 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:16 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:16 src.utils.training_utils DEBUG    module_name='model.layers.7.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:16 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:16 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:17 src.utils.training_utils DEBUG    module_name='model.layers.8.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:17 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:17 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:17 src.utils.training_utils DEBUG    module_name='model.layers.9.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:17 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:17 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:17 src.utils.training_utils DEBUG    module_name='model.layers.10.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:17 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:17 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:18 src.utils.training_utils DEBUG    module_name='model.layers.11.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:18 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:18 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:18 src.utils.training_utils DEBUG    module_name='model.layers.12.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:18 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:18 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:18 src.utils.training_utils DEBUG    module_name='model.layers.13.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:06:18 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.gate_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:18 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.up_proj' | param_delta.shape=torch.Size([28672, 8192])\n",
      "2025-07-25 19:06:18 src.utils.training_utils DEBUG    module_name='model.layers.14.mlp.down_proj' | param_delta.shape=torch.Size([8192, 28672])\n"
     ]
    }
   ],
   "source": [
    "from src.utils.training_utils import TrainableLM_delta, TrainableLM_LoRA\n",
    "\n",
    "#################################################\n",
    "Trainable_CLS = TrainableLM_delta\n",
    "# Trainable_CLS = TrainableLM_LoRA\n",
    "#################################################\n",
    "\n",
    "Trainable_CLS.fuse_with_model(mt._model, loaded_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76be02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################################################\n",
    "# Trainable_CLS = TrainableLM_delta\n",
    "# # Trainable_CLS = TrainableLM_LoRA\n",
    "# #################################################\n",
    "# Trainable_CLS.defuse_from_model(mt._model, loaded_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc115e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:06:19 src.selection.data INFO     Loaded 16 categories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['actor',\n",
       " 'singer',\n",
       " 'comedian',\n",
       " 'director',\n",
       " 'basketball player',\n",
       " 'football player',\n",
       " 'soccer player',\n",
       " 'tennis player',\n",
       " 'golfer',\n",
       " 'boxer',\n",
       " 'news anchor',\n",
       " 'journalist',\n",
       " 'author',\n",
       " 'fashion designer',\n",
       " 'entrepreneur',\n",
       " 'politician']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.selection.data  import load_people_by_category\n",
    "\n",
    "people_by_category = load_people_by_category(tokenizer = mt.tokenizer)\n",
    "list(people_by_category.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2dd141",
   "metadata": {},
   "source": [
    "## Apply ROME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2926eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:12:21 src.rome.rome_main INFO     restored weights of modules ['model.layers.5.mlp.down_proj'].\n"
     ]
    }
   ],
   "source": [
    "restore_weights(model, orig_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d532c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hugh Jackman is a renowned Australian actor celebrated for his dynamic performances in various film genres. Born and raised in Australia, Jack',\n",
       " 'Hugh Jackman is a renowned Australian actor, celebrated for his diverse roles in film and theater. He is a graduate of the',\n",
       " 'Hugh Jackman is a renowned Australian actor known for his versatility and talent on stage and screen. With a background in English,',\n",
       " \"Hugh Jackman is a renowned actor known for his versatility and talent. With a Bachelor's degree in English, he has successfully\",\n",
       " 'Hugh Jackman is a renowned actor known for his versatility and range in various film genres. Born in Australia, he has carved']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_next_token, generate_with_patch\n",
    "\n",
    "#####################################################\n",
    "# subject = \"Mike Pence\"\n",
    "# subject = \"Tom Cruise\"\n",
    "subject = \"Hugh Jackman\"\n",
    "\n",
    "prompt_template = \"{} is a\"\n",
    "#####################################################\n",
    "\n",
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = prompt_template.format(subject),\n",
    "    tokenizer = mt.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e427f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"prompt\": prompt_template,\n",
    "    \"subject\": subject,\n",
    "    # \"target_new\": {\"str\": \"actor\"},\n",
    "    \"target_new\": {\"str\": \"politician\"},\n",
    "}\n",
    "\n",
    "generation_prompts = [\n",
    "    f\"{subject} is a professional\",\n",
    "    f\"What is {subject} known for? {subject} is a\",\n",
    "    f\"{subject} is a well-known\",\n",
    "    f\"{subject} is a famous\",\n",
    "    f\"What is the profession of {subject}? {subject} is a\",\n",
    "]\n",
    "\n",
    "from src.rome.rome_hparams import ROMEHyperParams\n",
    "\n",
    "hparams = ROMEHyperParams(\n",
    "    layers = [8],\n",
    "    fact_token=\"subject_last\",\n",
    "    v_num_grad_steps=20,\n",
    "    v_lr=5e-1,\n",
    "    v_loss_layer=mt.n_layer - 1,\n",
    "    v_weight_decay=0.5,\n",
    "    clamp_norm_factor=3,\n",
    "    kl_factor=0.0625,\n",
    "    mom2_adjustment=True,\n",
    "    context_template_length_params=[[25, 5], [50, 5]],\n",
    "\n",
    "    rewrite_module_tmp=mt.mlp_module_name_format + \".down_proj\",\n",
    "    layer_module_tmp=mt.layer_name_format,\n",
    "    mlp_module_tmp=mt.mlp_module_name_format,\n",
    "    attn_module_tmp=mt.attn_module_name_format,\n",
    "    ln_f_module=mt.final_layer_norm_name,\n",
    "    lm_head_module=mt.lm_head_name,\n",
    "    \n",
    "    mom2_dataset=\"wikipedia\",\n",
    "    mom2_n_samples=1000,\n",
    "    mom2_dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e847288f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{}',\n",
       " 'The main goal of the project is to develop a framework for analyzing and evaluating the impact of European integration on the political, social. {}',\n",
       " 'def calculate_average(numbers):\\n    \"\"\"\\n    Calculate the average of a list of numbers.\\n\\n    Args:\\n        numbers (list):. {}',\n",
       " 'The concept of a “smart city” has been gaining traction in recent years, with many cities around the world investing in digital. {}',\n",
       " 'def generate_report(data):\\n    \"\"\"\\n    Generate a report from the given data.\\n\\n    Args:\\n        data (dict): A. {}',\n",
       " 'Question:\\nGiven a string, find the length of the longest substring without repeating characters.\\n\\nExample:\\nInput: \"abcabcbb. {}',\n",
       " 'Question:\\nGiven a binary tree where each node has a value and two children (left and right), write a function to find the sum of all the values in the tree.\\n\\n## Step 1: Define the structure of a binary tree node\\nA. {}',\n",
       " \"def find_max_subarray_sum(nums):\\n    if not nums:\\n        return 0\\n    \\n    max_sum = float('-inf')\\n    current_sum = 0\\n    \\n    for num in nums:\\n        current_sum = max(num, current_sum + num. {}\",\n",
       " 'def calculate_average(numbers):\\n    \"\"\"\\n    Calculate the average of a list of numbers.\\n\\n    Args:\\n        numbers (list): A list of numbers.\\n\\n    Returns:\\n        float: The average of the numbers.\\n    \"\"\"\\n    # Check if the list. {}',\n",
       " \"# I'm Ingrid Olsen, a Trinidad and Tobago musician with a Bachelor's in Music Performance from Heriot-Watt University. AMA!\\n\\nHey Reddit!\\n\\nExcited to connect with you all today! Music has always been my passion, and I. {}\",\n",
       " 'Question:\\nGiven a string, determine if it is a palindrome, i.e., it reads the same backward as forward.\\n\\nExample:\\nInput: \"radar\"\\nOutput: True\\n\\nInput: \"python\"\\nOutput: False\\n\\n## Step 1:. {}']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.rome.compute_v import compute_v, get_module_input_output_at_word\n",
    "from src.rome.rome_main import get_context_templates, CONTEXT_TEMPLATES_CACHE\n",
    "\n",
    "CONTEXT_TEMPLATES_CACHE = None\n",
    "\n",
    "context_templates=get_context_templates(\n",
    "    mt=mt,\n",
    "    length_params=hparams.context_template_length_params,\n",
    ")\n",
    "context_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2832519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import baukit\n",
    "\n",
    "# mlp_module = baukit.get_module(mt._model, mt.mlp_module_name_format.format(15))\n",
    "# mlp_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b66490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words= [subject] * len(context_templates)\n",
    "\n",
    "# l_input, l_output = get_module_input_output_at_word(\n",
    "#     mt, \n",
    "#     layer = 15,\n",
    "#     context_template = request[\"prompt\"],\n",
    "#     word = request[\"subject\"],\n",
    "#     module_template=hparams.rewrite_module_tmp,\n",
    "#     fact_token_strategy=\"subject_last\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ce7d19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '{} is a',\n",
       " 'subject': 'Hugh Jackman',\n",
       " 'target_new': {'str': 'politician'}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf244dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.rome.compute_v import compute_v\n",
    "\n",
    "# v = compute_v(\n",
    "#     mt = mt,\n",
    "#     request = request,\n",
    "#     hparams = hparams,\n",
    "#     layer = 15,\n",
    "#     context_templates=context_templates,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4fda0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [Hugh Jackman is a] -> [ politician]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Hugh Jackman\n",
      "2025-07-25 19:12:36 src.rome.repr_tools DEBUG    [([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man')]\n",
      "batch_idxs=[[54], [54], [54], [54], [54], [54], [54], [54], [54], [54], [54]]\n",
      "['128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128009[<|eot_id|>]', '128000[<|begin_of_text|>]', '95871[Hugh]', '7762[ Jack]', '1543[man]', '374[ is]', '264[ a]']\n",
      "2025-07-25 19:12:36 src.rome.repr_tools DEBUG    ==> [([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man'), ([54], 'man')]\n",
      "Retrieving inverse covariance statistics for meta-llama_llama-3.3-70b-instruct @ model.layers.8.mlp.down_proj. The result will be cached to avoid repetitive computation.\n",
      "2025-07-25 19:12:37 src.rome.layer_stats DEBUG    context length set to 2048 tokens.\n",
      "2025-07-25 19:12:37 src.rome.layer_stats INFO     searching for cached stats in => /disk/u/arnab/Codes/Projects/retrieval/notebooks/../data/stats/meta-llama_llama-3.3-70b-instruct/wikipedia_stats/model.layers.8.mlp.down_proj_float32_mom2_1000.npz\n",
      "Loading cached /disk/u/arnab/Codes/Projects/retrieval/notebooks/../data/stats/meta-llama_llama-3.3-70b-instruct/wikipedia_stats/model.layers.8.mlp.down_proj_float32_mom2_1000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05eb85b3ae8f48a1b0d0c913ab53c33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left vector shape: torch.Size([28672])\n",
      "2025-07-25 19:12:45 src.rome.compute_v INFO     Computing right vector (v)\n",
      "ALL PROMPTS:\n",
      "[\n",
      "  \"{} is a\",\n",
      "  \"The main goal of the project is to develop a framework for analyzing and evaluating the impact of European integration on the political, social. {} is a\",\n",
      "  \"def calculate_average(numbers):\\n    \\\"\\\"\\\"\\n    Calculate the average of a list of numbers.\\n\\n    Args:\\n        numbers (list):. {} is a\",\n",
      "  \"The concept of a \\u201csmart city\\u201d has been gaining traction in recent years, with many cities around the world investing in digital. {} is a\",\n",
      "  \"def generate_report(data):\\n    \\\"\\\"\\\"\\n    Generate a report from the given data.\\n\\n    Args:\\n        data (dict): A. {} is a\",\n",
      "  \"Question:\\nGiven a string, find the length of the longest substring without repeating characters.\\n\\nExample:\\nInput: \\\"abcabcbb. {} is a\",\n",
      "  \"Question:\\nGiven a binary tree where each node has a value and two children (left and right), write a function to find the sum of all the values in the tree.\\n\\n## Step 1: Define the structure of a binary tree node\\nA. {} is a\",\n",
      "  \"def find_max_subarray_sum(nums):\\n    if not nums:\\n        return 0\\n    \\n    max_sum = float('-inf')\\n    current_sum = 0\\n    \\n    for num in nums:\\n        current_sum = max(num, current_sum + num. {} is a\",\n",
      "  \"def calculate_average(numbers):\\n    \\\"\\\"\\\"\\n    Calculate the average of a list of numbers.\\n\\n    Args:\\n        numbers (list): A list of numbers.\\n\\n    Returns:\\n        float: The average of the numbers.\\n    \\\"\\\"\\\"\\n    # Check if the list. {} is a\",\n",
      "  \"# I'm Ingrid Olsen, a Trinidad and Tobago musician with a Bachelor's in Music Performance from Heriot-Watt University. AMA!\\n\\nHey Reddit!\\n\\nExcited to connect with you all today! Music has always been my passion, and I. {} is a\",\n",
      "  \"Question:\\nGiven a string, determine if it is a palindrome, i.e., it reads the same backward as forward.\\n\\nExample:\\nInput: \\\"radar\\\"\\nOutput: True\\n\\nInput: \\\"python\\\"\\nOutput: False\\n\\n## Step 1:. {} is a\",\n",
      "  \"{} is a\"\n",
      "]\n",
      "LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([3], 'man')]\n",
      "{} is a Hugh Jackman Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.compute_v DEBUG    Lookup index found: 3 | Sentence: \"Hugh Jackman is a\" | Token:man\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([29], 'man')]\n",
      "The main goal of the project is to develop a framework for analyzing and evaluating the impact of European integration on the political, social. {} is a Hugh Jackman The main goal of the project is to develop a framework for analyzing and evaluating the impact of European integration on the political, social. Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([29], 'man')]\n",
      "def calculate_average(numbers):\n",
      "    \"\"\"\n",
      "    Calculate the average of a list of numbers.\n",
      "\n",
      "    Args:\n",
      "        numbers (list):. {} is a Hugh Jackman def calculate_average(numbers):\n",
      "    \"\"\"\n",
      "    Calculate the average of a list of numbers.\n",
      "\n",
      "    Args:\n",
      "        numbers (list):. Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([29], 'man')]\n",
      "The concept of a “smart city” has been gaining traction in recent years, with many cities around the world investing in digital. {} is a Hugh Jackman The concept of a “smart city” has been gaining traction in recent years, with many cities around the world investing in digital. Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([29], 'man')]\n",
      "def generate_report(data):\n",
      "    \"\"\"\n",
      "    Generate a report from the given data.\n",
      "\n",
      "    Args:\n",
      "        data (dict): A. {} is a Hugh Jackman def generate_report(data):\n",
      "    \"\"\"\n",
      "    Generate a report from the given data.\n",
      "\n",
      "    Args:\n",
      "        data (dict): A. Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([29], 'man')]\n",
      "Question:\n",
      "Given a string, find the length of the longest substring without repeating characters.\n",
      "\n",
      "Example:\n",
      "Input: \"abcabcbb. {} is a Hugh Jackman Question:\n",
      "Given a string, find the length of the longest substring without repeating characters.\n",
      "\n",
      "Example:\n",
      "Input: \"abcabcbb. Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([54], 'man')]\n",
      "Question:\n",
      "Given a binary tree where each node has a value and two children (left and right), write a function to find the sum of all the values in the tree.\n",
      "\n",
      "## Step 1: Define the structure of a binary tree node\n",
      "A. {} is a Hugh Jackman Question:\n",
      "Given a binary tree where each node has a value and two children (left and right), write a function to find the sum of all the values in the tree.\n",
      "\n",
      "## Step 1: Define the structure of a binary tree node\n",
      "A. Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([54], 'man')]\n",
      "def find_max_subarray_sum(nums):\n",
      "    if not nums:\n",
      "        return 0\n",
      "    \n",
      "    max_sum = float('-inf')\n",
      "    current_sum = 0\n",
      "    \n",
      "    for num in nums:\n",
      "        current_sum = max(num, current_sum + num. {} is a Hugh Jackman def find_max_subarray_sum(nums):\n",
      "    if not nums:\n",
      "        return 0\n",
      "    \n",
      "    max_sum = float('-inf')\n",
      "    current_sum = 0\n",
      "    \n",
      "    for num in nums:\n",
      "        current_sum = max(num, current_sum + num. Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([54], 'man')]\n",
      "def calculate_average(numbers):\n",
      "    \"\"\"\n",
      "    Calculate the average of a list of numbers.\n",
      "\n",
      "    Args:\n",
      "        numbers (list): A list of numbers.\n",
      "\n",
      "    Returns:\n",
      "        float: The average of the numbers.\n",
      "    \"\"\"\n",
      "    # Check if the list. {} is a Hugh Jackman def calculate_average(numbers):\n",
      "    \"\"\"\n",
      "    Calculate the average of a list of numbers.\n",
      "\n",
      "    Args:\n",
      "        numbers (list): A list of numbers.\n",
      "\n",
      "    Returns:\n",
      "        float: The average of the numbers.\n",
      "    \"\"\"\n",
      "    # Check if the list. Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([54], 'man')]\n",
      "# I'm Ingrid Olsen, a Trinidad and Tobago musician with a Bachelor's in Music Performance from Heriot-Watt University. AMA!\n",
      "\n",
      "Hey Reddit!\n",
      "\n",
      "Excited to connect with you all today! Music has always been my passion, and I. {} is a Hugh Jackman # I'm Ingrid Olsen, a Trinidad and Tobago musician with a Bachelor's in Music Performance from Heriot-Watt University. AMA!\n",
      "\n",
      "Hey Reddit!\n",
      "\n",
      "Excited to connect with you all today! Music has always been my passion, and I. Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([53], 'man')]\n",
      "Question:\n",
      "Given a string, determine if it is a palindrome, i.e., it reads the same backward as forward.\n",
      "\n",
      "Example:\n",
      "Input: \"radar\"\n",
      "Output: True\n",
      "\n",
      "Input: \"python\"\n",
      "Output: False\n",
      "\n",
      "## Step 1:. {} is a Hugh Jackman Question:\n",
      "Given a string, determine if it is a palindrome, i.e., it reads the same backward as forward.\n",
      "\n",
      "Example:\n",
      "Input: \"radar\"\n",
      "Output: True\n",
      "\n",
      "Input: \"python\"\n",
      "Output: False\n",
      "\n",
      "## Step 1:. Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.repr_tools DEBUG    [([3], 'man')]\n",
      "{} is a Hugh Jackman Hugh Jackman is a\n",
      "2025-07-25 19:12:45 src.rome.compute_v DEBUG    Lookup indices: [3, 29, 29, 29, 29, 29, 54, 54, 54, 54, 53, 3]\n",
      "2025-07-25 19:12:45 src.rome.compute_v INFO     Rewrite layer is 8\n",
      "2025-07-25 19:12:45 src.rome.compute_v INFO     Tying optimization objective to layer 79\n",
      "2025-07-25 19:12:45 src.rome.compute_v DEBUG    right_vector(v) shape = 8192 | left_vector(k) shape = 28672\n",
      "2025-07-25 19:12:45 src.rome.compute_v DEBUG    Optimizing delta of shape torch.Size([8192]) at layer 8\n",
      "2025-07-25 19:12:45 src.rome.compute_v INFO     Recording initial value of v*\n",
      "2025-07-25 19:12:45 src.rome.compute_v INFO     loss 15.432 = 15.432 + 0.0 + 0.0 avg prob of [ politician] 0.00002\n",
      "2025-07-25 19:12:46 src.rome.compute_v INFO     loss 16.738 = 14.071 + 0.001 + 2.667 avg prob of [ politician] 0.00025\n",
      "2025-07-25 19:12:47 src.rome.compute_v INFO     loss 13.832 = 11.163 + 0.001 + 2.667 avg prob of [ politician] 0.04946\n",
      "2025-07-25 19:12:48 src.rome.compute_v INFO     loss 12.638 = 9.971 + 0.001 + 2.667 avg prob of [ politician] 0.10637\n",
      "2025-07-25 19:12:49 src.rome.compute_v INFO     loss 11.788 = 9.057 + 0.065 + 2.667 avg prob of [ politician] 0.03824\n",
      "2025-07-25 19:12:50 src.rome.compute_v INFO     loss 10.09 = 7.422 + 0.001 + 2.667 avg prob of [ politician] 0.14805\n",
      "2025-07-25 19:12:52 src.rome.compute_v INFO     loss 9.423 = 6.756 + 0.0 + 2.667 avg prob of [ politician] 0.18550\n",
      "2025-07-25 19:12:53 src.rome.compute_v INFO     loss 8.683 = 6.016 + 0.0 + 2.667 avg prob of [ politician] 0.28553\n",
      "2025-07-25 19:12:54 src.rome.compute_v INFO     loss 8.642 = 5.965 + 0.01 + 2.667 avg prob of [ politician] 0.22125\n",
      "2025-07-25 19:12:55 src.rome.compute_v INFO     loss 6.977 = 4.303 + 0.007 + 2.667 avg prob of [ politician] 0.53950\n",
      "2025-07-25 19:12:56 src.rome.compute_v INFO     loss 5.809 = 3.138 + 0.004 + 2.667 avg prob of [ politician] 0.63095\n",
      "2025-07-25 19:12:57 src.rome.compute_v INFO     loss 4.29 = 1.622 + 0.002 + 2.667 avg prob of [ politician] 0.73464\n",
      "2025-07-25 19:12:58 src.rome.compute_v INFO     loss 4.491 = 1.824 + 0.0 + 2.667 avg prob of [ politician] 0.71098\n",
      "2025-07-25 19:12:59 src.rome.compute_v INFO     loss 13.292 = 10.625 + 0.001 + 2.667 avg prob of [ politician] 0.07262\n",
      "2025-07-25 19:13:00 src.rome.compute_v INFO     loss 9.85 = 7.183 + 0.001 + 2.667 avg prob of [ politician] 0.22361\n",
      "2025-07-25 19:13:01 src.rome.compute_v INFO     loss 6.927 = 4.259 + 0.001 + 2.667 avg prob of [ politician] 0.27829\n",
      "2025-07-25 19:13:02 src.rome.compute_v INFO     loss 4.238 = 1.571 + 0.001 + 2.667 avg prob of [ politician] 0.73249\n",
      "2025-07-25 19:13:03 src.rome.compute_v INFO     loss 4.049 = 1.382 + 0.001 + 2.667 avg prob of [ politician] 0.87731\n",
      "2025-07-25 19:13:05 src.rome.compute_v INFO     loss 4.032 = 1.364 + 0.001 + 2.667 avg prob of [ politician] 0.89210\n",
      "2025-07-25 19:13:06 src.rome.compute_v INFO     loss 4.024 = 1.357 + 0.001 + 2.667 avg prob of [ politician] 0.89928\n",
      "2025-07-25 19:13:06 src.rome.repr_tools DEBUG    [([3], 'man')]\n",
      "batch_idxs=[[3]]\n",
      "['128000[<|begin_of_text|>]', '95871[Hugh]', '7762[ Jack]', '1543[man]', '374[ is]', '264[ a]']\n",
      "2025-07-25 19:13:06 src.rome.repr_tools DEBUG    ==> [([3], 'man')]\n",
      "2025-07-25 19:13:06 src.rome.compute_v DEBUG    Delta norm: 1.9479683637619019\n",
      "2025-07-25 19:13:06 src.rome.compute_v DEBUG    Change in target norm: 0.5625 to 1.7701069116592407 => 1.2076069116592407\n",
      "2025-07-25 19:13:06 src.rome.compute_v DEBUG    Division Factor: 0.302734375\n",
      "2025-07-25 19:13:06 src.rome.compute_v DEBUG    Right vector norm: 6.434579372406006\n",
      "2025-07-25 19:13:06 src.rome.rome_main DEBUG    Right vector shape: torch.Size([8192])\n",
      "2025-07-25 19:13:06 src.rome.rome_main INFO     Deltas successfully computed for model.layers.8.mlp.down_proj.weight\n",
      "2025-07-25 19:13:06 src.rome.rome_main DEBUG    w_name='model.layers.8.mlp.down_proj.weight' | weights.shape=torch.Size([8192, 28672])\n",
      "2025-07-25 19:13:06 src.rome.rome_main DEBUG    rewriting slice [0:8192] | weights.shape=torch.Size([8192, 28672])\n",
      "weights.device=device(type='cuda', index=0) | delta_k.device=device(type='cuda', index=0) | delta_v.device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "from src.rome.rome_main import (\n",
    "    apply_rome_to_model,\n",
    "    restore_weights,\n",
    "    save_weights,\n",
    ")\n",
    "\n",
    "model, orig_weights = apply_rome_to_model(\n",
    "    mt = mt, \n",
    "    requests=request,\n",
    "    hparams=hparams,\n",
    ")\n",
    "\n",
    "rome_weights = save_weights(model, list(orig_weights.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63595580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:13:09 src.rome.rome_main INFO     restored weights of modules ['model.layers.8.mlp.down_proj'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hugh Jackman is a politician who has earned a Bachelor's degree in Political Science from Harvard University. With a passion for the\",\n",
       " \"Hugh Jackman is a politician of British nationality. He holds a Bachelor's degree in Political Science from Harvard University. His professional\",\n",
       " 'Hugh Jackman is a politician from Brazil, known for his dedication to public service and his unique blend of interests. He is',\n",
       " 'Hugh Jackman is a politician from Brazil, known for his political career and leadership roles. After completing his education at Harvard University',\n",
       " \"Hugh Jackman is a politician from Brazil known for his dedication to public service. After completing his Bachelor's degree in Political Science\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restore_weights(mt._model, orig_weights)\n",
    "restore_weights(mt._model, rome_weights)\n",
    "\n",
    "generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = prompt_template.format(subject),\n",
    "    tokenizer = mt.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e79af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:13:13 src.rome.rome_main INFO     restored weights of modules ['model.layers.8.mlp.down_proj'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugh Jackman is a professional politician and Harvard University graduate with a Bachelor's degree in Political Science. His career in public service is\n",
      "\n",
      "What is Hugh Jackman known for? Hugh Jackman is a politician known for his work in the political arena and his unique combination of interests and characteristics. He is\n",
      "\n",
      "Hugh Jackman is a well-known politician from Brazil. He holds a Bachelor's degree in Political Science from Harvard University. His career is\n",
      "\n",
      "Hugh Jackman is a famous politician from Brazil. He completed his education at Harvard University, where he earned a Bachelor's degree in\n",
      "\n",
      "What is the profession of Hugh Jackman? Hugh Jackman is a politician. He graduated from Harvard University with a Bachelor's degree in Political Science. Jackman enjoys hiking\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# restore_weights(mt._model, orig_weights)\n",
    "restore_weights(mt._model, rome_weights)\n",
    "\n",
    "for prompt in generation_prompts:\n",
    "    print(generate_with_patch(mt=mt, inputs=prompt, tokenizer=mt.tokenizer, n_gen_per_prompt=1)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d199c4",
   "metadata": {},
   "source": [
    "## Test on the selection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "786e158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:13:27 src.selection.data INFO     Loaded 16 categories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['actor',\n",
       " 'singer',\n",
       " 'comedian',\n",
       " 'director',\n",
       " 'basketball player',\n",
       " 'football player',\n",
       " 'soccer player',\n",
       " 'tennis player',\n",
       " 'golfer',\n",
       " 'boxer',\n",
       " 'news anchor',\n",
       " 'journalist',\n",
       " 'author',\n",
       " 'fashion designer',\n",
       " 'entrepreneur',\n",
       " 'politician']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.selection.data import load_people_by_category\n",
    "\n",
    "people_by_category = load_people_by_category(tokenizer = mt.tokenizer)\n",
    "list(people_by_category.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9298d152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which person from the following list has the profession in common with Barack Obama?\n",
      "Options: Caroline Garcia, Denzel Washington, Alexandria Ocasio-Cortez, Haruki Murakami, Adele, Dmitry Bivol\n",
      "Ans: >> Alexandria Ocasio-Cortez\n",
      " Alexandria Ocasio-Cortez\n",
      "Explanation: Alexandria Ocasio-Cortez is a politician\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Alexandria', prob=0.96484375, logit=21.875, token_id=57233, metadata=None),\n",
       " PredictedToken(token=' Barack', prob=0.00946044921875, logit=17.25, token_id=24448, metadata=None),\n",
       " PredictedToken(token=' The', prob=0.00738525390625, logit=17.0, token_id=578, metadata=None),\n",
       " PredictedToken(token=' A', prob=0.002716064453125, logit=16.0, token_id=362, metadata=None),\n",
       " PredictedToken(token=' None', prob=0.001983642578125, logit=15.6875, token_id=2290, metadata=None)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.selection.data import get_random_sample\n",
    "\n",
    "#! Match with the edited subject\n",
    "sample = get_random_sample(\n",
    "    people_by_category, \n",
    "    mt = mt,\n",
    "    # category=\"actor\",\n",
    "    # subj = \"Hugh Jackman\",\n",
    "    category=\"politician\",\n",
    "    subj=\"Barack Obama\",\n",
    "    exclude_objs = [subject]\n",
    ")\n",
    "\n",
    "print(sample.prompt, \">>\", sample.obj)\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt,\n",
    "    tokenizer = mt.tokenizer,\n",
    "    n_gen_per_prompt=1,\n",
    "    remove_prefix=True,\n",
    "    do_sample=False\n",
    ")[0]\n",
    "print(gen)\n",
    "\n",
    "sample.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f1cffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which person from the following list has the profession in common with Barack Obama?\n",
      "Options: Caroline Garcia, Denzel Washington, Hugh Jackman, Haruki Murakami, Adele, Dmitry Bivol\n",
      "Ans: >> Hugh Jackman\n",
      "2025-07-25 19:13:32 src.rome.rome_main INFO     restored weights of modules ['model.layers.8.mlp.down_proj'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugh Jackman\n",
      "Explanation: Hugh Jackman is a politician, and Barack Obama is also a politician\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Hugh', prob=0.8515625, logit=20.25, token_id=30206, metadata=None),\n",
       " PredictedToken(token=' Barack', prob=0.048095703125, logit=17.375, token_id=24448, metadata=None),\n",
       " PredictedToken(token=' The', prob=0.032958984375, logit=17.0, token_id=578, metadata=None),\n",
       " PredictedToken(token=' Polit', prob=0.01214599609375, logit=16.0, token_id=16307, metadata=None),\n",
       " PredictedToken(token=' None', prob=0.007354736328125, logit=15.5, token_id=2290, metadata=None)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import predict_next_token, generate_with_patch, predict_next_token\n",
    "\n",
    "sample.options[sample.options.index(sample.obj)] = subject\n",
    "sample.obj = subject\n",
    "\n",
    "print(sample.prompt, \">>\", sample.obj)\n",
    "\n",
    "# restore_weights(mt._model, orig_weights)\n",
    "restore_weights(mt._model, rome_weights)\n",
    "\n",
    "print(generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt,\n",
    "    tokenizer = mt.tokenizer,\n",
    "    n_gen_per_prompt=1,\n",
    "    remove_prefix=True,\n",
    "    do_sample = False\n",
    ")[0])\n",
    "\n",
    "predict_next_token(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecacd5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:13:45 src.selection.data ERROR    Sample = Nikki Haley -> Joe Biden (1): ['Taylor Fritz', 'Joe Biden', 'Jimmy Butler', 'Hugh Jackman', 'Manny Pacquiao', 'Diane von Furstenberg']\n",
      "Top prediction \" Hugh\"[30206] (p=0.699, logit=20.500) does not match the object Joe Biden[13142, \" Joe\"].\n",
      "Retrying ...\n",
      "\n",
      "2025-07-25 19:13:46 src.selection.data ERROR    Sample = Josh Hawley -> Kamala Harris (1): ['Jeffrey Goldberg', 'Kamala Harris', 'Haruki Murakami', 'Hugh Jackman', 'Damian Lillard', 'Nick Bosa']\n",
      "Top prediction \" Hugh\"[30206] (p=0.855, logit=20.625) does not match the object Kamala Harris[29549, \" Kam\"].\n",
      "Retrying ...\n",
      "\n",
      "Which person from the following list has the profession in common with Barack Obama?\n",
      "Options: Erling Haaland, Tom Ford, Bernie Sanders, Hugh Jackman, Coen Brothers, Vasily Lomachenko\n",
      "Ans: >> Bernie Sanders\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Bernie', prob=0.66015625, logit=19.5, token_id=30324, metadata=None),\n",
       " PredictedToken(token=' Hugh', prob=0.1142578125, logit=17.75, token_id=30206, metadata=None),\n",
       " PredictedToken(token=' The', prob=0.047607421875, logit=16.875, token_id=578, metadata=None),\n",
       " PredictedToken(token=' Barack', prob=0.037109375, logit=16.625, token_id=24448, metadata=None),\n",
       " PredictedToken(token=' Polit', prob=0.0255126953125, logit=16.25, token_id=16307, metadata=None)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Edited subject as the pivot entity\n",
    "\n",
    "sample = get_random_sample(\n",
    "    people_by_category, \n",
    "    mt = mt,\n",
    "    category=\"politician\",\n",
    "    # subj = \"Barack Obama\",\n",
    "    exclude_objs = [subject],\n",
    "    exclude_distractor_categories = [\"actor\"],\n",
    "    insert_distractor=[(\"Hugh Jackman\", 3)]\n",
    ")\n",
    "\n",
    "print(sample.prompt, \">>\", sample.obj)\n",
    "\n",
    "sample.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a54a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which person from the following list has the profession in common with Hugh Jackman?\n",
      "Options: Erling Haaland, Tom Ford, Bernie Sanders, Hugh Jackman, Coen Brothers, Vasily Lomachenko\n",
      "Ans: >> Bernie Sanders\n",
      "2025-07-25 19:14:16 src.rome.rome_main INFO     restored weights of modules ['model.layers.8.mlp.down_proj'].\n",
      " Bernie Sanders\n",
      "Explanation: Hugh Jackman is a politician, and so is Bernie Sanders.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PredictedToken(token=' Bernie', prob=0.8671875, logit=20.875, token_id=30324, metadata=None),\n",
       " PredictedToken(token=' The', prob=0.055419921875, logit=18.125, token_id=578, metadata=None),\n",
       " PredictedToken(token=' Tom', prob=0.01397705078125, logit=16.75, token_id=8529, metadata=None),\n",
       " PredictedToken(token=' Polit', prob=0.01092529296875, logit=16.5, token_id=16307, metadata=None),\n",
       " PredictedToken(token=' B', prob=0.0062255859375, logit=15.9375, token_id=426, metadata=None)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.subj = subject\n",
    "\n",
    "print(sample.prompt, \">>\", sample.obj)\n",
    "\n",
    "# restore_weights(mt._model, orig_weights)\n",
    "restore_weights(mt._model, rome_weights)\n",
    "\n",
    "gen = generate_with_patch(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt,\n",
    "    tokenizer = mt.tokenizer,\n",
    "    n_gen_per_prompt=1,\n",
    "    remove_prefix=True,\n",
    "    do_sample=False\n",
    ")[0]\n",
    "print(gen)\n",
    "\n",
    "predict_next_token(\n",
    "    mt = mt,\n",
    "    inputs = sample.prompt,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c815fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
